\section{Wielowskaźnikowe modele regresji liniowej}
$ G $-równaniowy ekonometryczny model regresji wielorakiej - postać uogólniona.

\subsection{$ G $-równaniowy model regresji wielorakiej (bez restrykcji zerowych):}
\begin{align*}
&t_{t1}=\gamma_{12}t_{t2}+\dots+\gamma_{1G}y_{tG}+\beta_{10}+\beta_{11}x_{t1}+\beta_{12}x_{t2}+\dots+\beta_{1k}x_{tk}+\varepsilon_{t1}\\
&t_{t1}=\gamma_{21}t_{t1}+\gamma_{23}t_{t3}+\dots+\gamma_{2G}y_{tG}+\beta_{20}+\beta_{21}x_{t1}+\beta_{22}x_{t2}+\dots+\beta_{2k}x_{tk}+\varepsilon_{t2}\\
\vdots\\
&t_{tG}=\gamma_{G1}t_{t1}+\gamma_{G2}t_{t2}+\dots+\beta_{G0}+\beta_{G1}x_{t1}+\beta_{G2}x_{t2}+\dots+\beta_{Gk}x_{tk}+\varepsilon_{tG}
\end{align*}

\subsubsection{Syntetyczny zapis macierzowy modelu}
\begin{gather*}
y_t\cdot\Gamma+x_t\cdot B=\varepsilon_t
\end{gather*}
gdzie:\\
$ y_t=\begin{bmatrix}
y_{t1} &y_{t2}& \dots& y_{tG}
\end{bmatrix} $ - wektor zmiennych endogenicznych modelu\\
$ \Gamma=\begin{bmatrix}
\gamma_{G\times G}
\end{bmatrix} $ - macierz parametrów strukturalnych zmiennych endogenicznych\\
$ x_t=\begin{bmatrix}
1& x_{t1} &x_{t2} &\dots& x_{tk}
\end{bmatrix} $ - wektor zmiennych egzogenicznych modelu\\
$ B=\begin{bmatrix}
\beta_{(k+1)\times G}
\end{bmatrix} $ - macierz parametrów strukturalnych zmiennych egzogenicznych\\
$ \varepsilon_t=\begin{bmatrix}
\varepsilon_{t1}& \varepsilon_{t2}& \dots& \varepsilon_{tG}
\end{bmatrix} $ - wektor składników zakłócających modelu
\begin{gather*}
Y\cdot\Gamma+X\cdot B=E
\end{gather*}
$ Y=\begin{bmatrix}
	y_{11} & y_{12} & \dots  & y_{1G} \\
	y_{21} & y_{22} & \dots  & y_{2G} \\
	\vdots & \vdots & \ddots & \vdots \\
	y_{n1} & y_{n2} & \dots  & y_{nG}
\end{bmatrix} 
=Y_{n\times G}$
\begin{minipage}[t]{.5\textwidth}
- macierz obserwacji endogenicznych modelu
\end{minipage}\\
$ X=\begin{bmatrix}
	1 & x_{11} & x_{12} & \dots  & x_{1k} \\
	1 & x_{21} & x_{22} & \dots  & x_{2k} \\
	1 & \vdots & \vdots & \ddots & \vdots \\
	1 & x_{n1} & x_{n2} & \dots  & x_{nk}
\end{bmatrix} 
=X_{n\times(k+1)} $
\begin{minipage}[t]{.45\textwidth}
- macierz obserwacji zmiennych egzogenicznych modelu\\
\end{minipage}
$ E=\begin{bmatrix}
	\varepsilon_{11} & \varepsilon_{12} & \dots  & \varepsilon_{1G} \\
	\varepsilon_{21} & \varepsilon_{22} & \dots  & \varepsilon_{2G} \\
	\vdots & \vdots & \ddots & \vdots \\
	\varepsilon_{n1} & \varepsilon_{n2} & \dots  & \varepsilon_{nG}
\end{bmatrix} 
=E_{n\times G}$
\begin{minipage}[t]{.5\textwidth}
- macierz składników zakłócających $ G $-równaniowego modelu
\end{minipage}

\subsection{Wstępne założenia dotyczące struktury stochastycznej modelu wielorównaniowego.}
\begin{align*}
\mathbb E \varepsilon_{tj}=0&&t=1,2,\dots,n&&j=1,2,\dots,G\\
\mathbb E \varepsilon=0
\end{align*}
co oznacza, że składnik zakłócający jest zmienną losową o wartości oczekiwanej równe 0, w każdym okresie $ t $ i w każdym równaniu $ j $-tym.
\begin{gather*}
\mathbb E \varepsilon^2_{tj}=\sigma^2_{\varepsilon j}=\text{const.}\qquad j=1,2,\dots,G
\end{gather*}
co oznacza, że w każdym równaniu $ j $-tym wariancja składnika losowego jest stała i niezależna od okresu obserwacji.
\begin{gather*}
\mathbb E \varepsilon_{tj}\varepsilon_{(t-s)j}=0\qquad,(s\neq0)
\end{gather*}
co oznacza, że kowariancja między składnikami losowymi w poszczególnych równaniach dla różnych okresów jest równa 0.
{Klasyfikacja modeli z punktu widzenia powiązań pomiędzy zmiennymi endogenicznymi.}
\subsubsection{Model prosty}

Model prosty jest to model, w którym nieopóźnione zmienne endogeniczne nie oddziałują na siebie. Z uwagi na brak powiązań pomiędzy nieopóźnionymi zmiennymi endogenicznymi macierz $ \Gamma $ jest macierz diagonalną, będąc macierzą jednostkową.\\
Przykład trójrównaniowego modelu prostego:
\begin{align*}
&y_{t1}=\beta_{10}+\beta_{11}x_{t1}+\beta_{12}x_{t2}-\varepsilon_{t1}\\
&y_{t2}=\beta_{20}+\beta_{21}x_{t2}+\beta_{23}x_{t3}-\varepsilon_{t2}\\
&y_{t3}=\beta_{30}+\beta_{32}x_{t1}+\beta_{33}x_{t3}-\varepsilon_{t3}
\end{align*}
Zapis w postaci macierzowej
\begin{gather*}
Y\cdot
\begin{bmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 1 \\
\end{bmatrix}
+
X\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & -\beta _{32} \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=E
\end{gather*}

Wstępne założenia stochastyczne dla modelu prostego:\\
Zakładając nielosowość zmiennych z góry ustalonych (egzogenicznych) musimy uznać, ze zmienne egzogeniczne są niezależne od składników losowych, tzn.
\begin{gather*}
\mathbb E \left(X'_j\cdot \varepsilon_j\right)=\left(\mathbb E X'_j\right)\cdot \left(\mathbb E \varepsilon_j\right)=X'_j\cdot 0=0
\end{gather*}

\subsubsection{Model rekurencyjny}
Model rekurencyjny jest to model, w którym występują jednokierunkowe powiązania pomiędzy zmiennymi endogenicznymi nieopóźnionymi w czasie. Oznacza to, że macierz $ \Gamma $ - parametrów występujących przy zmiennych endogenicznych - jest macierzą trójkątną.\\
Przykład trójrównaniowego modelu rekurencyjnego:
\begin{gather*}
Y\cdot
\begin{bmatrix}
	1       & 0       & 0 \\
	-y_{12} & 1       & 0 \\
	-y_{13} & -y_{23} & 1
\end{bmatrix}
+
X\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & -\beta _{32} \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=E
\end{gather*}
\textbf{Wniosek}\\
W modelach rekurencyjnych istnieje zawsze takie równanie, w którym występują jedynie zmienne z góry ustalone (egzogeniczne).

W przypadku pozostałych równań musimy uznać, że w zbiorze zmiennych objaśniających występują zmienne losowe, którymi są zmienne endogeniczne występujące w charakterze zmiennych objaśniających. Zmienne endogeniczne występujące w charakterze zmiennych objaśniających mogą być uznanne za:
\begin{itemize}
\item nieskorelowane ze składnikiem zakłócającym danego równania, jeśli konstrukcja modelu nie wymusza odrzucenia takiego założenia
\item skorelowane ze składnikiem zakłócającym danego równania, jeśli wynika to z założeń tkwiących u podstaw konstrukcji modelu.
\end{itemize}

\subsection{Model o równaniach współzależnych}
Model o równaniach współzależnych jest ot model, w którym występują nieopóźnione w czasie sprzężenia zwrotne pomiędzy zmiennymi endogenicznymi. Oznacza to, ze macierz $ \Gamma $ nie jest macierzą ani diagonalną, ani trójkątną.\\
Przykład trójrównaniowego modelu regresji wielorakiej:
\begin{align*}
&y_{t1}=\gamma_{12}y_{t2}+\gamma_{13}y_{t3}+\beta_{10}+\beta_{11}x_{t1}+\beta_{12}x_{t2}+\varepsilon_{t1}\\
&t_{t2}=\gamma_{23}y_{t3}+\beta_{20}+\beta_{21}x_{t1}+\beta_{23}x_{t3}+\varepsilon_{t2}\\
&y_{t3}=\gamma_{31}y_{t1}+\beta_{30}+\beta_{33}x_{t3}+\varepsilon_{t3}
\end{align*}
\begin{gather*}
Y\cdot
\begin{bmatrix}
	   1    &    0    & 0 \\
	-y_{12} &    1    & 0 \\
	-y_{13} & -y_{23} & 1
\end{bmatrix}
+X\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & -\beta _{32} \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=E
\end{gather*}
Uporządkowany całościowy zapis modelu z restrykcjami zerowymi
\begin{gather*}
y_{t1}-\gamma_{12}
\end{gather*}
\begin{gather*}
\begin{bmatrix}
	y_1 & y_2 & y_3
\end{bmatrix}
\cdot
\begin{bmatrix}
	   1    &    0    & -\gamma_{31} \\
	-y_{12} &    1    & 0 \\
	-y_{13} & -y_{23} & 1
\end{bmatrix}
+\\+
\begin{bmatrix}
	1 & x_1 & x_2 & x_3
\end{bmatrix}
\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & 0            \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=
\begin{bmatrix}
\varepsilon_1&\varepsilon_2&\varepsilon_3
\end{bmatrix}
\end{gather*}

Z uwagi na to, że macierz $ \Gamma $ nie jest ani diagonalna, ani trójkątna, model uznajemy za model o równaniach współzależnych. Oznacza to, ze między zmiennymi endogenicznymi nieopóźnionymi w czasie występują sprzężenia zwrotne.\\
Zauważmy, że zmienne endogeniczne, będąc między innymi funkcjami zmiennych losowych ($ \varepsilon $), jednocześnie nawzajem się wyjaśniają. Tym samym musimy uznać, że zmienne te - występując w poszczególnych równaniach w charakterze zmiennych objaśniających - są skorelowane ze składnikami losowymi tychże równań.

\subsubsection{Redukcja modeli o równaniach współzależnych}
Procedura sprowadzania modelu o równaniach współzależnych do wielorównaniowego modelu prostego nazywamy \underline{redukcją}. Przykładowa procedura redukcji naszego modelu
\begin{gather*}
y_t\cdot\Gamma+x_t\cdot \beta=\varepsilon_t
\end{gather*}
Przekształcona postać modelu
\begin{gather*}
y_t\cdot\Gamma=x_t\cdot (-\beta)+\varepsilon_t
\end{gather*}
Zapis macierz w postaci zredukowanej
\begin{gather*}
y_t=x_t\cdot \pi+v_t
\end{gather*}
gdzie \begin{gather*}
\pi=-B\Gamma=
\begin{bmatrix}
\pi _{10} & \pi _{20} & \pi _{30} \\
\pi _{11} & \pi _{21} & \pi _{31} \\
\pi _{12} & \pi _{22} & \pi _{32} \\
\pi _{13} & \pi _{23} & \pi _{33} \\
\end{bmatrix}
\end{gather*}
$ \pi  $- macierz parametrów strukturalnych postaci zredukowanej o wymiarach $(k+1)\times G $ (gdzie $ k+1=4,\;G=3 $)\\
Wektor składników zakłócających postaci zredukowanej o wymiarach $ G\times G,\;G=3 $
\begin{gather*}
v_t=\varepsilon_t\Gamma^{-1}=
\end{gather*}
Tym samym model w postaci macierzowej zapiszemy następująco
\begin{gather*}
\begin{bmatrix}
v_{t1}&v_{t2}&v_{t3}
\end{bmatrix}
=
\begin{bmatrix}
1&x_{t1}&x_{t2}&x_{t3}
\end{bmatrix}
\cdot
\begin{bmatrix}
\pi _{10} & \pi _{20} & \pi _{30} \\
\pi _{11} & \pi _{21} & \pi _{31} \\
\pi _{12} & \pi _{22} & \pi _{32} \\
\pi _{13} & \pi _{23} & \pi _{33} \\
\end{bmatrix}
+
\begin{bmatrix}
v_{t1}&v_{t2}&v_{t3}
\end{bmatrix}
\end{gather*}
Co po rozpisaniu przybierze następującą postać
\begin{gather*}
y_{t1}=\pi _{10}+x_{t1} \pi _{11}+x_{t2} \pi _{12}+x_{t3} \pi _{13} + v_{t1}\\
y_{t2}=\pi _{20}+x_{t1} \pi _{21}+x_{t2} \pi _{22}+x_{t3} \pi _{23} + v_{t2}\\
y_{t3}=\pi _{30}+x_{t1} \pi _{31}+x_{t2} \pi _{32}+x_{t3} \pi _{33} + v_{t3}
\end{gather*}
Rozpisany zapis macierzowy modelu dla $ n $ obserwacji przedstawia się następująco
\begin{gather*}
Y=X\Pi+V
\end{gather*}
Zauważmy, że w każdym z równań postaci zredukowanej występują jedynie zmienne z góry ustalone (egzogeniczne) modelu. Jeśli uznamy, że są one nielosowe, mamy prawo wykluczyć ich ewentualną zależność ze składnikami losowymi poszczególnych równań. Wynika z tego, że każde z równań możemy oszacować stosując metodę najmniejszych kwadratów.

\subsubsection{Identyfikacja}
Identyfikacja jest ot proces rozpoznawania (identyfikowania) parametrów strukturalnych modelu (elementów macierzy $ \Gamma $ i $ B $)  na podstawie parametrów postaci zredukowanej, a więc na podstawie elementów macierzy $ \Pi $. Ze zdefiniowania macierzy $ \Pi $ wynikają następujące konsekwencje
\begin{gather*}
\Pi=-B\Gamma^{-1}\Leftrightarrow\Pi\Gamma=-B
\end{gather*}WYkorzystując badany model oraz jego postać zredukowaną i odpowiednio zdefiniowane dla tych modeli macierze $ \pi,\Gamma $ oraz $ B $ drugi człon powyższego wyrażenia zapiszemy następująco\\
<dużo rozpisanych macierzy

Powiemy, że $ j $-ty układ równań:
\begin{enumerate}
	\item Zawiera więcej parametrów $ \gamma $ i $ \beta $ aniżeli równań, to równanie Mj-te postaci strukturalnej uznajemy za \textbf{nieidentyfikowalne}.
	\item Zawiera taką samą ilość parametrów $ \gamma $ i $ \beta $ aniżeli równań, to równanie Mj-te postaci strukturalnej uznajemy za \textbf{jednoznacznie identyfikowalne}.
	\item Zawiera mniej parametrów $ \gamma $ i $ \beta $ aniżeli równań, to równanie Mj-te postaci strukturalnej uznajemy za \textbf{niejednoznacznie identyfikowalne}.
\end{enumerate}

W rozpatrywanym przypadku otrzymujemy następujące trzy układy równań. Pierwszy układ równań $ (a) $ dla $ j=1 $ równania postaci strukturalnej modelu
\begin{align*}
	 & \pi _{10}-\pi _{20} \gamma _{12}-\pi _{30} \gamma _{13} =\beta_{10} \\
	 & \pi _{11}-\pi _{21} \gamma _{12}-\pi _{31} \gamma _{13} =\beta_{11} \\
	 & \pi _{12}-\pi _{22} \gamma _{12}-\pi _{32} \gamma _{13} =\beta_{12} \\
	 & \pi _{13}-\pi _{23} \gamma _{12}-\pi _{33} \gamma _{13} =0
\end{align*}
Z uwagi na fakt, iż liczba poszukiwanych parametrów $ \gamma $ i $ \beta $ wynosi 5 (2 parametry $ \gamma $ oraz 3 parametry $ \beta $) jest większa od liczby równań, powyższy układ nie ma rozwiązania. Tym samym powiemy, że równanie pierwsze postaci strukturalnej jest nieidentyfikowalne.

Drugi układ równań $ (b) $ dla $ j=2 $ równania postaci strukturalnej modelu
\begin{align*}
	 & \pi _{20}-\pi _{30} \gamma _{23} = \beta _{20} \\
	 & \pi _{21}-\pi _{31} \gamma _{23} = \beta _{21} \\
	 & \pi _{22}-\pi _{32} \gamma _{23} = 0           \\
	 & \pi _{23}-\pi _{33} \gamma _{23} = \beta _{23}
\end{align*}
Z uwagi na fakt, iż liczba poszukiwanych parametrów $ \gamma $ i $ \beta $ wynosi 4 (1 parametr $ \gamma $ i 3 parametry $ \beta $) jest równa liczbie równań, powyższy układ ma jednoznaczne rozwiązanie. Tym samym powiemy ,że równanie drugie postaci strukturalnej jest jednoznacznie identyfikowalne. Zauważmy, że z równania trzeciego tego układu równań wynika, że
\begin{gather*}
\pi_{22}-\gamma_{23}\pi_{32}=0\Rightarrow\gamma_{23}=\left(\frac{\pi_{21}}{\pi_{32}}\right)
\end{gather*}

Trzeci ukłąd równań $ (c) $ dla $ j=3 $ równania postaci strukturalnej modelu
\begin{align*}
& \pi _{30}-\pi _{10} \gamma _{31} = \beta _{30} \\
& \pi _{31}-\pi _{11} \gamma _{31} = 0 \\
& \pi _{32}-\pi _{12} \gamma _{31} = 0 \\
& \pi _{33}-\pi _{13} \gamma _{31} = \beta _{33} \\
\end{align*}
Z uwagi na fakt ,iż liczba poszukiwanych parametrów $ \gamma $ i $ \beta $ wynosi 3 (1 parametr $ \gamma $ i 2 parametry $ \beta $) jest mniejsza od liczby równań, powyższy układ nie ma jednoznacznego rozwiązania. Tym samym powiemy ,że równanie trzecie postaci strukturalnej jest niejednoznacznie identyfikowalne. 

Aby sformułować użyteczne twierdzenia dotyczące identyfikacji modelu o rówaniach współzależnych przyjmujemy następujący system oznaczeń:
\begin{itemize}
	\item $ k_j $ - liczba zmiennych z góry ustalonych występujących w $ j $-tym równaniu modelu łącznie z wyrazem wolnym
	\item $ K $ - liczba zmiennych z góry ustalonych występujących w całym modelu łącznie z wyrazem wolnym
	\item $ G_j $ - liczba zmiennych łącznie współzależnych (endogenicznych) występujących w $ j $-tym równaniu
	\item $ G_{j-1} $ - liczba zmiennych łącznie współzależnych (endogenicznych) występujących w $ j $-tym równaniu w charakterze zmiennych objaśniających (bez zmiennej objaśnianej w tym równaniu)
	\item $ G $ - liczba zmiennych łącznie współzależnych zależnych (endogenicznych) występujących w całym modelu (równa liczbie równań modelu)
\end{itemize}
\begin{gather*}
K_j^*=K-k_j\\
G_j^*=G-G_j
\end{gather*}
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te było identyfikowalne, jest aby liczba zmiennych z góry ustalonych oraz łącznie współzależnych występujących w równaniu $ j $-tym w charakterze zmiennych objaśniających była mniejsza od liczby zmiennych z góry ustalonych występujących w całym modelu
	\begin{gather*}
	K\ge K_j+G_j-1
	\end{gather*}
\end{twr}
Równanie możemy przekształcić w następujący sposób
\begin{gather*}
K-k_j\ge G_j-1
\end{gather*}
Wprowadzając do siebie wyrażenia otrzymujemy
\begin{gather*}
K-k_j\ge G-G_j^*-1
\end{gather*}
a stąd ostatecznie otrzymujemy
\begin{gather*}
K_j^*+G_j^*\ge G-1
\end{gather*}
Wykorzystując powyższe wyrażenie formułujemy następujące twierdzenia
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{identyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była nie mniejsza od liczby równań modelu pomniejszonej o jeden.
\end{twr}
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{jednoznacznie identyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była równa liczbie równań modelu pomniejszonej o jeden.
\end{twr}
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{niejednoznacznie identyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była większa od liczby równań modelu pomniejszonej o jeden.
\end{twr}
\begin{twr}
	Warunkiem dostatecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{nieidentyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była mniejsza od liczby równań modelu pomniejszonej o jeden.
\end{twr}

Zadanie\\
Rozpatrz następujący model o równaniach współzależnych
\begin{align*}
&Y_t=q_0+q_1L_t+a_2k_t+a_3H_t+a_4ER_t+e_{t1}\\
&L_t=b_0+b_1Y_t+b_2K_t+b_3W_te_{t2}\\
&K_t=c_0+c_1Y_t+c_2R_t+e_{t3}
\end{align*}

\subsubsection{Metoda Zmiennych Instrumentalnych}
W dotychczas przedstawianych modelach przyjmowaliśmy założenie o braku korelacji między zmiennymi uwzględnionymi w specyfikacji modelu a składnikiem losowym Jednak w wielu ważnych z punktu widzenia teorii ekonomicznej zastosowaniach takie założenie nie jest spełnione. W takim przypadku nie można udowodnić zgodności estymatora MNK.

W modelu $ Y=X\beta+\varepsilon $
\begin{itemize}
	\item zmiennymi egzogenicznymi nazywamy zmienne, które nie są skorelowane ze składnikami losowymi
	\item zmiennymi endogenicznymi nazywamy zmienne, które są skorelowane ze składnikami losowymi
\end{itemize}

\section{Równoczesność}

O problemie równoczesności mówimy, gdy występuje niezerowa korelacja pomiędzy zmienną objaśniającą $ x_i $ a równoczesnym błędem losowym $ \varepsilon_j $. Gdy $ \mathbb E \left(\varepsilon|X\right) \neq0$ to
\begin{align*}
&\mathbb E (b)=\mathbb E \left(\left(X'X\right)^{-1}X'y|X\right)\\
&\mathbb E (b)=\beta+\left(X'X\right)^{-1}X'\mathbb E \left(\varepsilon|X\right)\neq\beta
\end{align*}
Więc estymator wektora parametrów jest obciążony.

\subsection{Przykłady}
Model Keynesowski gospodarki zakłada, że\begin{gather*}
\text{PKB=konsumpacja+inwestycje+export netto}
\end{gather*}
Z drugiej strony Keynesowska funkcja konsumpcji zakłada, że $ C_t=f(Y_t) $. Szacując jej parametry
\begin{gather*}
C_t=\beta_0+\beta_1Y_t+\varepsilon_t
\end{gather*}
nie są spełnione założenia modelu, gdyż $ \text{cov}\left(X_t,\varepsilon_t\right) \neq0$\\
Szacujemy model autoregresji postaci
\begin{gather*}
y_t=f\left(y_{t-1},y_{t-2},\dots\right)+\varepsilon_t
\end{gather*}
ale
\begin{gather*}
y_{t+1}=f\left(y_{t-2},y_{t-3},\dots\right)+\varepsilon_{t+1}
\end{gather*}
zatem $ \text{cov}\left(y_{t-1},\varepsilon_{t-1}\right) \neq0$. Wobec tego w modelu $ (1) $ zmienne objaśniające są skorelowane z błędem losowym. Zazwyczaj w zbiorach danych mikroekonomicznych brakuje informacji o zdolnościach respondentów. Mimo wszystko szacuje się równanie płacy typu Mincera
\begin{gather*}
\ln\left(placa\right)=\beta_0+\beta_1plec+\beta_2\text{wiek}+\beta_3\text{wiek2}+\beta_4\text{wykszt}+u
\end{gather*}
Ponieważ w modeelu pominięto zmienną niezależną zdolności to skłądnik losowy ma postać
\begin{gather*}
u=\gamma_0+\gamma_1zdolnosci+\phi
\end{gather*}
Z drugiej strony poziom wykształcenia jest determinowany przez zdolności respondenta. Zatem
\begin{gather*}
\text{cov}(u,wykszt)=\text{cov}(zdolnosci+\phi,wykszt)=\\=
\text{cov}(zdolnosci,wykszt)+\text{cov}(\phi,wykszt)\neq0
\end{gather*}
Ponieważ zmienna pominięta jest dodatnio skorelowana z uzyskanym wykształceniem i wpływa dodatnio na zmienną zależną to parametr przy zmiennej będzie dodatnio obciążony.\\
Metoda zmiennych instrumentalnych pozwala na uzyskanie zgodnych estymatorów w przypadku występowania korelacji między zmiennymi objaśniającymi a składnikiem losowym. Pozwala również uzyskać zgodne oszacowania parametrów w przypadku występowania problemu równoczesności. Polega ona na zastępowaniu oryginalnych zmiennych instrumentami. Instrumenty powinny być skorelowane ze zmiennymi objaśniającymi, ale nie powinny być skorelowane z błędem losowym. Znalezienie właściwych instrumentów jest najciekawszym, ale również najtrudniejszym etapem badania.

Oznaczmy przez $ Z $ macierz zmiennych instrumentalnych (instrumentów). Estymator MZI jest zgodny, gdy spełnione są następujące warunki
\begin{gather*}
plim\left(\tfrac{1}{n}Z'\varepsilon\right)=plim\left(\tfrac{1}{n}\sum_iz_i'\varepsilon_i\right)=\mathbb E \left(z_i'\varepsilon_i\right)=0\\
plim\left(\tfrac{1}{n}Z'X\right)=plim\left(\tfrac{1}{n}\sum_iz_i'x_i\right)=\mathbb E \left(z_i'x_i\right)\neq0
\end{gather*}
Dodatkowo $ r\left(\mathbb E \left(z_i'x_i\right)\right) =k$
\begin{gather*}
plim\left(\tfrac{1}{n}Z'Z\right)=plim\left(\tfrac{1}{n}\sum_iz_i'z_i\right)=\mathbb E \left(z_i'z_i\right)\neq0
\end{gather*}
Metoda polega na zastępowaniu oryginalnych wartości zmiennych objaśniających wartościami dopasowanymi uzyskanymi z regresji pomocniczej wykorzystującej zmienne instrumentalne.
\begin{itemize}
	\item macierz instrumentów $ Z $ musi zawierać co najmniej tyle zmiennych, ile oryginalna macierz $ X $
	\item Ale nie w każdym przypadku konieczne jest posiadanie $ k $ nowych zmiennych
	\item Zmienne z macierzy $ X $, które są nieskorelowane ze składnikiem losowym mogą same dla siebie stanowić instrumenty
	\item W rezultacie potrzeba co najmniej tylu dodatkowych zmiennych instrumentalnych ile jest zmiennych skorelowanych ze składnikiem losowym.
\end{itemize}
Macierz instrumentów uzyskujemy poprzez rzutowanie wektora $ X $ na przestrzeń rozpiętą przez kolumny macierzy instrumentów $ Z $
\begin{gather*}
\hat{X}=Z\underset{\beta}{\underbrace{\left(Z'Z\right)^{-1}Z'X}}=Z\hat{\beta=P_ZX}
\end{gather*}
Dysponując macierzą instrumentów $ \hat X $ budujemy estymator
\begin{gather*}
\beta_{MZI}=\left(\hat X'X\right)^{-1}\hat X'y=\left(X'P_ZX\right)^{-1}X'P_Zy=\\=
\left(X'Z\left(Z'Z\right)^{-1}Z'\right)^{-1}X'Z\left(Z'Z\right)^{-1}Z'y
\end{gather*}
jeżeli $ r(2)=r(X)$, czyli tyle nowych zmiennych, ile zmiennych w macierzy $ X $ skorelowanych ze składnikiem.

Estymator MZI jest również nazywany estymatorem dwustopniowej metody najmniejszych kwadratów.
\begin{itemize}
	\item W pierwszym kroku przeprowadzana jest regresja zmiennych endogenicznych na zmienne instrumentalne
	\item W drugim, oryginalne wartości zmiennych są zastępowane prze zwartości dopasowane z pierwszego kroku i oblicze są oszacowania poszczególnych parametrów
	\item W kolejnym kroku pokażemy, że przy warunkach zdefiniowanych wcześniej estymator MZI jest zgodny
\end{itemize}
Estymator MZI jest dany wzorem
\begin{gather*}
\beta_{MZI}
=
\left(X'Z\left(Z'Z\right)^{-1}Z'\right)^{-1}X'Z\left(Z'Z\right)^{-1}Z'y
\end{gather*}
więc
\begin{gather*}
plim\beta_{MZI}
=
plim\left(X'Z\left(Z'Z\right)^{-1}Z'\right)^{-1}X'Z\left(Z'Z\right)^{-1}Z'y
\end{gather*}
\textbf{Wnioski}\\
\begin{itemize}
	\item Estymator MZI jest zgodny nawet, gdy w modelu występują zmienne endogeniczne
	\item Ale, gdy nie ma takich zmiennych, a spełnione są założenia MNK to estymator MZI nie jest efektywny
	\item MZI można traktować jako uogólnienie MNK
	\item Wobec tego wszystkie testy stosowane przy MNK mogą być stosowane przy MZI
	\item Testy specyficzne dla MZI weryfikują założenie o egzogeniczności zmiennych oraz poprawności wykorzystanych instrumentów.
\end{itemize}

\subsection{Test Hausmana}
Przy estymacji MZI test Hausmana jest testem na egzogeniczność zmiennych. Estymator MZI jest estymatorem zawsze zgodnym.\\
Przy prawdziwej $ H_0 $ o egzogeniczności zmiennych $ X $ oba estymatory są zgodne, ale estymator MZI ma większą wariancję niż estymator MNK.\\
Przy fałszywej $ H_0 $ estymator MZI jest zgodny i efektywny, a estymator MNK nie jest zgodny.\\
Statystyka testowa jest dana przez formę kwadratową
\begin{gather*}
\left(\beta_{MZI}-\beta_{MNK}\right)'\Sigma_{\beta_{MZI}-\beta_{MNK}}^{-1}\left(\beta_{MZI}-\beta_{MNK}\right)\xrightarrow{D}\chi^2\left(r(\Sigma)\right)
\end{gather*}
Gdy różnica jest duża sugeruje to wykorzystanie MNK.

\subsection{Test Surgana}
\begin{itemize}
	\item Test Surgana weryfikuje poprawność instrumentów
	\item Zauważmy, że reszty modelu są równe
	\begin{gather*}
	e=\left(I-X\left(X'P_ZX\right)^{-1}X'P_Z\right)\varepsilon
	\end{gather*}
	\item Oczywiście $ M_Z $ jest macierzą idempotentną, rzędu $ p-k $
	\item Przy prawdziwości $ H_0 $ jest brak korelacji instrumentów z błędami losowymi
	\begin{gather*}
	\frac{eP_Ze}{\sigma^2}\sim \chi^2_{p-k}
\end{gather*}
\item Niestety przeprowadzenie testu jest wyłącznie możliwe, gdy liczba instrumentów przekracza liczbę zmiennych objaśniających
\end{itemize}
\section{Uogólniona metoda zminnych instrumentalnych}
Zakładamy, że mamy więcej instrumentów niż zminnych objaśniających
\begin{center}
wymiar macierzy $ X \neq $ wymiar macierzy $ Z $
\end{center}
Założyliśmy, że $ \mathbb E \left(Z'\varepsilon\right) =0$, zatem $ \mathbb E \left(Z'\left(y-X\beta\right)\right) =0$\\
Skoro średnia z próby jest oceną wartości oczekiwanej, to
\begin{gather*}
\tfrac{1}{n}\bigl(Z'\left(y-X\beta\right)\bigr)=0
\end{gather*}
\subsection{Estymator uogólnionej MZI (UMZI,GIV)}
Przy powyższym założeniu, UMZI polega na minimalizacji następującego wyrażenia
\begin{gather*}
Q_n(\beta)=
\left[\frac{1}{n}Z'\left(y-X\beta\right)\right]'
W_n
\left[\frac{1}{n}Z'\left(y-X\beta\right)\right]
\end{gather*}
gdzie $ W_n $ jest symetryczną, kwadratową macierzą wag, o wymiarze równym liczbie instrumentów w macierzy $ Z $. Macierz ta wskazuje jakie wagi należy przypisać poszczególnym równaniom w układzie równań $ \tfrac{1}{n}\bigl(Z'\left(y-X\beta\right)\bigr)=0 $, czyli określa które instrumenty są bardziej, a które mniej ważne.\\
Minimalizując $ Q_n $ pierwszą pochodną po $ \beta  $ przyrównujemy do zera
\begin{gather*}
-ZX'ZW_nZ'y+ZX'ZW_nZ'X\hat \beta =0
\end{gather*}
co oznacza, że
\begin{gather*}
X'ZW_nZ'y=X'ZW_nZ'X\hat \beta
\end{gather*}
Zakładając, że
\begin{gather*}
\det \left(X'ZW_nZ'X\right)\neq 0
\end{gather*}
uzyskujemy estymator UMZI
\begin{gather*}
\hat \beta_{GIV}=\left(X'ZW_nZ'X\right)^{-1}X'ZW_nZ'y
\end{gather*}
W jaki sposób dobieramy macierz wag $ W_n $?\\
Różne macierze wag prowadzą do estymatorów różnej postaci, ale przy spełnieniu wymaganych założeń wszystkie uzyskane estymatory są nieobciążone i zgodne. Optymalna macierz $ W_n $ jest proporcjonalna do odwrotności macierzy wariancji i kowariancji parametrów. W szczególnym przyadku, jeśli przyjmiemy, że składnik losowy jest sferyczny, uzyskujemy optymalną postać $ W_n $
\begin{gather*}
W_n^{opt}=\left(\tfrac{1}{n}Z'Z\right)^{-1}
\end{gather*}
Podstawiając optymalną macierz wag do estymatora, uzyskujemy estymator
\begin{gather*}
\hat \beta_{GIV}=\left(X'Z\left(\tfrac{1}{n}Z'Z\right)^{-1}Z'X\right)^{-1}X'Z\left(\tfrac{1}{n}Z'Z\right)^{-1}Z'y
\end{gather*}
Estymator ten, przyjmuje różną postać, w zależności od wybranej macierzy wag.
\section{Estymator potrójnej metody najmniejszy kwadratów (3MNK)}
\begin{itemize}
\item Estymacja parametrów równania za pomocą KMNK abstrahuje od endogeniczności niektórych zmiennych objaśniających
\item 2MNK uwzględnia endogeniczność, lecz za jej pomocą estymujemy parametry każdego równania osobno
\item Korelacje między składnikami losowymi poszczególnych równań nie zostają uwzględnione w procesie estymacji
\item Wady te w modelu nie będzie, gdy przeprowadzimy łączną estymację parametrów wszystkich równań, uwzględniając korelacje składników losowych poszczególnych równań
\end{itemize}
Potraktujemy model wielorównaniowy jako macierzowy model jednorównaniowy. Zapisujemy nasz $ n $-wymiarowy model jako
\begin{align*}
y&_1=\tilde{Z}_1\cdot\tilde{F}_1+\varepsilon_1\\
y&_2=\tilde{Z}_2\cdot\tilde{F}_2+\varepsilon_2\\
&\vdots\\
y&_m=\tilde{Z}_m\cdot\tilde{F}_m+\varepsilon_m
\end{align*}
gdzie $ \tilde{Z}_i=\begin{bmatrix}
\tilde{Y}_i&\tilde{Z}_i
\end{bmatrix} $ - wektor zmiennych objaśniających (endo- i egzogenicznych) w $ n $-tym równaniu modelu.\\
Niech $ \tilde{F}_j^{2MNK} $ - wektor oszacowań parametrów $ j $-tego równania za pomocą 2MNK.\\
Znając go, możemy oszacować macierz wariancji-kowariancji wektora składników losowych modelu
\begin{gather*}
\text{Var}(\varepsilon_t)=\mathbb E \left(\varepsilon_t\varepsilon_t^T\right)\\
\hat \Sigma=
\begin{bmatrix}
\tilde{\sigma_{ij}}
\end{bmatrix}\\
\tilde{\sigma_{ij}}=\left(y_i-Z_i\tilde{F}_i^{2MNK}\right)\left(y_i-Z_i\tilde{F}_i^{2MNK}\right)^T/T
\end{gather*}
gdzie $ T $ - liczba obserwacji.\\
Alternatywnie zamiast $ T $, można podzielić sumę iloczynów w liczniku przez średnią geometryczną liczbę stopni swobody obu równań $ i $ oraz $ j $
\begin{gather*}
\sqrt{\left(T-\tilde{M}_i-\tilde{K}_i\right)\left(T-\tilde{M}_j-\tilde{K}_j\right)}
\end{gather*}
Zapisujemy cały model w jednym równaniu macierzowym
\begin{gather*}
\underset{y}{\underbrace{\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_m
\end{bmatrix}}}
=
\underset{Z}{
\underbrace{\begin{bmatrix}
	\tilde Z_1 & 0          & \dots  & 0          \\
	0          & \tilde Z_2 & \dots  & 0          \\
	\vdots     & \vdots     & \ddots & \vdots     \\
	0          & 0          & \dots  & \tilde Z_m
\end{bmatrix}}}
\underset{F}{\underbrace{\begin{bmatrix}
\tilde F_1\\
\tilde F_2\\
\vdots\\
\tilde F_m
\end{bmatrix}}}
+
\underset{\varepsilon}{\underbrace{\begin{bmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_m
\end{bmatrix}}}
\end{gather*}
Przy czym $ \varepsilon-1,\varepsilon-2,\dots$ są wektorami pionowymi o wymiarach $ T\times 1$
\subsection{Kolejne kroki w estymacji 3MNK}
Krok 1. Estymacja w postaci zredukowanej $ (MNK) $ i obliczenie wartości teoretycznych dla równania $ j $
\begin{gather*}
\hat{ \tilde Z}_j=X\Pi_j=X\left(X^TX\right)^{-1}X^T\tilde Z_j
\end{gather*}
Macierzowo dla całego systemu
\begin{gather*}
\begin{bmatrix}
	X\left(X^TX\right)^{-1}X^T\tilde Z_1 & 0                                    & \dots  & 0                                    \\
	0                                    & X\left(X^TX\right)^{-1}X^T\tilde Z_2 & \dots  & 0                                    \\
	\vdots                               & \vdots                               & \ddots & \vdots                               \\
	0                                    & 0                                    & \dots  & X\left(X^TX\right)^{-1}X^T\tilde Z_m
\end{bmatrix}
\end{gather*}
lub inaczej
\begin{gather*}
\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z
\end{gather*}
Krok 2. Estymacja w postaci strukturalnej parametrów pojedynczych równań (2MNK). Wartości z próby w KMNK zamienione na wartości teoretyczne
\begin{gather*}
\hat F^{2MNK}=\left(\hat Z^T\hat Z\right)\hat Z^Ty
\end{gather*}
Krok 3. Uwzględnienie jednoczesnych korelacji składników losowych w procesie estymacji. Jeżeli poszczególne składniki losowe są sferyczne, to każdy taki wektor ma macierz wariancji-kowariancji postaci
\begin{gather*}
\begin{bmatrix}
	\sigma _{jj}^2 & 0            & \dots & 0            \\
	0            & \sigma _{jj}^2 & \dots & 0            \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	0            & 0            & \dots & \sigma _{jj}^2
\end{bmatrix}
\end{gather*}
\begin{gather*}
\left(
\begin{matrix}
\begin{matrix}
	\sigma ^2 _{11} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{11} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{11} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{11}
\end{matrix}
&
\begin{matrix}
	\sigma ^2 _{12} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{12} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{12} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{12}
\end{matrix}
& \dots &
\begin{matrix}
	\sigma ^2 _{1m} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{1m} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{1m} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{1m}
\end{matrix}
\\
\begin{matrix}
	\sigma ^2 _{21} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{21} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{21} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{21}
\end{matrix}
&
\begin{matrix}
	\sigma ^2 _{22} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{22} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{22} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{22}
\end{matrix}
& \dots &
\begin{matrix}
	\sigma ^2 _{2m} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{2m} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{2m} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{2m}
\end{matrix}
\\
 \vdots & \vdots & \ddots & \vdots \\
\begin{matrix}
	\sigma ^2 _{m1} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{m1} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{m1} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{m1}
\end{matrix}
&
\begin{matrix}
	\sigma ^2 _{m2} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{m2} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{m2} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{m2}
\end{matrix}
& \dots &
\begin{matrix}
	\sigma ^2 _{mm} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{mm} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{mm} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{mm}
\end{matrix}
\\
\end{matrix}
\right)
\end{gather*}
Wykorzystanie iloczynu Kroneckera w 3MNK
\begin{gather*}
\begin{bmatrix}
	a_{11} & a_{12} \\
	a_{21} & a_{22}
\end{bmatrix}
\otimes B=
\begin{bmatrix}
	a_{11}B & a_{12}B \\
	a_{21}B & a_{22}B
\end{bmatrix}
\end{gather*}
Macierz wariancji-kowariancji składników reszt z 2MNK
\begin{gather*}
\begin{bmatrix}
	\sigma ^2 _{11} & \sigma ^2 _{12} & \dots  & \sigma ^2 _{1m} \\
	\sigma ^2 _{21} & \sigma ^2 _{22} & \dots  & \sigma ^2 _{2m} \\
	\vdots          & \vdots          & \ddots & \vdots          \\
	\sigma ^2 _{m1} & \sigma ^2 _{m2} & \dots  & \sigma ^2 _{mm}
\end{bmatrix}
\otimes
\begin{bmatrix}
	1      & 0      & \dots  & 0      \\
	0      & 1      & \dots  & 0      \\
	\vdots & \vdots & \ddots & \vdots \\
	0      & 0      & \dots  & 1
\end{bmatrix}_{T\times T}=\Sigma \otimes I_T
\end{gather*}
W praktyce korzystamy z 2MNK i otrzymujemy wektor wartości teoretycznych i reszt dla każdego z równań osobno
\begin{gather*}
\forall_{j=1,\dots,m}\hat Y_j\hat \varepsilon_j
\end{gather*}
Obliczamy kowariancje między resztami losowymi poszczególnych równań
\begin{gather*}
\hat \sigma_{ij}=
\frac{\hat \varepsilon_i^T\hat \varepsilon_j}{T}
\end{gather*}
\begin{gather*}
\hat \Sigma=
\begin{bmatrix}
	\hat \sigma ^2 _{11} & \hat \sigma ^2 _{12} & \dots  & \hat \sigma ^2 _{1m} \\
	\hat \sigma ^2 _{21} & \hat \sigma ^2 _{22} & \dots  & \hat \sigma ^2 _{2m} \\
	\vdots               & \vdots               & \ddots & \vdots               \\
	\hat \sigma ^2 _{m1} & \hat \sigma ^2 _{m2} & \dots  & \hat \sigma ^2 _{mm}
\end{bmatrix}
\end{gather*}
Jeżeli znamy macierz wariancji-koawriancji całego wektora składników losowych po oszacowaniu $ \hat \Sigma $, to możemy zastosować UMNK w odniesieniu do modelu $ yZF+\varepsilon,\varepsilon\sim(o,\Omega) $
\begin{gather*}
\tilde{F}^{UMNK}=
\left(Z^T\Omega^{-1}Z\right)^{-1}
Z^T\Omega^{-1}y
\end{gather*}
Przy jednoczesnym zastosowaniu 2MNK zastępujemy w powyższym wzorze $ Z $ wartościami teoretycznymi z kroku 1.
\begin{gather*}
\hat F^{UMNK}=
\left(\hat Z^T\Omega^{-1}\hat Z\right)^{-1}
\hat Z^T\Omega^{-1}y
\end{gather*}
Skoro $ \hat \Omega=\hat \Sigma\otimes I_T $ oraz $ \hat Z=\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z $, to
\begin{gather*}
\tilde{F}^{UMNK}
=\\=
\left[Z^T\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}
\left(\hat \Sigma\otimes I_T\right)^{-1}
\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z\right]^{-1}
\cdot\\\cdot
Z^T\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}^T
\left(\hat \Sigma\otimes I_T\right)^{-1}y
\end{gather*}
Z własności iloczynu Kroneckera
\begin{gather*}
\tilde{F}^{UMNK}
=\\=
\left[^ZT\left\{\hat \Sigma^{-1}\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z\right]^{-1}
Z^T
\left\{\hat \Sigma^{-1}\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}y
\end{gather*}
\subsection{Własności estymatora 3MNK}
\begin{enumerate}
\item Jest zgodny
\item Jest asymptotycznie najlepszy w pewnej klasie estymatorów zmiennych instrumentalnych (do której należy 2MNK), więc jest bardziej efektywny od 2MNK.
\item Jeżeli założymy normalność $ \varepsilon $, estymator jest asymptotycznie  równoważny estymatorowi MNK z pełną informacją (przedstawionej dalej): 3MNK ma ten sam rozkład asymptotyczny, ale nie oznacza to numerycznej równoważności oszacowań w skończonej próbie.
\end{enumerate}
\subsection{Szczególne przypadki 3MNK}
\begin{itemize}
\item W sytuacji, gdy wiem, \textit{a priori}, że $ \Sigma $ jest macierzą diagonalną, czyli równoczesne kowariancje składników losowych są zerowe, metoda 3MNK oczywiście nie poprawia efektywności. W takim przypadku 3MNK sprowadza się do 2MNK. Taki sąd \textit{a priori} jest jednak dość mocny i w praktyce zakładamy, raczej niezerowe równoczesne kowariancje.
\item Kiedy wszystkie równania są jednoznacznie identyfikowalne, 3MNK sprowadza się do 2MNK.
\end{itemize}
\section{Estymator metody największej wiarygodności z pełną informacją (MNWPO,FIML)}
Układ równań w postaci strukturalnej $ Ay+BX=\varepsilon $. Składnik losowy postaci zredukowanej ma macierz wariancji-kowariancji $ \hat \Sigma $. Jeżeli poszczególne wektory składnika losowego z różnych okresów (i) są niezależne i (ii) mają wielowymiarowy rozkład normalny, to ich łączna funkcja gęstości
\begin{gather*}
L_\varepsilon=\left(2\pi \right)^{-\frac{mT}{2}}\left|\hat \Sigma \otimes I_T\right|^{-\frac{1}{2}}\exp \left(-\tfrac{1}{2}\varepsilon^T\left(\hat\Sigma\otimes I_T\right)^{-1}\varepsilon\right)
\end{gather*}
Funkcja gęstości obserwacji i składnika losowego zależą od siebie $ L_y=L_\varepsilon\cdot \left|\det A\right| $
\begin{gather*}
\left(2\pi \right)^{-\frac{mT}{2}}\left|\hat \Sigma \otimes I_T\right|^{-\frac{1}{2}}
\left|\det A\right|
\exp\left(-\tfrac{1}{2}\left(y-\tilde{Z}\tilde{F}\right)^T\left(\hat \Sigma\otimes I_T\right)^{-1}\left(y-\tilde{Z}\tilde{F}\right)\right)
\end{gather*}
Po zlogarytmowaniu
\begin{gather*}
L_y=-\tfrac{mT}{2}\ln (2\pi)-\tfrac{T}{2}\ln \left|\Sigma\right|+T\cdot\ln\left|\det A\right|
-\\-
\frac{1}{2}
\left(\left(y-\tilde{Z}\tilde{F}\right)^T\left(\hat \Sigma\otimes I_T\right)^{-1}\left(y-\tilde{Z}\tilde{F}\right)\right)
\end{gather*}
Oszacowanie FIML otrzymamy maksymalizując powyższe wyrażenie ze względu na niezerowe wartości parametrów modelu.
\subsection{Estymacja parametrów modelu liniowego estymatorem uogólnionym Aitkena}
W sytuacji, w której niesferyczność składników zakłócających, przejawiająca się zmiennością wariancji, bądź kowariancji w czasie składników zakłócających, nie jest wynikiem błędów konstrukcji modelu estymator KMNK nie jest efektywny.
Zdefiniujmy strukturę stochastyczną modelu, w którym składniki zakłócające nie są sferyczne
\begin{gather*}
\mathbb E \varepsilon=0\\
\mathbb E \varepsilon\varepsilon'=\sigma^2_\varepsilon\Omega,
\end{gather*}
gdzie $ \Omega $ jest $ T\times T $ wymiarową macierzą, określoną dodatnio
\begin{gather*}
\mathbb E X'\varepsilon=0
\end{gather*}
Jeżeli składniki zakłócające są nieskorelowane w czasie, ale mają zmienne w czasie wariancje, wtedy macierz $ \Omega $ ma postać
\begin{gather*}
\Omega=\begin{bmatrix}
	\omega _{11} & 0            & \ldots & 0            \\
	0            & \omega _{22} & \ldots & 0            \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	0            & 0            & \ldots & \omega _{TT}
\end{bmatrix}
\end{gather*}
gdzie $ \sigma^2_\varepsilon\omega_{tt} $ jest wariancją jednoczesnych składników zakłócających.\\
Jeśli składniki zakłócające maja stałe wariancje, ale są skorelowane w czasie, wtedy macierz $ \Omega $ może być zapisana następująco
\begin{gather*}
\Omega=\begin{bmatrix}
	1            & \omega _{12} & \ldots & \omega _{1T} \\
	\omega _{21} & 1            & \ldots & \omega _{2T} \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	\omega _{T1} & \omega _{T2} & \ldots & 1
\end{bmatrix}
\end{gather*}
gdzie $ \sigma^2_\varepsilon\omega_{ij} $ jest kowariancją niejednoczesnych składników zakłócających modelu.\\
W przypadku, gdy występuje zarówno skorelowanie w czasie, jak również zmienność wariancji, wtedy
\begin{gather*}
\Omega=\begin{bmatrix}
	\omega _{11} & \omega _{12} & \ldots & \omega _{1T} \\
	\omega _{21} & \omega _{22} & \ldots & \omega _{2T} \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	\omega _{T1} & \omega _{T2} & \ldots & \omega _{TT}
\end{bmatrix}
\end{gather*}

Rozważmy estymator parametrów strukturalnych $ \hat{\beta} $ modelu $ y=X\beta+\varepsilon $, w którym składniki zakłócające są niesferyczne i który minimalizuje uogólnioną sumę kwadratów reszt
\begin{gather*}
\tilde S=\tilde \varepsilon'\Omega^{-1}\tilde \varepsilon
\end{gather*}
gdzie $ \tilde \varepsilon=y-X\tilde \beta $ jest wektorem reszt. Rozwijając otrzymujemy
\begin{gather*}
\tilde S=y'\Omega^{-1}y-2\tilde \beta'X'\Omega^{-1}y+\tilde \beta'X'\Omega^{-1}X\tilde \beta.
\end{gather*}
Szukać będziemy takiego $ \tilde \beta $, dla którego zachodzi
\begin{gather*}
\tilde S\underset{\tilde \beta}{\to} \min
\end{gather*}
Obliczamy wektor pochodnych cząstkowych $ \tilde S $ względem $ \tilde \beta $ i przyrównujemy go od zera. Otrzymamy w ten sposób
\begin{gather*}
\frac{\partial \tilde S}{\partial \tilde \beta}=-2X'\Omega^{-1}y+2X'\Omega^{-1}X\tilde \beta =0
\end{gather*}
W konsekwencji mozemy zapisać tzw. uogólniony układ równań normalnych
\begin{gather*}
X'\Omega^{-1}X\tilde \beta=X'\Omega^{-1}y
\end{gather*}
którego rozwiązaniem jest, jeśli $ X $ jest macierzą o pełnym rzędzie kolumnowym, tak, że $ \left|X'\Omega^{-1}X\right|>0 $, estymator uogólnionej metody najmniejszych kwadratów, zwany estymatorem Aitkena i mającym postać
\begin{gather*}
\tilde \beta=\left(X'\Omega X\right)^{-1}X'\Omega^{-1}y
\end{gather*}
Ponieważ macierz drugich pochodnych $ \tilde S $ względem $ \tilde \beta  $, tj.
\begin{gather*}
\frac{\partial ^2\tilde S}{\partial \tilde \beta \partial \beta'}=2X'\Omega^{-1}X
\end{gather*}
jest macierzą określoną dodatnio, zatem estymator Aitkena rzeczywiście minimalizuje uogólnioną sumę kwadratów reszt.

Estymator $ \tilde \beta  $ należy do klasy estymatorów liniowych, który można zapisać jako
\begin{gather*}
\tilde \beta=\tilde C'y
\end{gather*}
gdzie $ \tilde C'=\left(X'\Omega X\right)^{-1}X'\Omega^{-1} $ jest macierzą $ (K+1)\times T $ wymiarową.
Jeśli elementy macierzy $ X $ są nielosowe lub losowe, ale nieskorelowane ze składnikami zakłócającymi modelu, estymator Aitkena jest nieobciążony. Możemy zapisać, że
\begin{gather*}
\tilde \beta=\beta+\tilde C'\varepsilon
\end{gather*}
dlatego, że spełniony jest warunek konieczny nieobciążoności, tj.
\begin{gather*}
\tilde C'X=I_{K+1}
\end{gather*}
Zatem możemy zapisać, że
\begin{gather*}
\mathbb E \tilde \beta=\beta 
\end{gather*}
Można udowodnić, zakładają nielosowość macierzy $ X $, że macierz wariancji-kowariancji błędów estymacji w tym przypadku jest równa
\begin{gather*}
\Sigma^2_{\tilde {\beta }}=
\mathbb E \left(\tilde \beta-\beta\right)\left(\tilde \beta-\beta\right)'=
\mathbb E \tilde C'\varepsilon\left(\tilde C'\varepsilon\right)'=
\tilde C'\sigma^2_\varepsilon\Omega\tilde C=
\sigma^2_\varepsilon\tilde C'\Omega\tilde C
\end{gather*}
Znając postać macierzy $ \tilde C' $ możemy wykazać, że
\begin{gather*}
\Sigma_{\tilde \beta}=\sigma^2_\varepsilon \left(X'\Omega^{-1}X\right)^{-1}
\end{gather*}
Estymator Aitkena jest najlepszym liniowym, nieobciążonym estymatorem w klasie liniowych estymatorów dla modelu z niesferycznymi składnikami zakłócającymi. Zatem dowolny estymator nieobciążony typu
\begin{gather*}
\tilde \beta_D=\tilde C'_Dy
\end{gather*}
dla którego zachodzi $ \tilde C_DX=I_{K+1} $, charakteryzuje się macierzą wariancji-kowariancji
\begin{gather*}
\Sigma^2_{\tilde {\beta_D }}=
\mathbb E \left(\tilde \beta_D-\beta\right)\left(\tilde \beta_D-\beta\right)'=
\sigma^2_\varepsilon\tilde C_D'\Omega\tilde C_D
\end{gather*}
której elementy są nie mniejsze niż macierz wariancji-kowariancji estymatora Aitkena.
Jeśli macierz $ \Omega $ jest określona dodatnio, wtedy istnieje $ T\times T $ wymiarowa macierz $ P $ spełniająca następujące własności
\begin{gather*}
P'P=\Omega^{-1}\\
P\Omega\Omega'=I_T
\end{gather*}
gdzie $ I_T $ jest macierzą jednostkową stopnia $ T $.
Macierz $ P $ nazywać będziemy macierzą transformacji. Macierz ta umożliwia ukazanie estymatora Aitkena jako specyficznego estymatora MNK, dla transformowanych obserwacji zmiennych modelu. Niech pierwotny model
\begin{gather*}
y=X\beta+\varepsilon
\end{gather*}
zostanie przemnożony przez macierz $ P $ tak, że
\begin{gather*}
Py=PX\beta+P\varepsilon
\end{gather*}
Ponieważ macierz $ P $ ma nielosowe elementy, zatem jest prawdą, że
\begin{align*}
&\mathbb E P\varepsilon=0\\
&\mathbb E P\varepsilon\left(P\varepsilon\right)'-\sigma_\varepsilon^2 P\Omega\Omega'=\sigma_\varepsilon^2 I_T
\end{align*}
Widzimy zatem, że składniki zakłócające modelu transformowanego maja sferyczne składniki zakłócające i parametry tego modelu można szacować metodą najmniejszych kwadratów. Niech
\begin{gather*}
Y^*=X^*\beta+\varepsilon^*
\end{gather*}
gdzie
\begin{align*}
&Y^*=Py\\
&X^*=PX\\
&\varepsilon^*=P\varepsilon
\end{align*}
Stosując MNK modelu, w którym pierwotne obserwacje zostały zważone elementami macierzy $ \Omega $ jest równoważne estymacji uogólnioną metodą najmniejszych kwadratów.
\subsection{Przypadek zmienności wariancji}
Dla przypadku składników zakłócających o zmiennych wariancjach z macierzą $ \Omega $ o postaci
\begin{gather*}
\Omega=
\begin{bmatrix}
	\omega _{11} & 0             & \ldots & 0             \\
	0             & \omega _{22} & \ldots & 0             \\
	\vdots        & \vdots        & \ddots & \vdots        \\
	0             & 0             & \ldots & \omega _{TT}
\end{bmatrix}
\end{gather*}
można wykazać, że
\begin{align*}
\Omega^{-1}=
\begin{bmatrix}
	\frac{1}{\omega _{11}} & 0                       & \ldots & 0                       \\
	0                       & \frac{1}{\omega _{22}} & \ldots & 0                       \\
	\vdots                  & \vdots                  & \ddots & \vdots                  \\
	0                       & 0                       & \ldots & \frac{1}{\omega _{TT}}
\end{bmatrix}
&&
P=
\begin{bmatrix}
	\frac{1}{\sqrt{\omega _{11}}} & 0                       & \ldots & 0                       \\
	0                       & \frac{1}{\sqrt{\omega _{22}}} & \ldots & 0                       \\
	\vdots                  & \vdots                  & \ddots & \vdots                  \\
	0                       & 0                       & \ldots & \frac{1}{\sqrt{\omega _{TT}}}
\end{bmatrix}
\end{align*}
Macierze te spełniają równania
\begin{align*}
P'P=\Omega^{-1}
&&
P\Omega\Omega'=I_T
\end{align*}
\subsection{Przypadek autokorelacji}
W przypadku, gdy składniki zakłócające są skorelowane w czasie i gdy skorelowanie nie jest wynikiem niepoprawnej specyfikacji modelu, staramy się wykorzystać estymator Aitkena w celu eliminacji skutków jej występowania.
W takim przypadku elementy macierzy $ \Omega $ i $ P $ są funkcjami parametrów występujących w procesie generującym skorelowane w czasie składniki losowe. Rozważmy przykład, w którym składniki zakłócające w liniowym modelu.
\begin{gather*}
y_t\beta_{0}+\beta_{1}x_{1t}+\dots+\beta_kx_{tk}+\varepsilon_t
\end{gather*}
są generowane przez stacjonarny proces autoregresyjny rzędu pierwszego.
\begin{gather*}
\varepsilon_t=\rho _1\varepsilon_{t-1}+\eta_t
\end{gather*}
gdzie
\begin{itemize}
\item $ |\rho _1|<1 $ jest współczynnikiem autokorelacji rzędu pierwszego
\item $ \eta_t $ jest czysto losowym (sferycznym) składnikiem zakłócającym, spełniającym następujące warunki
\begin{itemize}
\item $ \mathbb E \eta_t=0 $
\item $ \mathbb E \eta^2_t=\sigma_\eta^2 $
\item $ \mathbb E \eta_t\eta_s=0 $ dla $ t\neq s $
\end{itemize}
\end{itemize}
Rozważmy jakie parametry rozkładu charakteryzować będą kolejne składniki zakłócające $ \varepsilon_t $, generowane przez model autoregresyjny. Przedstawiamy obecnie składnik zakłócający jako funkcję wszystkich przeszłych i bieżących zakłóceń $ \eta_t $. Drogą kolejnych podstawień otrzymamy
\begin{align*}
\varepsilon_t =&\rho _1\underbrace{\left(\rho _1\varepsilon_{t-2}+\eta_{t-1}\right)}_{\varepsilon_{t-1}}+\eta_t
=
\rho _1^2\varepsilon_{t-2}+\rho _1\eta_{t-1}+\eta_t \\
\varepsilon_t =&\rho _1^2\underbrace{\left(\rho _1\varepsilon_{t-3}+\eta_{t-2}\right)}_{\varepsilon_{t-2}}+\rho _1\eta_{t-1}+\eta_t
=
\rho _1^3\varepsilon_{t-3}+\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\\
& \vdots\\
\varepsilon_t =&\rho _1^{s+1}\varepsilon_{t-s-1}+\rho _1^{s}\eta_{t-s}+\dots +\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\end{align*}
Jeśli proces generujący składniki zakłócające jest stacjonarny, tj $ |\rho _1|<1 $ wtedy
\begin{gather*}
\rho _1^{s+1}\varepsilon_{t-s-1}\to0 
\end{gather*}
i w postaci końcowej
\begin{gather*}
\varepsilon_t =\rho _1^{s}\eta_{t-s}+\dots +\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\end{gather*}
Składnik losowy $ \varepsilon_t $ jest przedstawiony jako ważona funkcja wszystkich (przyszłych i bieżącego) sferycznych składników $ \eta $. Ponieważ parametry rozkładu $ \eta_t $ są zadane, zatem można wyznaczyć parametry rozkładu $ \varepsilon_t $. W przypadku wariancji możemy zapisać, że
\begin{gather*}
\mathbb E \varepsilon_t^2=
\mathbb E \bigl(
\rho _1^{s}\eta_{t-s}+\dots +\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\bigr)^2
=\\=
\rho_1^{2s}\mathbb E \eta_{t-s}^2+\dots
+
\rho_1^{4}\mathbb E \eta_{t-2}^2+
\rho_1^{2}\mathbb E \eta_{t-1}^2+
\mathbb E \eta_t^2
\end{gather*}
gdzie wszystkie kowariancje niejednoczesnych składników zakłócających $ \eta_t,\eta_s $, dla $ t\neq s $ przyrównano do zera. W konsekwencji można zapisać, że
\begin{gather*}
\mathbb E \varepsilon_t^2=
\eta _1^{2s} \sigma_\eta^2+\dots +\eta _1^4 \sigma_\eta^2+\eta _1^2 \sigma_\eta^2+\sigma_\eta^2
=
\sigma_\eta^2
\left(\eta _1^{2s} +\dots +\eta _1^4 +\eta _1^2 +1\right)
\end{gather*}
ponieważ suma elementów w nawiasie stanowi szereg geometryczny zbieżny o ilorazie $ \rho_1^2 $, zatem jego suma jest równa $ \frac{1}{1-\rho_1^2} $. Ostatecznie więc mamy:
\begin{gather*}
>E\varepsilon_t^2=\sigma_\eta^2 \cdot \frac{1}{1-\rho_1^2}
=
\sigma_\varepsilon^2
\end{gather*}
Z powyższego zapisu widać, że skorelowane w czasie składniki zakłócające mają stałe w czasie wariancje. Aby obliczyć kowariancje przeprowadzimy niestępującą procedurę kolejnych podstawień
\begin{align*}
\varepsilon_{t+1}=&
\rho_1\varepsilon_t+\eta_{t+1}
\\
\varepsilon_{t+2}=&
\rho_1
\underbrace{\left(
\rho_1\varepsilon_t+\eta_{t+1}
\right)}_{\varepsilon_{t+1}}
+\eta_{t+2}
=
\rho_1^2\varepsilon_t+\rho_1\eta_{t+1}+\eta_{t+2}
\\
\varepsilon_{t+3}=&
\rho_1
\underbrace{\left(
\rho_1^2\varepsilon_t+\rho_1\eta_{t+1}+\eta_{t+2}
\right)}_{\varepsilon_{t+2}}
+\eta_{t+3}
=
\rho_1^3\varepsilon_t+\rho_1^2\eta_{t+1}+\rho_1\eta_{t+2}+\eta_{t+3}
\\
&\vdots\\
\varepsilon_{t+\tau}=&
\rho_1^\tau \varepsilon_t+\rho_1^{\tau-1}\eta_{t+1}+\dots+\rho_1^2\eta_{t+\tau-2}+\rho_1\eta_{t+\tau-1}
\\
\end{align*}
Mnożąc obie strony ostatniej równości przez $ \varepsilon_t $ i obliczając wartość oczekiwaną otrzymamy
\begin{gather*}
\mathbb E \varepsilon_t\varepsilon_{t+\tau}=
\rho_1^\tau \mathbb E \varepsilon_t^2
+
\rho_1^{\tau -1}\mathbb E \varepsilon_t\eta_{t+1}
+\dots+
\rho_1^2\mathbb E \varepsilon_t\eta_{t+\tau-1}
+
\mathbb E \varepsilon_t\eta_{t+\tau}
=
\rho_1^\tau \sigma_\varepsilon^2
\end{gather*}
Ponieważ $ \varepsilon_t $ zależny tylko od opóźnionych i bieżących składników zakłócających $ \eta_{t-i} $ oraz $ \eta_t $, nie zależy natomiast od przyszłych składników $ \eta_{t+j} $. Jeśli składniki zakłócające generowane są przez proces autoregresyjny rzędu pierwszego, to prawdziwe są następujące własności:
\begin{align*}
&\mathbb E \varepsilon_t>0\\
&\mathbb E \varepsilon_t^2=\sigma_\varepsilon^2=\frac{\sigma_\eta^2}{1-\rho_1^2}\\
&\mathbb E\varepsilon_t\varepsilon_{t+\tau}=\rho_1^\tau\sigma_\varepsilon^2 
\end{align*}
Możemy pokazać, że $ T\times T $ macierz wariancji-kowariancji składników zakłócających $ \sigma_\varepsilon^2\Omega $ ma elementy będące funkcjami współczynnika $ \rho_1 $. Możemy zapisać, że
\begin{align*}
\Omega=
&\begin{bmatrix}
	1         & \rho _1   & \rho _1^2 & \rho _1^3 & \ldots & \rho _1^5 \\
	\rho _1   & 1         & \rho _1   & \rho _1^2 & \ldots & \rho _1^4 \\
	\rho _1^2 & \rho _1   & 1         & \rho _1   & \ldots & \rho _1^3 \\
	\rho _1^3 & \rho _1^2 & \rho _1   & 1         & \ldots & \rho _1^2 \\
	\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots    \\
	\rho _1^5 & \rho _1^4 & \rho _1^3 & \rho _1^2 & \ldots & 1
\end{bmatrix}
\\
\Omega^{-1}=
\frac{1}{1-\rho_1^2}
&\begin{bmatrix}
	1        & -\rho _1    & 0           & 0           & \ldots & 0           & 0        \\
	-\rho _1 & 1+\rho _1^2 & -\rho _1    & 0           & \ldots & 0           & 0        \\
	0        & -\rho _1    & 1+\rho _1^2 & -\rho _1    & \ldots & 0           & 0        \\
	0        & 0           & -\rho _1    & 1+\rho _1^2 & \ldots & 0           & 0        \\
	\vdots   & \vdots      & \vdots      & \vdots      & \ddots & \vdots      & \vdots   \\
	0        & 0           & 0           & 0           & \ldots & 1+\rho _1^2 & -\rho _1 \\
	0        & 0           & 0           & 0           & \ldots & -\rho _1    & 1
\end{bmatrix}
\end{align*}
Macierz transformacji $ P $ może być przedstawiona w postaci
\begin{gather*}
P=
\frac{1}{\sqrt{1-\rho_1^2}}
\begin{bmatrix}
	\sqrt{1-\rho_1^2} & 0        & 0      & \ldots & 0        & 0      \\
	-\rho _1          & 1        & 0      & \ldots & 0        & 0      \\
	0                 & -\rho _1 & 1      & \ldots & 0        & 0      \\
	\vdots            & \vdots   & \vdots & \ddots & \vdots   & \vdots \\
	0                 & 0        & 0      & \ldots & 1        & 0      \\
	0                 & 0        & 0      & \ldots & -\rho _1 & 1
\end{bmatrix}
\end{gather*}
Rozważmy równanie $ Py=PX\beta +P\varepsilon $. Ponieważ obie strony tej równości możemy podzielić przez stałą $ \frac{1}{\sqrt{1-\rho_1^2}} $, zatem transformowanymi obserwacjami zmiennej endogenicznej, zmiennych objaśniających oraz składników zakłócających są
\begin{gather*}
\sqrt{1-\rho_1^2}Py=
\begin{bmatrix}
	\sqrt{1-\rho_1^2} & 0        & 0      & \ldots & 0      \\
	-\rho _1          & 1        & 0      & \ldots & 0      \\
	0                 & -\rho _1 & 1      & \ldots & 0      \\
	\vdots            & \vdots   & \vdots & \ddots & \vdots \\
	0                 & 0        & 0      & \ldots & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
	y_1    \\
	y_2    \\
	y_3    \\
	\vdots \\
	y_T
\end{bmatrix}
=
\begin{bmatrix}
	y_1 \sqrt{1-\rho _1^2} \\
	y_2-\rho _1 y_1        \\
	y_3-\rho _1 y_2        \\
	\vdots                 \\
	y_T-\rho _1 y_{T-1}
\end{bmatrix}
\end{gather*}
\begin{gather*}
\sqrt{1-\rho_1^2}Px=
\begin{bmatrix}
	\sqrt{1-\rho_1^2} & 0        & 0      & \ldots & 0      \\
	-\rho _1          & 1        & 0      & \ldots & 0      \\
	0                 & -\rho _1 & 1      & \ldots & 0      \\
	\vdots            & \vdots   & \vdots & \ddots & \vdots \\
	0                 & 0        & 0      & \ldots & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
	  1    & x_{11} & x_{12} & x_{14} & \ldots & x_{1T} \\
	  1    & x_{21} & x_{22} & x_{24} & \ldots & x_{2T} \\
	  1    & x_{31} & x_{32} & x_{34} & \ldots & x_{3T} \\
	\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
	  1    & x_{T1} & x_{T2} & x_{T4} & \ldots & x_{TT}
\end{bmatrix}
=\\=
\begin{bmatrix}
	\sqrt{1-\rho _1^2} & \sqrt{1-\rho _1^2} x_{11} & \sqrt{1-\rho _1^2} x_{12} & \ldots & \sqrt{1-\rho _1^2} x_{1T} \\
	1-\rho _1          & x_{21}-\rho _1 x_{11}     & x_{22}-\rho _1 x_{12}     & \ldots & x_{2T}-\rho _1 x_{1T}     \\
	1-\rho _1          & x_{31}-\rho _1 x_{21}     & x_{32}-\rho _1 x_{22}     & \ldots & x_{3T}-\rho _1 x_{2T}     \\
	\vdots             & \vdots                    & \vdots                    & \ddots & \vdots                    \\
	1-\rho _1          & x_{T1}-\rho _1 x_{(T-1)1} & x_{T2}-\rho _1 x_{(T-1)2} & \ldots & x_{T}-\rho _1 x_{(T-1)T}
\end{bmatrix}
\end{gather*}
Zatem typowe przekształcenia auteregresyjne maja postać
\begin{align*}
	 & \text{dla}t=1         &  & y_1^*=y_1\sqrt{1-\rho_1^2} &  & x_{ti}^*=x_{ti}\sqrt{1-\rho_1^2} &  & i=1,\dots,k \\
	 & \text{dla}t=2,\dots,T &  & y_t^*=y_t-\rho_1y_{t-1}    &  & x_{ti}^*=x_{ti}-\rho_1 x_{t-1,i} &  & i=1,\dots,k
\end{align*}
Jeśli nie przeprowadzone jest autoregresyjne przekształcenie pierwszych obserwacji zmiennych, wtedy zamiast macierzy $ P $ występuje macierz $ Q $ o wymiarach $ )T+1=<times T $
\begin{gather*}
Q=
\begin{bmatrix}
 -\rho _1 & 1 & 0 & \ldots  & 0 \\
 0 & -\rho _1 & 1 & \ldots  & 0 \\
 0 & 0 & -\rho _1 & \ldots  & 0 \\
 \vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
 0 & 0 & 0 & \ldots  & -\rho _1 \\
\end{bmatrix}
\end{gather*}
Macierz ta nie spełnia warunków takich jak $ P $. W ogólnym przypadku zachodzi bowiem, że $ Q'Q\neq P'P=\Omega^{-1} $. Różnica występuje w górnym lewym elemencie obu macierzy.

Nieznany współczynnik $ \rho_1 $ estymujemy następująco
\begin{gather*}
\hat{\rho_1}=\frac{\sum_{t=2}^{T}\varepsilon_t\varepsilon_{t-1}}{\sum_{t=1}^{T}\hat{\varepsilon}^2_t}
\end{gather*}
Utwórzmy macierz $ \hat \Omega $ zgodnie z
\begin{gather*}
\hat \Omega=\begin{bmatrix}
	1                 & \tilde{\rho }_1   & \tilde{\rho }_1^2 & \ldots & \tilde{\rho }_1^{T-1} \\
	\tilde{\rho }_1   & 1                 & \tilde{\rho }_1   & \ldots & \tilde{\rho }_1^{T-2} \\
	\tilde{\rho }_1^2 & \tilde{\rho }_1   & 1                 & \ldots & \tilde{\rho }_1^{T-3} \\
	\vdots            & \vdots            & \vdots            & \ddots & \vdots            \\
	\tilde{\rho }_1^{T-1} & \tilde{\rho }_1^{T-2} & \tilde{\rho }_1^{T-3} & \ldots & 1
\end{bmatrix}
\end{gather*}
Wykorzystując powyższą macierz zdefiniujemy dwukrotnie estymator Aitkena w następujący sposób
\begin{gather*}
\hat{ \tilde\beta}=\left(X'\hat \Omega^{-1}X\right)^{-1}X'\hat \Omega^{-1}y
\end{gather*}
Estymator ten jest przybliżeniem optymalnego estymatora Aitkena za znaną macierzą wariancji-kowariancji zakłóceń modelu. Jego macierz wariancji-kowariancji jest przybliżeniem macierzy wariancji-kowariancji estymatora Aitkena. Można tę macierz zapisać
\begin{gather*}
\Sigma^2_{\hat{\tilde{\beta}}}\approx\left(X'\hat \Omega^{-1}X\right)^{-1}
\end{gather*}
\subsection{Konsekwencje występowania niesferycznych składników losowych}
Istotnym pytaniem jest jakie własności posiada estymator MNK, jeśli prawdziwym modelem generującym obserwacje zmiennej endogenicznej nie jest model klasyczny, lecz uogólniony. Aby to prześledzić rozważmy raz jeszcze estymator MNK zapisany w postaci
\begin{gather*}
\tilde{\beta}=\beta +\left(X'X\right)^{-1}X'\varepsilon
\end{gather*}
Niesferyczność składników losowych nie wpływa na nieobciążenie estymatora MNK, jeśli tylko zmienne objaśniające pozostają nieskorelowane ze składnikami losowymi modelu. Istotne konsekwencje związane są z szacowaniem wariancji składników losowych oraz z szacowaniem macierz wariancji-kowariancji błędów estymacji.
Wariancja reszt oblicza na "klasycznie" jako suma kwadratów reszt podzielona przez liczbę stopni swobody nie jest już nieobciążonym estymatorem wariancji składników losowych. Zauważmy, że wariancja reszt KMNK jest zdefiniowana jako
\begin{gather*}
\tilde{\sigma}^2_\varepsilon=\frac{1}{T-K-1}\sum_{t=1}^{T}\hat{\varepsilon}^2_t=
\frac{1}{T-K-1}\hat{\varepsilon}'\hat{\varepsilon}=
\frac{1}{T-K-1}{\varepsilon}'M{\varepsilon}
\end{gather*}
gdzie
\begin{gather*}
M=I_T-X(X'X)^{-1}X'
\end{gather*}
jest macierzą indepotentną o wymiarach $ T\times T $, taką, że $ M=M' $ oraz $ MM=M $.\\
Suma kwadratów reszt jest kwadratową formą wektora składników losowych. Obliczając jej wartość oczekiwaną, przy wykorzystaniu własności śladu macierzy otrzymujemy
\begin{align*}
&\mathbb E \hat{\varepsilon'}\hat{\varepsilon}
=\\=&
\mathbb E \varepsilon' M\varepsilon
=\\=&
\mathbb E  tr\left(\varepsilon'M\varepsilon\right)
=\\=&
\mathbb E  tr\left(M\varepsilon'\varepsilon\right)
=\\=&
tr\mathbb E  \left(M\varepsilon'\varepsilon\right)
=\\=&
trM\sigma^2_\varepsilon\Omega
=\\=&
\sigma^2_\varepsilon trM\Omega
\end{align*}
Aby wariancja reszt była nieobciążonym estymatorem wariancji składników losowych, wyrażenie powyższe powinno równać się
\begin{gather*}
\mathbb E \hat{\varepsilon}'\hat{\varepsilon}=\sigma^2_\varepsilon\left(T-K-1\right)
\end{gather*}
Jest to możliwe wtedy, gdy $ \Omega=I_T $. W takim przypadku, z uwagi na własności macierz $ M $ zachodzi
\begin{align*}
&trM
=\\=&
tr\left(I_T-X(X'X)^{-1}X'\right)
=\\=&
T-trX(X'X)^{-1}X'
=\\=&
T-tr(X'X)^{-1}X'X
=\\=&
T-trI_{K+1}
=\\=&
T-K-1
\end{align*}
Jeśli zatem $ \Omega\neq I_T $, wtedy $ \mathbb E \hat{\varepsilon}'\hat{\varepsilon}\neq\sigma^2_\varepsilon\left(T-K-1\right) $ i w konsekwencji
\begin{gather*}
\mathbb E \sigma^2_\varepsilon\neq
\mathbb E \tfrac{\hat{\varepsilon}'\hat{\varepsilon}}{T-K-1}=\sigma^2_\varepsilon
\end{gather*}
zatem wariancja reszt jest w takim przypadku obciążonym estymatorem wariancji składników losowych. Jeszcze poważniejszy problem związany jest z macierzą wariancji-kowariancji błędów estymacji. Jest ona równa macierzy $ \Sigma^2_{\hat{\beta}}=\sigma^2_\varepsilon\left(X'X\right) $ tylko wtedy, gdy $ \Omega=I_T $. Jeśli powyższa równość nie zachodzi, wtedy macierz wariancji-kowariancji przyjmuje postać
\begin{gather*}
\Sigma^2_{\hat{\beta}}=
\mathbb E \left(\hat{\beta}-\beta\right)\left(\hat{\beta}-\beta\right)'=
\sigma_\varepsilon^2\left(X'X\right)^{-1}\left(X'\Omega X\right)^{-1}
\end{gather*}
Wykorzystując estymator macierzy wariancji-kowariancji, prawdziwy dla sferycznych składników losowych, tj. $ \hat\Sigma_{\hat{\beta}}=\hat{\sigma}_\varepsilon^2(X'X)^{-1} $, w sytuacji, gdy składniki te nie są sferyczne popełniamy błąd zarówno z tytułu niewłaściwej postaci macierzy wariancji-kowariancji, jak również z tytułu obciążonej estymacji wariancji składników losowych. Na dodatek nie potrafimy w ogólnym przypadku określić jakiego rodzaju obciążenie towarzyszy wykorzystaniu macierzy $ \hat\Sigma_{\hat{\beta}}=\hat{\sigma}_\varepsilon^2(X'X)^{-1} $ sytuacji niesferycznych składników zakłócających. Wykorzystanie tej macierzy prowadzi do wyznaczenia niepoprawnych średnich błędów ocen parametrów strukturalnych.
Sugestia jest zatem następująca:\\
budując model ekonometryczny mależy dążyć do wyeliminowania niesferycznych składników losowych (drogą respecyfikacji modelu) lub jeśli nie jest to możliwe, oszacować parametry modelu UMNK, tj. eliminować skutki występowania niesferycznych składników losowych.
\section{Wybrane jednowymiarowe modele zmiennosci cen i stóp zwrotu}
Analiza szeregów czasowych rynku finansowego o wysokiej częstotliwosci obejmuje
\begin{itemize}
\item cechy analizy krótkookresowej (szeregi z tzw. krótką pamięcią)
\item podstawowy i uogólniony model grupowania wariancji ARCH
\item testowanie efektu ARCH i uogólnionego GARCH
\item niestandardowe modele ARCH (tzw. in mean, z symetrią, E-GARCH)
\item estymację i wykorzystanie modeli klasy GARCH
\end{itemize}
\subsection{Cechy analizy krótkookresowej}
Do cech procesów losowych (najczęściej procesów na rynkach finansowych) charakteryzują się wysoką częstotliwością zalicza się:
\begin{itemize}
\item naprzemienne występowanie okresów o zwiększonej fluktuacji i okresów niskiej zmienności zmiennej finansowe
\item skupiania wariancji w kolejnych jednostkach czasu, tj. dodatniej korelacji w dziedzinie zmienności zmiennej będącej przedmiotem zainteresowania, co przejawia się w wysokiej wariancji zmiennej powodowanej wzrostem tej wariancji w okresie poprzedzającym i analogicznie spadkiem wariancji na skutek niskiej wariancji w okresie poprzedzającym.
\end{itemize}
\subsection{Podstawowy i uogólniony model ARCH}
Rodzaje nieliniowych procesów stochastycznych.\\
W nieliniowej analizie jdenowymiarowych szeregów czasowych poszukuje się funkcji $ f $ wiążącej deny profces z ciagiem niezależnych zmiennych losowych o jednakowym rozkładzie
\begin{gather*}
Y_t=f\left(\varepsilon_t,\varepsilon_{t-1},\varepsilon_{t-2},\dots\right )
\end{gather*}
gdzie $ \varepsilon_t $ jest zmienną losową o średniej zero i jednostkowej wariancji.
Powyższa reprezentacja jest na tyle ogólna, że nie wiadomo jak dobierać postać funkcji $ f $. Najczęściej przyjmuje się, że nieliniowy proces ekonomiczny na postać
\begin{gather*}
Y_t=g\left(\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)+\varepsilon_th\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)
\end{gather*}
Procesy $ Y_t $ wyrażone z nieliniową funkcją $ g $ nazywamy procesami nieliniowymi w warunkowej wariancji.

Powyższa klasyfikacja ma sens, gdyż:
\begin{enumerate}
\item Warunkowa wartość oczekiwana $ Y_t $ możne być zapisana
\begin{gather*}
\mathbb E \left(Y_t\backslash\Omega_{t-1}\right)=g\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)
\end{gather*}
funkcja $ g $ opisuje zmiany wartości średniej procesu $ Y_t $ warunkowo względem informacji z przeszłości (zbiór $ \Omega_{t-1} $ oznacza zbiór wszystkich informacji dostępnych do momentu $ t-1 $).
\item Kwadrat funkcji $ h $ przedstawia zmiany warunkowej wariancji procesu $ Y_t $
\begin{align*}
&D^2\left(Y_t\backslash\Omega_{t-1}\right)
=\\=&
\mathbb E \left[Y_t-\mathbb E \left(Y_t\backslash\Omega_{t-1}\right)^2\backslash\Omega_{t-1}\right]
=\\=&
\mathbb E \left[\varepsilon_t^2 h^2\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)\backslash\Omega_{t-1}\right]
=\\=&
h^2\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)\mathbb E \left(\varepsilon_t^2 \right)
=\\=&
h^2\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)
\end{align*}
\end{enumerate}
Do najbardziej znanych modeli nieliniowych w warunkowej wartości średniej należą: procesy dwuliniowe, nieliniowe procesy autoregresji i średniej ruchomej, autoregresyjne modele progowe, przełącznikowe i wygładzonej przejścia, procesy autogresyjne o losowych współczynnikach. Znanymi procesami o zmiennej wariancji warunkowej są procesy ARCH/GARCH