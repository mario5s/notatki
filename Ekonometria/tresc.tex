\section{Wielowskaźnikowe modele regresji liniowej}
$ G $-równaniowy ekonometryczny model regresji wielorakiej - postać uogólniona.

\subsection{$ G $-równaniowy model regresji wielorakiej (bez restrykcji zerowych):}
\begin{align*}
&t_{t1}=\gamma_{12}t_{t2}+\dots+\gamma_{1G}y_{tG}+\beta_{10}+\beta_{11}x_{t1}+\beta_{12}x_{t2}+\dots+\beta_{1k}x_{tk}+\varepsilon_{t1}\\
&t_{t1}=\gamma_{21}t_{t1}+\gamma_{23}t_{t3}+\dots+\gamma_{2G}y_{tG}+\beta_{20}+\beta_{21}x_{t1}+\beta_{22}x_{t2}+\dots+\beta_{2k}x_{tk}+\varepsilon_{t2}\\
\vdots\\
&t_{tG}=\gamma_{G1}t_{t1}+\gamma_{G2}t_{t2}+\dots+\beta_{G0}+\beta_{G1}x_{t1}+\beta_{G2}x_{t2}+\dots+\beta_{Gk}x_{tk}+\varepsilon_{tG}
\end{align*}

\subsubsection{Syntetyczny zapis macierzowy modelu}
\begin{gather*}
y_t\cdot\Gamma+x_t\cdot B=\varepsilon_t
\end{gather*}
gdzie:\\
$ y_t=\begin{bmatrix}
y_{t1} &y_{t2}& \dots& y_{tG}
\end{bmatrix} $ - wektor zmiennych endogenicznych modelu\\
$ \Gamma=\begin{bmatrix}
\gamma_{G\times G}
\end{bmatrix} $ - macierz parametrów strukturalnych zmiennych endogenicznych\\
$ x_t=\begin{bmatrix}
1& x_{t1} &x_{t2} &\dots& x_{tk}
\end{bmatrix} $ - wektor zmiennych egzogenicznych modelu\\
$ B=\begin{bmatrix}
\beta_{(k+1)\times G}
\end{bmatrix} $ - macierz parametrów strukturalnych zmiennych egzogenicznych\\
$ \varepsilon_t=\begin{bmatrix}
\varepsilon_{t1}& \varepsilon_{t2}& \dots& \varepsilon_{tG}
\end{bmatrix} $ - wektor składników zakłócających modelu
\begin{gather*}
Y\cdot\Gamma+X\cdot B=E
\end{gather*}
$ Y=\begin{bmatrix}
	y_{11} & y_{12} & \dots  & y_{1G} \\
	y_{21} & y_{22} & \dots  & y_{2G} \\
	\vdots & \vdots & \ddots & \vdots \\
	y_{n1} & y_{n2} & \dots  & y_{nG}
\end{bmatrix} 
=Y_{n\times G}$
\begin{minipage}[t]{.5\textwidth}
- macierz obserwacji endogenicznych modelu
\end{minipage}\\
$ X=\begin{bmatrix}
	1 & x_{11} & x_{12} & \dots  & x_{1k} \\
	1 & x_{21} & x_{22} & \dots  & x_{2k} \\
	1 & \vdots & \vdots & \ddots & \vdots \\
	1 & x_{n1} & x_{n2} & \dots  & x_{nk}
\end{bmatrix} 
=X_{n\times(k+1)} $
\begin{minipage}[t]{.45\textwidth}
- macierz obserwacji zmiennych egzogenicznych modelu\\
\end{minipage}
$ E=\begin{bmatrix}
	\varepsilon_{11} & \varepsilon_{12} & \dots  & \varepsilon_{1G} \\
	\varepsilon_{21} & \varepsilon_{22} & \dots  & \varepsilon_{2G} \\
	\vdots & \vdots & \ddots & \vdots \\
	\varepsilon_{n1} & \varepsilon_{n2} & \dots  & \varepsilon_{nG}
\end{bmatrix} 
=E_{n\times G}$
\begin{minipage}[t]{.5\textwidth}
- macierz składników zakłócających $ G $-równaniowego modelu
\end{minipage}

\subsection{Wstępne założenia dotyczące struktury stochastycznej modelu wielorównaniowego.}
\begin{align*}
\mathbb E \varepsilon_{tj}=0&&t=1,2,\dots,n&&j=1,2,\dots,G\\
\mathbb E \varepsilon=0
\end{align*}
co oznacza, że składnik zakłócający jest zmienną losową o wartości oczekiwanej równe 0, w każdym okresie $ t $ i w każdym równaniu $ j $-tym.
\begin{gather*}
\mathbb E \varepsilon^2_{tj}=\sigma^2_{\varepsilon j}=\text{const.}\qquad j=1,2,\dots,G
\end{gather*}
co oznacza, że w każdym równaniu $ j $-tym wariancja składnika losowego jest stała i niezależna od okresu obserwacji.
\begin{gather*}
\mathbb E \varepsilon_{tj}\varepsilon_{(t-s)j}=0\qquad,(s\neq0)
\end{gather*}
co oznacza, że kowariancja między składnikami losowymi w poszczególnych równaniach dla różnych okresów jest równa 0.
{Klasyfikacja modeli z punktu widzenia powiązań pomiędzy zmiennymi endogenicznymi.}
\subsubsection{Model prosty}

Model prosty jest to model, w którym nieopóźnione zmienne endogeniczne nie oddziałują na siebie. Z uwagi na brak powiązań pomiędzy nieopóźnionymi zmiennymi endogenicznymi macierz $ \Gamma $ jest macierz diagonalną, będąc macierzą jednostkową.\\
Przykład trójrównaniowego modelu prostego:
\begin{align*}
&y_{t1}=\beta_{10}+\beta_{11}x_{t1}+\beta_{12}x_{t2}-\varepsilon_{t1}\\
&y_{t2}=\beta_{20}+\beta_{21}x_{t2}+\beta_{23}x_{t3}-\varepsilon_{t2}\\
&y_{t3}=\beta_{30}+\beta_{32}x_{t1}+\beta_{33}x_{t3}-\varepsilon_{t3}
\end{align*}
Zapis w postaci macierzowej
\begin{gather*}
Y\cdot
\begin{bmatrix}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 1 \\
\end{bmatrix}
+
X\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & -\beta _{32} \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=E
\end{gather*}

Wstępne założenia stochastyczne dla modelu prostego:\\
Zakładając nielosowość zmiennych z góry ustalonych (egzogenicznych) musimy uznać, ze zmienne egzogeniczne są niezależne od składników losowych, tzn.
\begin{gather*}
\mathbb E \left(X'_j\cdot \varepsilon_j\right)=\left(\mathbb E X'_j\right)\cdot \left(\mathbb E \varepsilon_j\right)=X'_j\cdot 0=0
\end{gather*}

\subsubsection{Model rekurencyjny}
Model rekurencyjny jest to model, w którym występują jednokierunkowe powiązania pomiędzy zmiennymi endogenicznymi nieopóźnionymi w czasie. Oznacza to, że macierz $ \Gamma $ - parametrów występujących przy zmiennych endogenicznych - jest macierzą trójkątną.\\
Przykład trójrównaniowego modelu rekurencyjnego:
\begin{gather*}
Y\cdot
\begin{bmatrix}
	1       & 0       & 0 \\
	-y_{12} & 1       & 0 \\
	-y_{13} & -y_{23} & 1
\end{bmatrix}
+
X\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & -\beta _{32} \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=E
\end{gather*}
\textbf{Wniosek}\\
W modelach rekurencyjnych istnieje zawsze takie równanie, w którym występują jedynie zmienne z góry ustalone (egzogeniczne).

W przypadku pozostałych równań musimy uznać, że w zbiorze zmiennych objaśniających występują zmienne losowe, którymi są zmienne endogeniczne występujące w charakterze zmiennych objaśniających. Zmienne endogeniczne występujące w charakterze zmiennych objaśniających mogą być uznanne za:
\begin{itemize}
\item nieskorelowane ze składnikiem zakłócającym danego równania, jeśli konstrukcja modelu nie wymusza odrzucenia takiego założenia
\item skorelowane ze składnikiem zakłócającym danego równania, jeśli wynika to z założeń tkwiących u podstaw konstrukcji modelu.
\end{itemize}

\subsection{Model o równaniach współzależnych}
Model o równaniach współzależnych jest ot model, w którym występują nieopóźnione w czasie sprzężenia zwrotne pomiędzy zmiennymi endogenicznymi. Oznacza to, ze macierz $ \Gamma $ nie jest macierzą ani diagonalną, ani trójkątną.\\
Przykład trójrównaniowego modelu regresji wielorakiej:
\begin{align*}
&y_{t1}=\gamma_{12}y_{t2}+\gamma_{13}y_{t3}+\beta_{10}+\beta_{11}x_{t1}+\beta_{12}x_{t2}+\varepsilon_{t1}\\
&t_{t2}=\gamma_{23}y_{t3}+\beta_{20}+\beta_{21}x_{t1}+\beta_{23}x_{t3}+\varepsilon_{t2}\\
&y_{t3}=\gamma_{31}y_{t1}+\beta_{30}+\beta_{33}x_{t3}+\varepsilon_{t3}
\end{align*}
\begin{gather*}
Y\cdot
\begin{bmatrix}
	   1    &    0    & 0 \\
	-y_{12} &    1    & 0 \\
	-y_{13} & -y_{23} & 1
\end{bmatrix}
+X\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & -\beta _{32} \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=E
\end{gather*}
Uporządkowany całościowy zapis modelu z restrykcjami zerowymi
\begin{gather*}
y_{t1}-\gamma_{12}
\end{gather*}
\begin{gather*}
\begin{bmatrix}
	y_1 & y_2 & y_3
\end{bmatrix}
\cdot
\begin{bmatrix}
	   1    &    0    & -\gamma_{31} \\
	-y_{12} &    1    & 0 \\
	-y_{13} & -y_{23} & 1
\end{bmatrix}
+\\+
\begin{bmatrix}
	1 & x_1 & x_2 & x_3
\end{bmatrix}
\cdot
\begin{bmatrix}
	-\beta _{10} & -\beta _{20} & -\beta _{30} \\
	-\beta _{11} & -\beta _{21} & 0            \\
	-\beta _{12} & 0            & 0            \\
	0            & -\beta _{23} & -\beta _{33}
\end{bmatrix}
=
\begin{bmatrix}
\varepsilon_1&\varepsilon_2&\varepsilon_3
\end{bmatrix}
\end{gather*}

Z uwagi na to, że macierz $ \Gamma $ nie jest ani diagonalna, ani trójkątna, model uznajemy za model o równaniach współzależnych. Oznacza to, ze między zmiennymi endogenicznymi nieopóźnionymi w czasie występują sprzężenia zwrotne.\\
Zauważmy, że zmienne endogeniczne, będąc między innymi funkcjami zmiennych losowych ($ \varepsilon $), jednocześnie nawzajem się wyjaśniają. Tym samym musimy uznać, że zmienne te - występując w poszczególnych równaniach w charakterze zmiennych objaśniających - są skorelowane ze składnikami losowymi tychże równań.

\subsubsection{Redukcja modeli o równaniach współzależnych}
Procedura sprowadzania modelu o równaniach współzależnych do wielorównaniowego modelu prostego nazywamy \underline{redukcją}. Przykładowa procedura redukcji naszego modelu
\begin{gather*}
y_t\cdot\Gamma+x_t\cdot \beta=\varepsilon_t
\end{gather*}
Przekształcona postać modelu
\begin{gather*}
y_t\cdot\Gamma=x_t\cdot (-\beta)+\varepsilon_t
\end{gather*}
Zapis macierz w postaci zredukowanej
\begin{gather*}
y_t=x_t\cdot \pi+v_t
\end{gather*}
gdzie \begin{gather*}
\pi=-B\Gamma=
\begin{bmatrix}
\pi _{10} & \pi _{20} & \pi _{30} \\
\pi _{11} & \pi _{21} & \pi _{31} \\
\pi _{12} & \pi _{22} & \pi _{32} \\
\pi _{13} & \pi _{23} & \pi _{33} \\
\end{bmatrix}
\end{gather*}
$ \pi  $- macierz parametrów strukturalnych postaci zredukowanej o wymiarach $(k+1)\times G $ (gdzie $ k+1=4,\;G=3 $)\\
Wektor składników zakłócających postaci zredukowanej o wymiarach $ G\times G,\;G=3 $
\begin{gather*}
v_t=\varepsilon_t\Gamma^{-1}=
\end{gather*}
Tym samym model w postaci macierzowej zapiszemy następująco
\begin{gather*}
\begin{bmatrix}
v_{t1}&v_{t2}&v_{t3}
\end{bmatrix}
=
\begin{bmatrix}
1&x_{t1}&x_{t2}&x_{t3}
\end{bmatrix}
\cdot
\begin{bmatrix}
\pi _{10} & \pi _{20} & \pi _{30} \\
\pi _{11} & \pi _{21} & \pi _{31} \\
\pi _{12} & \pi _{22} & \pi _{32} \\
\pi _{13} & \pi _{23} & \pi _{33} \\
\end{bmatrix}
+
\begin{bmatrix}
v_{t1}&v_{t2}&v_{t3}
\end{bmatrix}
\end{gather*}
Co po rozpisaniu przybierze następującą postać
\begin{gather*}
y_{t1}=\pi _{10}+x_{t1} \pi _{11}+x_{t2} \pi _{12}+x_{t3} \pi _{13} + v_{t1}\\
y_{t2}=\pi _{20}+x_{t1} \pi _{21}+x_{t2} \pi _{22}+x_{t3} \pi _{23} + v_{t2}\\
y_{t3}=\pi _{30}+x_{t1} \pi _{31}+x_{t2} \pi _{32}+x_{t3} \pi _{33} + v_{t3}
\end{gather*}
Rozpisany zapis macierzowy modelu dla $ n $ obserwacji przedstawia się następująco
\begin{gather*}
Y=X\Pi+V
\end{gather*}
Zauważmy, że w każdym z równań postaci zredukowanej występują jedynie zmienne z góry ustalone (egzogeniczne) modelu. Jeśli uznamy, że są one nielosowe, mamy prawo wykluczyć ich ewentualną zależność ze składnikami losowymi poszczególnych równań. Wynika z tego, że każde z równań możemy oszacować stosując metodę najmniejszych kwadratów.

\subsubsection{Identyfikacja}
Identyfikacja jest ot proces rozpoznawania (identyfikowania) parametrów strukturalnych modelu (elementów macierzy $ \Gamma $ i $ B $)  na podstawie parametrów postaci zredukowanej, a więc na podstawie elementów macierzy $ \Pi $. Ze zdefiniowania macierzy $ \Pi $ wynikają następujące konsekwencje
\begin{gather*}
\Pi=-B\Gamma^{-1}\Leftrightarrow\Pi\Gamma=-B
\end{gather*}WYkorzystując badany model oraz jego postać zredukowaną i odpowiednio zdefiniowane dla tych modeli macierze $ \pi,\Gamma $ oraz $ B $ drugi człon powyższego wyrażenia zapiszemy następująco\\
<dużo rozpisanych macierzy

Powiemy, że $ j $-ty układ równań:
\begin{enumerate}
	\item Zawiera więcej parametrów $ \gamma $ i $ \beta $ aniżeli równań, to równanie Mj-te postaci strukturalnej uznajemy za \textbf{nieidentyfikowalne}.
	\item Zawiera taką samą ilość parametrów $ \gamma $ i $ \beta $ aniżeli równań, to równanie Mj-te postaci strukturalnej uznajemy za \textbf{jednoznacznie identyfikowalne}.
	\item Zawiera mniej parametrów $ \gamma $ i $ \beta $ aniżeli równań, to równanie Mj-te postaci strukturalnej uznajemy za \textbf{niejednoznacznie identyfikowalne}.
\end{enumerate}

W rozpatrywanym przypadku otrzymujemy następujące trzy układy równań. Pierwszy układ równań $ (a) $ dla $ j=1 $ równania postaci strukturalnej modelu
\begin{align*}
	 & \pi _{10}-\pi _{20} \gamma _{12}-\pi _{30} \gamma _{13} =\beta_{10} \\
	 & \pi _{11}-\pi _{21} \gamma _{12}-\pi _{31} \gamma _{13} =\beta_{11} \\
	 & \pi _{12}-\pi _{22} \gamma _{12}-\pi _{32} \gamma _{13} =\beta_{12} \\
	 & \pi _{13}-\pi _{23} \gamma _{12}-\pi _{33} \gamma _{13} =0
\end{align*}
Z uwagi na fakt, iż liczba poszukiwanych parametrów $ \gamma $ i $ \beta $ wynosi 5 (2 parametry $ \gamma $ oraz 3 parametry $ \beta $) jest większa od liczby równań, powyższy układ nie ma rozwiązania. Tym samym powiemy, że równanie pierwsze postaci strukturalnej jest nieidentyfikowalne.

Drugi układ równań $ (b) $ dla $ j=2 $ równania postaci strukturalnej modelu
\begin{align*}
	 & \pi _{20}-\pi _{30} \gamma _{23} = \beta _{20} \\
	 & \pi _{21}-\pi _{31} \gamma _{23} = \beta _{21} \\
	 & \pi _{22}-\pi _{32} \gamma _{23} = 0           \\
	 & \pi _{23}-\pi _{33} \gamma _{23} = \beta _{23}
\end{align*}
Z uwagi na fakt, iż liczba poszukiwanych parametrów $ \gamma $ i $ \beta $ wynosi 4 (1 parametr $ \gamma $ i 3 parametry $ \beta $) jest równa liczbie równań, powyższy układ ma jednoznaczne rozwiązanie. Tym samym powiemy ,że równanie drugie postaci strukturalnej jest jednoznacznie identyfikowalne. Zauważmy, że z równania trzeciego tego układu równań wynika, że
\begin{gather*}
\pi_{22}-\gamma_{23}\pi_{32}=0\Rightarrow\gamma_{23}=\left(\frac{\pi_{21}}{\pi_{32}}\right)
\end{gather*}

Trzeci ukłąd równań $ (c) $ dla $ j=3 $ równania postaci strukturalnej modelu
\begin{align*}
& \pi _{30}-\pi _{10} \gamma _{31} = \beta _{30} \\
& \pi _{31}-\pi _{11} \gamma _{31} = 0 \\
& \pi _{32}-\pi _{12} \gamma _{31} = 0 \\
& \pi _{33}-\pi _{13} \gamma _{31} = \beta _{33} \\
\end{align*}
Z uwagi na fakt ,iż liczba poszukiwanych parametrów $ \gamma $ i $ \beta $ wynosi 3 (1 parametr $ \gamma $ i 2 parametry $ \beta $) jest mniejsza od liczby równań, powyższy układ nie ma jednoznacznego rozwiązania. Tym samym powiemy ,że równanie trzecie postaci strukturalnej jest niejednoznacznie identyfikowalne. 

Aby sformułować użyteczne twierdzenia dotyczące identyfikacji modelu o rówaniach współzależnych przyjmujemy następujący system oznaczeń:
\begin{itemize}
	\item $ k_j $ - liczba zmiennych z góry ustalonych występujących w $ j $-tym równaniu modelu łącznie z wyrazem wolnym
	\item $ K $ - liczba zmiennych z góry ustalonych występujących w całym modelu łącznie z wyrazem wolnym
	\item $ G_j $ - liczba zmiennych łącznie współzależnych (endogenicznych) występujących w $ j $-tym równaniu
	\item $ G_{j-1} $ - liczba zmiennych łącznie współzależnych (endogenicznych) występujących w $ j $-tym równaniu w charakterze zmiennych objaśniających (bez zmiennej objaśnianej w tym równaniu)
	\item $ G $ - liczba zmiennych łącznie współzależnych zależnych (endogenicznych) występujących w całym modelu (równa liczbie równań modelu)
\end{itemize}
\begin{gather*}
K_j^*=K-k_j\\
G_j^*=G-G_j
\end{gather*}
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te było identyfikowalne, jest aby liczba zmiennych z góry ustalonych oraz łącznie współzależnych występujących w równaniu $ j $-tym w charakterze zmiennych objaśniających była mniejsza od liczby zmiennych z góry ustalonych występujących w całym modelu
	\begin{gather*}
	K\ge K_j+G_j-1
	\end{gather*}
\end{twr}
Równanie możemy przekształcić w następujący sposób
\begin{gather*}
K-k_j\ge G_j-1
\end{gather*}
Wprowadzając do siebie wyrażenia otrzymujemy
\begin{gather*}
K-k_j\ge G-G_j^*-1
\end{gather*}
a stąd ostatecznie otrzymujemy
\begin{gather*}
K_j^*+G_j^*\ge G-1
\end{gather*}
Wykorzystując powyższe wyrażenie formułujemy następujące twierdzenia
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{identyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była nie mniejsza od liczby równań modelu pomniejszonej o jeden.
\end{twr}
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{jednoznacznie identyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była równa liczbie równań modelu pomniejszonej o jeden.
\end{twr}
\begin{twr}
	Warunkiem koniecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{niejednoznacznie identyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była większa od liczby równań modelu pomniejszonej o jeden.
\end{twr}
\begin{twr}
	Warunkiem dostatecznym na to, aby równanie $ j $-te postaci strukturalnej modelu było \textbf{nieidentyfikowalne}, jest by liczba zmiennych nie występujących w $ j $-tym równaniu była mniejsza od liczby równań modelu pomniejszonej o jeden.
\end{twr}

Zadanie\\
Rozpatrz następujący model o równaniach współzależnych
\begin{align*}
&Y_t=q_0+q_1L_t+a_2k_t+a_3H_t+a_4ER_t+e_{t1}\\
&L_t=b_0+b_1Y_t+b_2K_t+b_3W_te_{t2}\\
&K_t=c_0+c_1Y_t+c_2R_t+e_{t3}
\end{align*}

\subsubsection{Metoda Zmiennych Instrumentalnych}
W dotychczas przedstawianych modelach przyjmowaliśmy założenie o braku korelacji między zmiennymi uwzględnionymi w specyfikacji modelu a składnikiem losowym Jednak w wielu ważnych z punktu widzenia teorii ekonomicznej zastosowaniach takie założenie nie jest spełnione. W takim przypadku nie można udowodnić zgodności estymatora MNK.

W modelu $ Y=X\beta+\varepsilon $
\begin{itemize}
	\item zmiennymi egzogenicznymi nazywamy zmienne, które nie są skorelowane ze składnikami losowymi
	\item zmiennymi endogenicznymi nazywamy zmienne, które są skorelowane ze składnikami losowymi
\end{itemize}

\section{Równoczesność}

O problemie równoczesności mówimy, gdy występuje niezerowa korelacja pomiędzy zmienną objaśniającą $ x_i $ a równoczesnym błędem losowym $ \varepsilon_j $. Gdy $ \mathbb E \left(\varepsilon|X\right) \neq0$ to
\begin{align*}
&\mathbb E (b)=\mathbb E \left(\left(X'X\right)^{-1}X'y|X\right)\\
&\mathbb E (b)=\beta+\left(X'X\right)^{-1}X'\mathbb E \left(\varepsilon|X\right)\neq\beta
\end{align*}
Więc estymator wektora parametrów jest obciążony.

\subsection{Przykłady}
Model Keynesowski gospodarki zakłada, że\begin{gather*}
\text{PKB=konsumpacja+inwestycje+export netto}
\end{gather*}
Z drugiej strony Keynesowska funkcja konsumpcji zakłada, że $ C_t=f(Y_t) $. Szacując jej parametry
\begin{gather*}
C_t=\beta_0+\beta_1Y_t+\varepsilon_t
\end{gather*}
nie są spełnione założenia modelu, gdyż $ \text{cov}\left(X_t,\varepsilon_t\right) \neq0$\\
Szacujemy model autoregresji postaci
\begin{gather*}
y_t=f\left(y_{t-1},y_{t-2},\dots\right)+\varepsilon_t
\end{gather*}
ale
\begin{gather*}
y_{t+1}=f\left(y_{t-2},y_{t-3},\dots\right)+\varepsilon_{t+1}
\end{gather*}
zatem $ \text{cov}\left(y_{t-1},\varepsilon_{t-1}\right) \neq0$. Wobec tego w modelu $ (1) $ zmienne objaśniające są skorelowane z błędem losowym. Zazwyczaj w zbiorach danych mikroekonomicznych brakuje informacji o zdolnościach respondentów. Mimo wszystko szacuje się równanie płacy typu Mincera
\begin{gather*}
\ln\left(placa\right)=\beta_0+\beta_1plec+\beta_2\text{wiek}+\beta_3\text{wiek2}+\beta_4\text{wykszt}+u
\end{gather*}
Ponieważ w modeelu pominięto zmienną niezależną zdolności to skłądnik losowy ma postać
\begin{gather*}
u=\gamma_0+\gamma_1zdolnosci+\phi
\end{gather*}
Z drugiej strony poziom wykształcenia jest determinowany przez zdolności respondenta. Zatem
\begin{gather*}
\text{cov}(u,wykszt)=\text{cov}(zdolnosci+\phi,wykszt)=\\=
\text{cov}(zdolnosci,wykszt)+\text{cov}(\phi,wykszt)\neq0
\end{gather*}
Ponieważ zmienna pominięta jest dodatnio skorelowana z uzyskanym wykształceniem i wpływa dodatnio na zmienną zależną to parametr przy zmiennej będzie dodatnio obciążony.\\
Metoda zmiennych instrumentalnych pozwala na uzyskanie zgodnych estymatorów w przypadku występowania korelacji między zmiennymi objaśniającymi a składnikiem losowym. Pozwala również uzyskać zgodne oszacowania parametrów w przypadku występowania problemu równoczesności. Polega ona na zastępowaniu oryginalnych zmiennych instrumentami. Instrumenty powinny być skorelowane ze zmiennymi objaśniającymi, ale nie powinny być skorelowane z błędem losowym. Znalezienie właściwych instrumentów jest najciekawszym, ale również najtrudniejszym etapem badania.

Oznaczmy przez $ Z $ macierz zmiennych instrumentalnych (instrumentów). Estymator MZI jest zgodny, gdy spełnione są następujące warunki
\begin{gather*}
plim\left(\tfrac{1}{n}Z'\varepsilon\right)=plim\left(\tfrac{1}{n}\sum_iz_i'\varepsilon_i\right)=\mathbb E \left(z_i'\varepsilon_i\right)=0\\
plim\left(\tfrac{1}{n}Z'X\right)=plim\left(\tfrac{1}{n}\sum_iz_i'x_i\right)=\mathbb E \left(z_i'x_i\right)\neq0
\end{gather*}
Dodatkowo $ r\left(\mathbb E \left(z_i'x_i\right)\right) =k$
\begin{gather*}
plim\left(\tfrac{1}{n}Z'Z\right)=plim\left(\tfrac{1}{n}\sum_iz_i'z_i\right)=\mathbb E \left(z_i'z_i\right)\neq0
\end{gather*}
Metoda polega na zastępowaniu oryginalnych wartości zmiennych objaśniających wartościami dopasowanymi uzyskanymi z regresji pomocniczej wykorzystującej zmienne instrumentalne.
\begin{itemize}
	\item macierz instrumentów $ Z $ musi zawierać co najmniej tyle zmiennych, ile oryginalna macierz $ X $
	\item Ale nie w każdym przypadku konieczne jest posiadanie $ k $ nowych zmiennych
	\item Zmienne z macierzy $ X $, które są nieskorelowane ze składnikiem losowym mogą same dla siebie stanowić instrumenty
	\item W rezultacie potrzeba co najmniej tylu dodatkowych zmiennych instrumentalnych ile jest zmiennych skorelowanych ze składnikiem losowym.
\end{itemize}
Macierz instrumentów uzyskujemy poprzez rzutowanie wektora $ X $ na przestrzeń rozpiętą przez kolumny macierzy instrumentów $ Z $
\begin{gather*}
\hat{X}=Z\underset{\beta}{\underbrace{\left(Z'Z\right)^{-1}Z'X}}=Z\hat{\beta=P_ZX}
\end{gather*}
Dysponując macierzą instrumentów $ \hat X $ budujemy estymator
\begin{gather*}
\beta_{MZI}=\left(\hat X'X\right)^{-1}\hat X'y=\left(X'P_ZX\right)^{-1}X'P_Zy=\\=
\left(X'Z\left(Z'Z\right)^{-1}Z'\right)^{-1}X'Z\left(Z'Z\right)^{-1}Z'y
\end{gather*}
jeżeli $ r(2)=r(X)$, czyli tyle nowych zmiennych, ile zmiennych w macierzy $ X $ skorelowanych ze składnikiem.

Estymator MZI jest również nazywany estymatorem dwustopniowej metody najmniejszych kwadratów.
\begin{itemize}
	\item W pierwszym kroku przeprowadzana jest regresja zmiennych endogenicznych na zmienne instrumentalne
	\item W drugim, oryginalne wartości zmiennych są zastępowane prze zwartości dopasowane z pierwszego kroku i oblicze są oszacowania poszczególnych parametrów
	\item W kolejnym kroku pokażemy, że przy warunkach zdefiniowanych wcześniej estymator MZI jest zgodny
\end{itemize}
Estymator MZI jest dany wzorem
\begin{gather*}
\beta_{MZI}
=
\left(X'Z\left(Z'Z\right)^{-1}Z'\right)^{-1}X'Z\left(Z'Z\right)^{-1}Z'y
\end{gather*}
więc
\begin{gather*}
plim\beta_{MZI}
=
plim\left(X'Z\left(Z'Z\right)^{-1}Z'\right)^{-1}X'Z\left(Z'Z\right)^{-1}Z'y
\end{gather*}
\textbf{Wnioski}\\
\begin{itemize}
	\item Estymator MZI jest zgodny nawet, gdy w modelu występują zmienne endogeniczne
	\item Ale, gdy nie ma takich zmiennych, a spełnione są założenia MNK to estymator MZI nie jest efektywny
	\item MZI można traktować jako uogólnienie MNK
	\item Wobec tego wszystkie testy stosowane przy MNK mogą być stosowane przy MZI
	\item Testy specyficzne dla MZI weryfikują założenie o egzogeniczności zmiennych oraz poprawności wykorzystanych instrumentów.
\end{itemize}

\subsection{Test Hausmana}
Przy estymacji MZI test Hausmana jest testem na egzogeniczność zmiennych. Estymator MZI jest estymatorem zawsze zgodnym.\\
Przy prawdziwej $ H_0 $ o egzogeniczności zmiennych $ X $ oba estymatory są zgodne, ale estymator MZI ma większą wariancję niż estymator MNK.\\
Przy fałszywej $ H_0 $ estymator MZI jest zgodny i efektywny, a estymator MNK nie jest zgodny.\\
Statystyka testowa jest dana przez formę kwadratową
\begin{gather*}
\left(\beta_{MZI}-\beta_{MNK}\right)'\Sigma_{\beta_{MZI}-\beta_{MNK}}^{-1}\left(\beta_{MZI}-\beta_{MNK}\right)\xrightarrow{D}\chi^2\left(r(\Sigma)\right)
\end{gather*}
Gdy różnica jest duża sugeruje to wykorzystanie MNK.

\subsection{Test Surgana}
\begin{itemize}
	\item Test Surgana weryfikuje poprawność instrumentów
	\item Zauważmy, że reszty modelu są równe
	\begin{gather*}
	e=\left(I-X\left(X'P_ZX\right)^{-1}X'P_Z\right)\varepsilon
	\end{gather*}
	\item Oczywiście $ M_Z $ jest macierzą idempotentną, rzędu $ p-k $
	\item Przy prawdziwości $ H_0 $ jest brak korelacji instrumentów z błędami losowymi
	\begin{gather*}
	\frac{eP_Ze}{\sigma^2}\sim \chi^2_{p-k}
\end{gather*}
\item Niestety przeprowadzenie testu jest wyłącznie możliwe, gdy liczba instrumentów przekracza liczbę zmiennych objaśniających
\end{itemize}
\section{Uogólniona metoda zminnych instrumentalnych}
Zakładamy, że mamy więcej instrumentów niż zminnych objaśniających
\begin{center}
wymiar macierzy $ X \neq $ wymiar macierzy $ Z $
\end{center}
Założyliśmy, że $ \mathbb E \left(Z'\varepsilon\right) =0$, zatem $ \mathbb E \left(Z'\left(y-X\beta\right)\right) =0$\\
Skoro średnia z próby jest oceną wartości oczekiwanej, to
\begin{gather*}
\tfrac{1}{n}\bigl(Z'\left(y-X\beta\right)\bigr)=0
\end{gather*}
\subsection{Estymator uogólnionej MZI (UMZI,GIV)}
Przy powyższym założeniu, UMZI polega na minimalizacji następującego wyrażenia
\begin{gather*}
Q_n(\beta)=
\left[\frac{1}{n}Z'\left(y-X\beta\right)\right]'
W_n
\left[\frac{1}{n}Z'\left(y-X\beta\right)\right]
\end{gather*}
gdzie $ W_n $ jest symetryczną, kwadratową macierzą wag, o wymiarze równym liczbie instrumentów w macierzy $ Z $. Macierz ta wskazuje jakie wagi należy przypisać poszczególnym równaniom w układzie równań $ \tfrac{1}{n}\bigl(Z'\left(y-X\beta\right)\bigr)=0 $, czyli określa które instrumenty są bardziej, a które mniej ważne.\\
Minimalizując $ Q_n $ pierwszą pochodną po $ \beta  $ przyrównujemy do zera
\begin{gather*}
-ZX'ZW_nZ'y+ZX'ZW_nZ'X\hat \beta =0
\end{gather*}
co oznacza, że
\begin{gather*}
X'ZW_nZ'y=X'ZW_nZ'X\hat \beta
\end{gather*}
Zakładając, że
\begin{gather*}
\det \left(X'ZW_nZ'X\right)\neq 0
\end{gather*}
uzyskujemy estymator UMZI
\begin{gather*}
\hat \beta_{GIV}=\left(X'ZW_nZ'X\right)^{-1}X'ZW_nZ'y
\end{gather*}
W jaki sposób dobieramy macierz wag $ W_n $?\\
Różne macierze wag prowadzą do estymatorów różnej postaci, ale przy spełnieniu wymaganych założeń wszystkie uzyskane estymatory są nieobciążone i zgodne. Optymalna macierz $ W_n $ jest proporcjonalna do odwrotności macierzy wariancji i kowariancji parametrów. W szczególnym przyadku, jeśli przyjmiemy, że składnik losowy jest sferyczny, uzyskujemy optymalną postać $ W_n $
\begin{gather*}
W_n^{opt}=\left(\tfrac{1}{n}Z'Z\right)^{-1}
\end{gather*}
Podstawiając optymalną macierz wag do estymatora, uzyskujemy estymator
\begin{gather*}
\hat \beta_{GIV}=\left(X'Z\left(\tfrac{1}{n}Z'Z\right)^{-1}Z'X\right)^{-1}X'Z\left(\tfrac{1}{n}Z'Z\right)^{-1}Z'y
\end{gather*}
Estymator ten, przyjmuje różną postać, w zależności od wybranej macierzy wag.
\section{Estymator potrójnej metody najmniejszy kwadratów (3MNK)}
\begin{itemize}
\item Estymacja parametrów równania za pomocą KMNK abstrahuje od endogeniczności niektórych zmiennych objaśniających
\item 2MNK uwzględnia endogeniczność, lecz za jej pomocą estymujemy parametry każdego równania osobno
\item Korelacje między składnikami losowymi poszczególnych równań nie zostają uwzględnione w procesie estymacji
\item Wady te w modelu nie będzie, gdy przeprowadzimy łączną estymację parametrów wszystkich równań, uwzględniając korelacje składników losowych poszczególnych równań
\end{itemize}
Potraktujemy model wielorównaniowy jako macierzowy model jednorównaniowy. Zapisujemy nasz $ n $-wymiarowy model jako
\begin{align*}
y&_1=\tilde{Z}_1\cdot\tilde{F}_1+\varepsilon_1\\
y&_2=\tilde{Z}_2\cdot\tilde{F}_2+\varepsilon_2\\
&\vdots\\
y&_m=\tilde{Z}_m\cdot\tilde{F}_m+\varepsilon_m
\end{align*}
gdzie $ \tilde{Z}_i=\begin{bmatrix}
\tilde{Y}_i&\tilde{Z}_i
\end{bmatrix} $ - wektor zmiennych objaśniających (endo- i egzogenicznych) w $ n $-tym równaniu modelu.\\
Niech $ \tilde{F}_j^{2MNK} $ - wektor oszacowań parametrów $ j $-tego równania za pomocą 2MNK.\\
Znając go, możemy oszacować macierz wariancji-kowariancji wektora składników losowych modelu
\begin{gather*}
\text{Var}(\varepsilon_t)=\mathbb E \left(\varepsilon_t\varepsilon_t^T\right)\\
\hat \Sigma=
\begin{bmatrix}
\tilde{\sigma_{ij}}
\end{bmatrix}\\
\tilde{\sigma_{ij}}=\left(y_i-Z_i\tilde{F}_i^{2MNK}\right)\left(y_i-Z_i\tilde{F}_i^{2MNK}\right)^T/T
\end{gather*}
gdzie $ T $ - liczba obserwacji.\\
Alternatywnie zamiast $ T $, można podzielić sumę iloczynów w liczniku przez średnią geometryczną liczbę stopni swobody obu równań $ i $ oraz $ j $
\begin{gather*}
\sqrt{\left(T-\tilde{M}_i-\tilde{K}_i\right)\left(T-\tilde{M}_j-\tilde{K}_j\right)}
\end{gather*}
Zapisujemy cały model w jednym równaniu macierzowym
\begin{gather*}
\underset{y}{\underbrace{\begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_m
\end{bmatrix}}}
=
\underset{Z}{
\underbrace{\begin{bmatrix}
	\tilde Z_1 & 0          & \dots  & 0          \\
	0          & \tilde Z_2 & \dots  & 0          \\
	\vdots     & \vdots     & \ddots & \vdots     \\
	0          & 0          & \dots  & \tilde Z_m
\end{bmatrix}}}
\underset{F}{\underbrace{\begin{bmatrix}
\tilde F_1\\
\tilde F_2\\
\vdots\\
\tilde F_m
\end{bmatrix}}}
+
\underset{\varepsilon}{\underbrace{\begin{bmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_m
\end{bmatrix}}}
\end{gather*}
Przy czym $ \varepsilon-1,\varepsilon-2,\dots$ są wektorami pionowymi o wymiarach $ T\times 1$
\subsection{Kolejne kroki w estymacji 3MNK}
Krok 1. Estymacja w postaci zredukowanej $ (MNK) $ i obliczenie wartości teoretycznych dla równania $ j $
\begin{gather*}
\hat{ \tilde Z}_j=X\Pi_j=X\left(X^TX\right)^{-1}X^T\tilde Z_j
\end{gather*}
Macierzowo dla całego systemu
\begin{gather*}
\begin{bmatrix}
	X\left(X^TX\right)^{-1}X^T\tilde Z_1 & 0                                    & \dots  & 0                                    \\
	0                                    & X\left(X^TX\right)^{-1}X^T\tilde Z_2 & \dots  & 0                                    \\
	\vdots                               & \vdots                               & \ddots & \vdots                               \\
	0                                    & 0                                    & \dots  & X\left(X^TX\right)^{-1}X^T\tilde Z_m
\end{bmatrix}
\end{gather*}
lub inaczej
\begin{gather*}
\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z
\end{gather*}
Krok 2. Estymacja w postaci strukturalnej parametrów pojedynczych równań (2MNK). Wartości z próby w KMNK zamienione na wartości teoretyczne
\begin{gather*}
\hat F^{2MNK}=\left(\hat Z^T\hat Z\right)\hat Z^Ty
\end{gather*}
Krok 3. Uwzględnienie jednoczesnych korelacji składników losowych w procesie estymacji. Jeżeli poszczególne składniki losowe są sferyczne, to każdy taki wektor ma macierz wariancji-kowariancji postaci
\begin{gather*}
\begin{bmatrix}
	\sigma _{jj}^2 & 0            & \dots & 0            \\
	0            & \sigma _{jj}^2 & \dots & 0            \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	0            & 0            & \dots & \sigma _{jj}^2
\end{bmatrix}
\end{gather*}
\begin{gather*}
\left(
\begin{matrix}
\begin{matrix}
	\sigma ^2 _{11} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{11} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{11} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{11}
\end{matrix}
&
\begin{matrix}
	\sigma ^2 _{12} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{12} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{12} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{12}
\end{matrix}
& \dots &
\begin{matrix}
	\sigma ^2 _{1m} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{1m} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{1m} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{1m}
\end{matrix}
\\
\begin{matrix}
	\sigma ^2 _{21} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{21} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{21} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{21}
\end{matrix}
&
\begin{matrix}
	\sigma ^2 _{22} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{22} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{22} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{22}
\end{matrix}
& \dots &
\begin{matrix}
	\sigma ^2 _{2m} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{2m} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{2m} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{2m}
\end{matrix}
\\
 \vdots & \vdots & \ddots & \vdots \\
\begin{matrix}
	\sigma ^2 _{m1} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{m1} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{m1} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{m1}
\end{matrix}
&
\begin{matrix}
	\sigma ^2 _{m2} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{m2} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{m2} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{m2}
\end{matrix}
& \dots &
\begin{matrix}
	\sigma ^2 _{mm} & 0               & \dots           & 0               \\
	0               & \sigma ^2 _{mm} & \dots           & 0               \\
	\vdots          & \vdots          & \sigma ^2 _{mm} & \vdots          \\
	0               & 0               & \dots           & \sigma ^2 _{mm}
\end{matrix}
\\
\end{matrix}
\right)
\end{gather*}
Wykorzystanie iloczynu Kroneckera w 3MNK
\begin{gather*}
\begin{bmatrix}
	a_{11} & a_{12} \\
	a_{21} & a_{22}
\end{bmatrix}
\otimes B=
\begin{bmatrix}
	a_{11}B & a_{12}B \\
	a_{21}B & a_{22}B
\end{bmatrix}
\end{gather*}
Macierz wariancji-kowariancji składników reszt z 2MNK
\begin{gather*}
\begin{bmatrix}
	\sigma ^2 _{11} & \sigma ^2 _{12} & \dots  & \sigma ^2 _{1m} \\
	\sigma ^2 _{21} & \sigma ^2 _{22} & \dots  & \sigma ^2 _{2m} \\
	\vdots          & \vdots          & \ddots & \vdots          \\
	\sigma ^2 _{m1} & \sigma ^2 _{m2} & \dots  & \sigma ^2 _{mm}
\end{bmatrix}
\otimes
\begin{bmatrix}
	1      & 0      & \dots  & 0      \\
	0      & 1      & \dots  & 0      \\
	\vdots & \vdots & \ddots & \vdots \\
	0      & 0      & \dots  & 1
\end{bmatrix}_{T\times T}=\Sigma \otimes I_T
\end{gather*}
W praktyce korzystamy z 2MNK i otrzymujemy wektor wartości teoretycznych i reszt dla każdego z równań osobno
\begin{gather*}
\forall_{j=1,\dots,m}\hat Y_j\hat \varepsilon_j
\end{gather*}
Obliczamy kowariancje między resztami losowymi poszczególnych równań
\begin{gather*}
\hat \sigma_{ij}=
\frac{\hat \varepsilon_i^T\hat \varepsilon_j}{T}
\end{gather*}
\begin{gather*}
\hat \Sigma=
\begin{bmatrix}
	\hat \sigma ^2 _{11} & \hat \sigma ^2 _{12} & \dots  & \hat \sigma ^2 _{1m} \\
	\hat \sigma ^2 _{21} & \hat \sigma ^2 _{22} & \dots  & \hat \sigma ^2 _{2m} \\
	\vdots               & \vdots               & \ddots & \vdots               \\
	\hat \sigma ^2 _{m1} & \hat \sigma ^2 _{m2} & \dots  & \hat \sigma ^2 _{mm}
\end{bmatrix}
\end{gather*}
Jeżeli znamy macierz wariancji-koawriancji całego wektora składników losowych po oszacowaniu $ \hat \Sigma $, to możemy zastosować UMNK w odniesieniu do modelu $ yZF+\varepsilon,\varepsilon\sim(o,\Omega) $
\begin{gather*}
\tilde{F}^{UMNK}=
\left(Z^T\Omega^{-1}Z\right)^{-1}
Z^T\Omega^{-1}y
\end{gather*}
Przy jednoczesnym zastosowaniu 2MNK zastępujemy w powyższym wzorze $ Z $ wartościami teoretycznymi z kroku 1.
\begin{gather*}
\hat F^{UMNK}=
\left(\hat Z^T\Omega^{-1}\hat Z\right)^{-1}
\hat Z^T\Omega^{-1}y
\end{gather*}
Skoro $ \hat \Omega=\hat \Sigma\otimes I_T $ oraz $ \hat Z=\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z $, to
\begin{gather*}
\tilde{F}^{UMNK}
=\\=
\left[Z^T\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}
\left(\hat \Sigma\otimes I_T\right)^{-1}
\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z\right]^{-1}
\cdot\\\cdot
Z^T\left\{I_m\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}^T
\left(\hat \Sigma\otimes I_T\right)^{-1}y
\end{gather*}
Z własności iloczynu Kroneckera
\begin{gather*}
\tilde{F}^{UMNK}
=\\=
\left[^ZT\left\{\hat \Sigma^{-1}\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}Z\right]^{-1}
Z^T
\left\{\hat \Sigma^{-1}\otimes\left[X\left(X^TX\right)^{-1}X^T\right]\right\}y
\end{gather*}
\subsection{Własności estymatora 3MNK}
\begin{enumerate}
\item Jest zgodny
\item Jest asymptotycznie najlepszy w pewnej klasie estymatorów zmiennych instrumentalnych (do której należy 2MNK), więc jest bardziej efektywny od 2MNK.
\item Jeżeli założymy normalność $ \varepsilon $, estymator jest asymptotycznie  równoważny estymatorowi MNK z pełną informacją (przedstawionej dalej): 3MNK ma ten sam rozkład asymptotyczny, ale nie oznacza to numerycznej równoważności oszacowań w skończonej próbie.
\end{enumerate}
\subsection{Szczególne przypadki 3MNK}
\begin{itemize}
\item W sytuacji, gdy wiem, \textit{a priori}, że $ \Sigma $ jest macierzą diagonalną, czyli równoczesne kowariancje składników losowych są zerowe, metoda 3MNK oczywiście nie poprawia efektywności. W takim przypadku 3MNK sprowadza się do 2MNK. Taki sąd \textit{a priori} jest jednak dość mocny i w praktyce zakładamy, raczej niezerowe równoczesne kowariancje.
\item Kiedy wszystkie równania są jednoznacznie identyfikowalne, 3MNK sprowadza się do 2MNK.
\end{itemize}
\section{Estymator metody największej wiarygodności z pełną informacją (MNWPO,FIML)}
Układ równań w postaci strukturalnej $ Ay+BX=\varepsilon $. Składnik losowy postaci zredukowanej ma macierz wariancji-kowariancji $ \hat \Sigma $. Jeżeli poszczególne wektory składnika losowego z różnych okresów (i) są niezależne i (ii) mają wielowymiarowy rozkład normalny, to ich łączna funkcja gęstości
\begin{gather*}
L_\varepsilon=\left(2\pi \right)^{-\frac{mT}{2}}\left|\hat \Sigma \otimes I_T\right|^{-\frac{1}{2}}\exp \left(-\tfrac{1}{2}\varepsilon^T\left(\hat\Sigma\otimes I_T\right)^{-1}\varepsilon\right)
\end{gather*}
Funkcja gęstości obserwacji i składnika losowego zależą od siebie $ L_y=L_\varepsilon\cdot \left|\det A\right| $
\begin{gather*}
\left(2\pi \right)^{-\frac{mT}{2}}\left|\hat \Sigma \otimes I_T\right|^{-\frac{1}{2}}
\left|\det A\right|
\exp\left(-\tfrac{1}{2}\left(y-\tilde{Z}\tilde{F}\right)^T\left(\hat \Sigma\otimes I_T\right)^{-1}\left(y-\tilde{Z}\tilde{F}\right)\right)
\end{gather*}
Po zlogarytmowaniu
\begin{gather*}
L_y=-\tfrac{mT}{2}\ln (2\pi)-\tfrac{T}{2}\ln \left|\Sigma\right|+T\cdot\ln\left|\det A\right|
-\\-
\frac{1}{2}
\left(\left(y-\tilde{Z}\tilde{F}\right)^T\left(\hat \Sigma\otimes I_T\right)^{-1}\left(y-\tilde{Z}\tilde{F}\right)\right)
\end{gather*}
Oszacowanie FIML otrzymamy maksymalizując powyższe wyrażenie ze względu na niezerowe wartości parametrów modelu.
\subsection{Estymacja parametrów modelu liniowego estymatorem uogólnionym Aitkena}
W sytuacji, w której niesferyczność składników zakłócających, przejawiająca się zmiennością wariancji, bądź kowariancji w czasie składników zakłócających, nie jest wynikiem błędów konstrukcji modelu estymator KMNK nie jest efektywny.
Zdefiniujmy strukturę stochastyczną modelu, w którym składniki zakłócające nie są sferyczne
\begin{gather*}
\mathbb E \varepsilon=0\\
\mathbb E \varepsilon\varepsilon'=\sigma^2_\varepsilon\Omega,
\end{gather*}
gdzie $ \Omega $ jest $ T\times T $ wymiarową macierzą, określoną dodatnio
\begin{gather*}
\mathbb E X'\varepsilon=0
\end{gather*}
Jeżeli składniki zakłócające są nieskorelowane w czasie, ale mają zmienne w czasie wariancje, wtedy macierz $ \Omega $ ma postać
\begin{gather*}
\Omega=\begin{bmatrix}
	\omega _{11} & 0            & \ldots & 0            \\
	0            & \omega _{22} & \ldots & 0            \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	0            & 0            & \ldots & \omega _{TT}
\end{bmatrix}
\end{gather*}
gdzie $ \sigma^2_\varepsilon\omega_{tt} $ jest wariancją jednoczesnych składników zakłócających.\\
Jeśli składniki zakłócające maja stałe wariancje, ale są skorelowane w czasie, wtedy macierz $ \Omega $ może być zapisana następująco
\begin{gather*}
\Omega=\begin{bmatrix}
	1            & \omega _{12} & \ldots & \omega _{1T} \\
	\omega _{21} & 1            & \ldots & \omega _{2T} \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	\omega _{T1} & \omega _{T2} & \ldots & 1
\end{bmatrix}
\end{gather*}
gdzie $ \sigma^2_\varepsilon\omega_{ij} $ jest kowariancją niejednoczesnych składników zakłócających modelu.\\
W przypadku, gdy występuje zarówno skorelowanie w czasie, jak również zmienność wariancji, wtedy
\begin{gather*}
\Omega=\begin{bmatrix}
	\omega _{11} & \omega _{12} & \ldots & \omega _{1T} \\
	\omega _{21} & \omega _{22} & \ldots & \omega _{2T} \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	\omega _{T1} & \omega _{T2} & \ldots & \omega _{TT}
\end{bmatrix}
\end{gather*}

Rozważmy estymator parametrów strukturalnych $ \hat{\beta} $ modelu $ y=X\beta+\varepsilon $, w którym składniki zakłócające są niesferyczne i który minimalizuje uogólnioną sumę kwadratów reszt
\begin{gather*}
\tilde S=\tilde \varepsilon'\Omega^{-1}\tilde \varepsilon
\end{gather*}
gdzie $ \tilde \varepsilon=y-X\tilde \beta $ jest wektorem reszt. Rozwijając otrzymujemy
\begin{gather*}
\tilde S=y'\Omega^{-1}y-2\tilde \beta'X'\Omega^{-1}y+\tilde \beta'X'\Omega^{-1}X\tilde \beta.
\end{gather*}
Szukać będziemy takiego $ \tilde \beta $, dla którego zachodzi
\begin{gather*}
\tilde S\underset{\tilde \beta}{\to} \min
\end{gather*}
Obliczamy wektor pochodnych cząstkowych $ \tilde S $ względem $ \tilde \beta $ i przyrównujemy go od zera. Otrzymamy w ten sposób
\begin{gather*}
\frac{\partial \tilde S}{\partial \tilde \beta}=-2X'\Omega^{-1}y+2X'\Omega^{-1}X\tilde \beta =0
\end{gather*}
W konsekwencji mozemy zapisać tzw. uogólniony układ równań normalnych
\begin{gather*}
X'\Omega^{-1}X\tilde \beta=X'\Omega^{-1}y
\end{gather*}
którego rozwiązaniem jest, jeśli $ X $ jest macierzą o pełnym rzędzie kolumnowym, tak, że $ \left|X'\Omega^{-1}X\right|>0 $, estymator uogólnionej metody najmniejszych kwadratów, zwany estymatorem Aitkena i mającym postać
\begin{gather*}
\tilde \beta=\left(X'\Omega X\right)^{-1}X'\Omega^{-1}y
\end{gather*}
Ponieważ macierz drugich pochodnych $ \tilde S $ względem $ \tilde \beta  $, tj.
\begin{gather*}
\frac{\partial ^2\tilde S}{\partial \tilde \beta \partial \beta'}=2X'\Omega^{-1}X
\end{gather*}
jest macierzą określoną dodatnio, zatem estymator Aitkena rzeczywiście minimalizuje uogólnioną sumę kwadratów reszt.

Estymator $ \tilde \beta  $ należy do klasy estymatorów liniowych, który można zapisać jako
\begin{gather*}
\tilde \beta=\tilde C'y
\end{gather*}
gdzie $ \tilde C'=\left(X'\Omega X\right)^{-1}X'\Omega^{-1} $ jest macierzą $ (K+1)\times T $ wymiarową.
Jeśli elementy macierzy $ X $ są nielosowe lub losowe, ale nieskorelowane ze składnikami zakłócającymi modelu, estymator Aitkena jest nieobciążony. Możemy zapisać, że
\begin{gather*}
\tilde \beta=\beta+\tilde C'\varepsilon
\end{gather*}
dlatego, że spełniony jest warunek konieczny nieobciążoności, tj.
\begin{gather*}
\tilde C'X=I_{K+1}
\end{gather*}
Zatem możemy zapisać, że
\begin{gather*}
\mathbb E \tilde \beta=\beta 
\end{gather*}
Można udowodnić, zakładają nielosowość macierzy $ X $, że macierz wariancji-kowariancji błędów estymacji w tym przypadku jest równa
\begin{gather*}
\Sigma^2_{\tilde {\beta }}=
\mathbb E \left(\tilde \beta-\beta\right)\left(\tilde \beta-\beta\right)'=
\mathbb E \tilde C'\varepsilon\left(\tilde C'\varepsilon\right)'=
\tilde C'\sigma^2_\varepsilon\Omega\tilde C=
\sigma^2_\varepsilon\tilde C'\Omega\tilde C
\end{gather*}
Znając postać macierzy $ \tilde C' $ możemy wykazać, że
\begin{gather*}
\Sigma_{\tilde \beta}=\sigma^2_\varepsilon \left(X'\Omega^{-1}X\right)^{-1}
\end{gather*}
Estymator Aitkena jest najlepszym liniowym, nieobciążonym estymatorem w klasie liniowych estymatorów dla modelu z niesferycznymi składnikami zakłócającymi. Zatem dowolny estymator nieobciążony typu
\begin{gather*}
\tilde \beta_D=\tilde C'_Dy
\end{gather*}
dla którego zachodzi $ \tilde C_DX=I_{K+1} $, charakteryzuje się macierzą wariancji-kowariancji
\begin{gather*}
\Sigma^2_{\tilde {\beta_D }}=
\mathbb E \left(\tilde \beta_D-\beta\right)\left(\tilde \beta_D-\beta\right)'=
\sigma^2_\varepsilon\tilde C_D'\Omega\tilde C_D
\end{gather*}
której elementy są nie mniejsze niż macierz wariancji-kowariancji estymatora Aitkena.
Jeśli macierz $ \Omega $ jest określona dodatnio, wtedy istnieje $ T\times T $ wymiarowa macierz $ P $ spełniająca następujące własności
\begin{gather*}
P'P=\Omega^{-1}\\
P\Omega\Omega'=I_T
\end{gather*}
gdzie $ I_T $ jest macierzą jednostkową stopnia $ T $.
Macierz $ P $ nazywać będziemy macierzą transformacji. Macierz ta umożliwia ukazanie estymatora Aitkena jako specyficznego estymatora MNK, dla transformowanych obserwacji zmiennych modelu. Niech pierwotny model
\begin{gather*}
y=X\beta+\varepsilon
\end{gather*}
zostanie przemnożony przez macierz $ P $ tak, że
\begin{gather*}
Py=PX\beta+P\varepsilon
\end{gather*}
Ponieważ macierz $ P $ ma nielosowe elementy, zatem jest prawdą, że
\begin{align*}
&\mathbb E P\varepsilon=0\\
&\mathbb E P\varepsilon\left(P\varepsilon\right)'-\sigma_\varepsilon^2 P\Omega\Omega'=\sigma_\varepsilon^2 I_T
\end{align*}
Widzimy zatem, że składniki zakłócające modelu transformowanego maja sferyczne składniki zakłócające i parametry tego modelu można szacować metodą najmniejszych kwadratów. Niech
\begin{gather*}
Y^*=X^*\beta+\varepsilon^*
\end{gather*}
gdzie
\begin{align*}
&Y^*=Py\\
&X^*=PX\\
&\varepsilon^*=P\varepsilon
\end{align*}
Stosując MNK modelu, w którym pierwotne obserwacje zostały zważone elementami macierzy $ \Omega $ jest równoważne estymacji uogólnioną metodą najmniejszych kwadratów.
\subsection{Przypadek zmienności wariancji}
Dla przypadku składników zakłócających o zmiennych wariancjach z macierzą $ \Omega $ o postaci
\begin{gather*}
\Omega=
\begin{bmatrix}
	\omega _{11} & 0             & \ldots & 0             \\
	0             & \omega _{22} & \ldots & 0             \\
	\vdots        & \vdots        & \ddots & \vdots        \\
	0             & 0             & \ldots & \omega _{TT}
\end{bmatrix}
\end{gather*}
można wykazać, że
\begin{align*}
\Omega^{-1}=
\begin{bmatrix}
	\frac{1}{\omega _{11}} & 0                       & \ldots & 0                       \\
	0                       & \frac{1}{\omega _{22}} & \ldots & 0                       \\
	\vdots                  & \vdots                  & \ddots & \vdots                  \\
	0                       & 0                       & \ldots & \frac{1}{\omega _{TT}}
\end{bmatrix}
&&
P=
\begin{bmatrix}
	\frac{1}{\sqrt{\omega _{11}}} & 0                       & \ldots & 0                       \\
	0                       & \frac{1}{\sqrt{\omega _{22}}} & \ldots & 0                       \\
	\vdots                  & \vdots                  & \ddots & \vdots                  \\
	0                       & 0                       & \ldots & \frac{1}{\sqrt{\omega _{TT}}}
\end{bmatrix}
\end{align*}
Macierze te spełniają równania
\begin{align*}
P'P=\Omega^{-1}
&&
P\Omega\Omega'=I_T
\end{align*}
\subsection{Przypadek autokorelacji}
W przypadku, gdy składniki zakłócające są skorelowane w czasie i gdy skorelowanie nie jest wynikiem niepoprawnej specyfikacji modelu, staramy się wykorzystać estymator Aitkena w celu eliminacji skutków jej występowania.
W takim przypadku elementy macierzy $ \Omega $ i $ P $ są funkcjami parametrów występujących w procesie generującym skorelowane w czasie składniki losowe. Rozważmy przykład, w którym składniki zakłócające w liniowym modelu.
\begin{gather*}
y_t\beta_{0}+\beta_{1}x_{1t}+\dots+\beta_kx_{tk}+\varepsilon_t
\end{gather*}
są generowane przez stacjonarny proces autoregresyjny rzędu pierwszego.
\begin{gather*}
\varepsilon_t=\rho _1\varepsilon_{t-1}+\eta_t
\end{gather*}
gdzie
\begin{itemize}
\item $ |\rho _1|<1 $ jest współczynnikiem autokorelacji rzędu pierwszego
\item $ \eta_t $ jest czysto losowym (sferycznym) składnikiem zakłócającym, spełniającym następujące warunki
\begin{itemize}
\item $ \mathbb E \eta_t=0 $
\item $ \mathbb E \eta^2_t=\sigma_\eta^2 $
\item $ \mathbb E \eta_t\eta_s=0 $ dla $ t\neq s $
\end{itemize}
\end{itemize}
Rozważmy jakie parametry rozkładu charakteryzować będą kolejne składniki zakłócające $ \varepsilon_t $, generowane przez model autoregresyjny. Przedstawiamy obecnie składnik zakłócający jako funkcję wszystkich przeszłych i bieżących zakłóceń $ \eta_t $. Drogą kolejnych podstawień otrzymamy
\begin{align*}
\varepsilon_t =&\rho _1\underbrace{\left(\rho _1\varepsilon_{t-2}+\eta_{t-1}\right)}_{\varepsilon_{t-1}}+\eta_t
=
\rho _1^2\varepsilon_{t-2}+\rho _1\eta_{t-1}+\eta_t \\
\varepsilon_t =&\rho _1^2\underbrace{\left(\rho _1\varepsilon_{t-3}+\eta_{t-2}\right)}_{\varepsilon_{t-2}}+\rho _1\eta_{t-1}+\eta_t
=
\rho _1^3\varepsilon_{t-3}+\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\\
& \vdots\\
\varepsilon_t =&\rho _1^{s+1}\varepsilon_{t-s-1}+\rho _1^{s}\eta_{t-s}+\dots +\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\end{align*}
Jeśli proces generujący składniki zakłócające jest stacjonarny, tj $ |\rho _1|<1 $ wtedy
\begin{gather*}
\rho _1^{s+1}\varepsilon_{t-s-1}\to0 
\end{gather*}
i w postaci końcowej
\begin{gather*}
\varepsilon_t =\rho _1^{s}\eta_{t-s}+\dots +\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\end{gather*}
Składnik losowy $ \varepsilon_t $ jest przedstawiony jako ważona funkcja wszystkich (przyszłych i bieżącego) sferycznych składników $ \eta $. Ponieważ parametry rozkładu $ \eta_t $ są zadane, zatem można wyznaczyć parametry rozkładu $ \varepsilon_t $. W przypadku wariancji możemy zapisać, że
\begin{gather*}
\mathbb E \varepsilon_t^2=
\mathbb E \bigl(
\rho _1^{s}\eta_{t-s}+\dots +\rho _1^2\eta_{t-2}+\rho _1\eta_{t-1}+\eta_t
\bigr)^2
=\\=
\rho_1^{2s}\mathbb E \eta_{t-s}^2+\dots
+
\rho_1^{4}\mathbb E \eta_{t-2}^2+
\rho_1^{2}\mathbb E \eta_{t-1}^2+
\mathbb E \eta_t^2
\end{gather*}
gdzie wszystkie kowariancje niejednoczesnych składników zakłócających $ \eta_t,\eta_s $, dla $ t\neq s $ przyrównano do zera. W konsekwencji można zapisać, że
\begin{gather*}
\mathbb E \varepsilon_t^2=
\eta _1^{2s} \sigma_\eta^2+\dots +\eta _1^4 \sigma_\eta^2+\eta _1^2 \sigma_\eta^2+\sigma_\eta^2
=
\sigma_\eta^2
\left(\eta _1^{2s} +\dots +\eta _1^4 +\eta _1^2 +1\right)
\end{gather*}
ponieważ suma elementów w nawiasie stanowi szereg geometryczny zbieżny o ilorazie $ \rho_1^2 $, zatem jego suma jest równa $ \frac{1}{1-\rho_1^2} $. Ostatecznie więc mamy:
\begin{gather*}
>E\varepsilon_t^2=\sigma_\eta^2 \cdot \frac{1}{1-\rho_1^2}
=
\sigma_\varepsilon^2
\end{gather*}
Z powyższego zapisu widać, że skorelowane w czasie składniki zakłócające mają stałe w czasie wariancje. Aby obliczyć kowariancje przeprowadzimy niestępującą procedurę kolejnych podstawień
\begin{align*}
\varepsilon_{t+1}=&
\rho_1\varepsilon_t+\eta_{t+1}
\\
\varepsilon_{t+2}=&
\rho_1
\underbrace{\left(
\rho_1\varepsilon_t+\eta_{t+1}
\right)}_{\varepsilon_{t+1}}
+\eta_{t+2}
=
\rho_1^2\varepsilon_t+\rho_1\eta_{t+1}+\eta_{t+2}
\\
\varepsilon_{t+3}=&
\rho_1
\underbrace{\left(
\rho_1^2\varepsilon_t+\rho_1\eta_{t+1}+\eta_{t+2}
\right)}_{\varepsilon_{t+2}}
+\eta_{t+3}
=
\rho_1^3\varepsilon_t+\rho_1^2\eta_{t+1}+\rho_1\eta_{t+2}+\eta_{t+3}
\\
&\vdots\\
\varepsilon_{t+\tau}=&
\rho_1^\tau \varepsilon_t+\rho_1^{\tau-1}\eta_{t+1}+\dots+\rho_1^2\eta_{t+\tau-2}+\rho_1\eta_{t+\tau-1}
\\
\end{align*}
Mnożąc obie strony ostatniej równości przez $ \varepsilon_t $ i obliczając wartość oczekiwaną otrzymamy
\begin{gather*}
\mathbb E \varepsilon_t\varepsilon_{t+\tau}=
\rho_1^\tau \mathbb E \varepsilon_t^2
+
\rho_1^{\tau -1}\mathbb E \varepsilon_t\eta_{t+1}
+\dots+
\rho_1^2\mathbb E \varepsilon_t\eta_{t+\tau-1}
+
\mathbb E \varepsilon_t\eta_{t+\tau}
=
\rho_1^\tau \sigma_\varepsilon^2
\end{gather*}
Ponieważ $ \varepsilon_t $ zależny tylko od opóźnionych i bieżących składników zakłócających $ \eta_{t-i} $ oraz $ \eta_t $, nie zależy natomiast od przyszłych składników $ \eta_{t+j} $. Jeśli składniki zakłócające generowane są przez proces autoregresyjny rzędu pierwszego, to prawdziwe są następujące własności:
\begin{align*}
&\mathbb E \varepsilon_t>0\\
&\mathbb E \varepsilon_t^2=\sigma_\varepsilon^2=\frac{\sigma_\eta^2}{1-\rho_1^2}\\
&\mathbb E\varepsilon_t\varepsilon_{t+\tau}=\rho_1^\tau\sigma_\varepsilon^2 
\end{align*}
Możemy pokazać, że $ T\times T $ macierz wariancji-kowariancji składników zakłócających $ \sigma_\varepsilon^2\Omega $ ma elementy będące funkcjami współczynnika $ \rho_1 $. Możemy zapisać, że
\begin{align*}
\Omega=
&\begin{bmatrix}
	1         & \rho _1   & \rho _1^2 & \rho _1^3 & \ldots & \rho _1^5 \\
	\rho _1   & 1         & \rho _1   & \rho _1^2 & \ldots & \rho _1^4 \\
	\rho _1^2 & \rho _1   & 1         & \rho _1   & \ldots & \rho _1^3 \\
	\rho _1^3 & \rho _1^2 & \rho _1   & 1         & \ldots & \rho _1^2 \\
	\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots    \\
	\rho _1^5 & \rho _1^4 & \rho _1^3 & \rho _1^2 & \ldots & 1
\end{bmatrix}
\\
\Omega^{-1}=
\frac{1}{1-\rho_1^2}
&\begin{bmatrix}
	1        & -\rho _1    & 0           & 0           & \ldots & 0           & 0        \\
	-\rho _1 & 1+\rho _1^2 & -\rho _1    & 0           & \ldots & 0           & 0        \\
	0        & -\rho _1    & 1+\rho _1^2 & -\rho _1    & \ldots & 0           & 0        \\
	0        & 0           & -\rho _1    & 1+\rho _1^2 & \ldots & 0           & 0        \\
	\vdots   & \vdots      & \vdots      & \vdots      & \ddots & \vdots      & \vdots   \\
	0        & 0           & 0           & 0           & \ldots & 1+\rho _1^2 & -\rho _1 \\
	0        & 0           & 0           & 0           & \ldots & -\rho _1    & 1
\end{bmatrix}
\end{align*}
Macierz transformacji $ P $ może być przedstawiona w postaci
\begin{gather*}
P=
\frac{1}{\sqrt{1-\rho_1^2}}
\begin{bmatrix}
	\sqrt{1-\rho_1^2} & 0        & 0      & \ldots & 0        & 0      \\
	-\rho _1          & 1        & 0      & \ldots & 0        & 0      \\
	0                 & -\rho _1 & 1      & \ldots & 0        & 0      \\
	\vdots            & \vdots   & \vdots & \ddots & \vdots   & \vdots \\
	0                 & 0        & 0      & \ldots & 1        & 0      \\
	0                 & 0        & 0      & \ldots & -\rho _1 & 1
\end{bmatrix}
\end{gather*}
Rozważmy równanie $ Py=PX\beta +P\varepsilon $. Ponieważ obie strony tej równości możemy podzielić przez stałą $ \frac{1}{\sqrt{1-\rho_1^2}} $, zatem transformowanymi obserwacjami zmiennej endogenicznej, zmiennych objaśniających oraz składników zakłócających są
\begin{gather*}
\sqrt{1-\rho_1^2}Py=
\begin{bmatrix}
	\sqrt{1-\rho_1^2} & 0        & 0      & \ldots & 0      \\
	-\rho _1          & 1        & 0      & \ldots & 0      \\
	0                 & -\rho _1 & 1      & \ldots & 0      \\
	\vdots            & \vdots   & \vdots & \ddots & \vdots \\
	0                 & 0        & 0      & \ldots & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
	y_1    \\
	y_2    \\
	y_3    \\
	\vdots \\
	y_T
\end{bmatrix}
=
\begin{bmatrix}
	y_1 \sqrt{1-\rho _1^2} \\
	y_2-\rho _1 y_1        \\
	y_3-\rho _1 y_2        \\
	\vdots                 \\
	y_T-\rho _1 y_{T-1}
\end{bmatrix}
\end{gather*}
\begin{gather*}
\sqrt{1-\rho_1^2}Px=
\begin{bmatrix}
	\sqrt{1-\rho_1^2} & 0        & 0      & \ldots & 0      \\
	-\rho _1          & 1        & 0      & \ldots & 0      \\
	0                 & -\rho _1 & 1      & \ldots & 0      \\
	\vdots            & \vdots   & \vdots & \ddots & \vdots \\
	0                 & 0        & 0      & \ldots & 1
\end{bmatrix}
\cdot
\begin{bmatrix}
	  1    & x_{11} & x_{12} & x_{14} & \ldots & x_{1T} \\
	  1    & x_{21} & x_{22} & x_{24} & \ldots & x_{2T} \\
	  1    & x_{31} & x_{32} & x_{34} & \ldots & x_{3T} \\
	\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
	  1    & x_{T1} & x_{T2} & x_{T4} & \ldots & x_{TT}
\end{bmatrix}
=\\=
\begin{bmatrix}
	\sqrt{1-\rho _1^2} & \sqrt{1-\rho _1^2} x_{11} & \sqrt{1-\rho _1^2} x_{12} & \ldots & \sqrt{1-\rho _1^2} x_{1T} \\
	1-\rho _1          & x_{21}-\rho _1 x_{11}     & x_{22}-\rho _1 x_{12}     & \ldots & x_{2T}-\rho _1 x_{1T}     \\
	1-\rho _1          & x_{31}-\rho _1 x_{21}     & x_{32}-\rho _1 x_{22}     & \ldots & x_{3T}-\rho _1 x_{2T}     \\
	\vdots             & \vdots                    & \vdots                    & \ddots & \vdots                    \\
	1-\rho _1          & x_{T1}-\rho _1 x_{(T-1)1} & x_{T2}-\rho _1 x_{(T-1)2} & \ldots & x_{T}-\rho _1 x_{(T-1)T}
\end{bmatrix}
\end{gather*}
Zatem typowe przekształcenia auteregresyjne maja postać
\begin{align*}
	 & \text{dla}t=1         &  & y_1^*=y_1\sqrt{1-\rho_1^2} &  & x_{ti}^*=x_{ti}\sqrt{1-\rho_1^2} &  & i=1,\dots,k \\
	 & \text{dla}t=2,\dots,T &  & y_t^*=y_t-\rho_1y_{t-1}    &  & x_{ti}^*=x_{ti}-\rho_1 x_{t-1,i} &  & i=1,\dots,k
\end{align*}
Jeśli nie przeprowadzone jest autoregresyjne przekształcenie pierwszych obserwacji zmiennych, wtedy zamiast macierzy $ P $ występuje macierz $ Q $ o wymiarach $ )T+1=<times T $
\begin{gather*}
Q=
\begin{bmatrix}
 -\rho _1 & 1 & 0 & \ldots  & 0 \\
 0 & -\rho _1 & 1 & \ldots  & 0 \\
 0 & 0 & -\rho _1 & \ldots  & 0 \\
 \vdots  & \vdots  & \vdots  & \ddots & \vdots  \\
 0 & 0 & 0 & \ldots  & -\rho _1 \\
\end{bmatrix}
\end{gather*}
Macierz ta nie spełnia warunków takich jak $ P $. W ogólnym przypadku zachodzi bowiem, że $ Q'Q\neq P'P=\Omega^{-1} $. Różnica występuje w górnym lewym elemencie obu macierzy.

Nieznany współczynnik $ \rho_1 $ estymujemy następująco
\begin{gather*}
\hat{\rho_1}=\frac{\sum_{t=2}^{T}\varepsilon_t\varepsilon_{t-1}}{\sum_{t=1}^{T}\hat{\varepsilon}^2_t}
\end{gather*}
Utwórzmy macierz $ \hat \Omega $ zgodnie z
\begin{gather*}
\hat \Omega=\begin{bmatrix}
	1                 & \tilde{\rho }_1   & \tilde{\rho }_1^2 & \ldots & \tilde{\rho }_1^{T-1} \\
	\tilde{\rho }_1   & 1                 & \tilde{\rho }_1   & \ldots & \tilde{\rho }_1^{T-2} \\
	\tilde{\rho }_1^2 & \tilde{\rho }_1   & 1                 & \ldots & \tilde{\rho }_1^{T-3} \\
	\vdots            & \vdots            & \vdots            & \ddots & \vdots            \\
	\tilde{\rho }_1^{T-1} & \tilde{\rho }_1^{T-2} & \tilde{\rho }_1^{T-3} & \ldots & 1
\end{bmatrix}
\end{gather*}
Wykorzystując powyższą macierz zdefiniujemy dwukrotnie estymator Aitkena w następujący sposób
\begin{gather*}
\hat{ \tilde\beta}=\left(X'\hat \Omega^{-1}X\right)^{-1}X'\hat \Omega^{-1}y
\end{gather*}
Estymator ten jest przybliżeniem optymalnego estymatora Aitkena za znaną macierzą wariancji-kowariancji zakłóceń modelu. Jego macierz wariancji-kowariancji jest przybliżeniem macierzy wariancji-kowariancji estymatora Aitkena. Można tę macierz zapisać
\begin{gather*}
\Sigma^2_{\hat{\tilde{\beta}}}\approx\left(X'\hat \Omega^{-1}X\right)^{-1}
\end{gather*}
\subsection{Konsekwencje występowania niesferycznych składników losowych}
Istotnym pytaniem jest jakie własności posiada estymator MNK, jeśli prawdziwym modelem generującym obserwacje zmiennej endogenicznej nie jest model klasyczny, lecz uogólniony. Aby to prześledzić rozważmy raz jeszcze estymator MNK zapisany w postaci
\begin{gather*}
\tilde{\beta}=\beta +\left(X'X\right)^{-1}X'\varepsilon
\end{gather*}
Niesferyczność składników losowych nie wpływa na nieobciążenie estymatora MNK, jeśli tylko zmienne objaśniające pozostają nieskorelowane ze składnikami losowymi modelu. Istotne konsekwencje związane są z szacowaniem wariancji składników losowych oraz z szacowaniem macierz wariancji-kowariancji błędów estymacji.
Wariancja reszt oblicza na "klasycznie" jako suma kwadratów reszt podzielona przez liczbę stopni swobody nie jest już nieobciążonym estymatorem wariancji składników losowych. Zauważmy, że wariancja reszt KMNK jest zdefiniowana jako
\begin{gather*}
\tilde{\sigma}^2_\varepsilon=\frac{1}{T-K-1}\sum_{t=1}^{T}\hat{\varepsilon}^2_t=
\frac{1}{T-K-1}\hat{\varepsilon}'\hat{\varepsilon}=
\frac{1}{T-K-1}{\varepsilon}'M{\varepsilon}
\end{gather*}
gdzie
\begin{gather*}
M=I_T-X(X'X)^{-1}X'
\end{gather*}
jest macierzą indepotentną o wymiarach $ T\times T $, taką, że $ M=M' $ oraz $ MM=M $.\\
Suma kwadratów reszt jest kwadratową formą wektora składników losowych. Obliczając jej wartość oczekiwaną, przy wykorzystaniu własności śladu macierzy otrzymujemy
\begin{align*}
&\mathbb E \hat{\varepsilon'}\hat{\varepsilon}
=\\=&
\mathbb E \varepsilon' M\varepsilon
=\\=&
\mathbb E  tr\left(\varepsilon'M\varepsilon\right)
=\\=&
\mathbb E  tr\left(M\varepsilon'\varepsilon\right)
=\\=&
tr\mathbb E  \left(M\varepsilon'\varepsilon\right)
=\\=&
trM\sigma^2_\varepsilon\Omega
=\\=&
\sigma^2_\varepsilon trM\Omega
\end{align*}
Aby wariancja reszt była nieobciążonym estymatorem wariancji składników losowych, wyrażenie powyższe powinno równać się
\begin{gather*}
\mathbb E \hat{\varepsilon}'\hat{\varepsilon}=\sigma^2_\varepsilon\left(T-K-1\right)
\end{gather*}
Jest to możliwe wtedy, gdy $ \Omega=I_T $. W takim przypadku, z uwagi na własności macierz $ M $ zachodzi
\begin{align*}
&trM
=\\=&
tr\left(I_T-X(X'X)^{-1}X'\right)
=\\=&
T-trX(X'X)^{-1}X'
=\\=&
T-tr(X'X)^{-1}X'X
=\\=&
T-trI_{K+1}
=\\=&
T-K-1
\end{align*}
Jeśli zatem $ \Omega\neq I_T $, wtedy $ \mathbb E \hat{\varepsilon}'\hat{\varepsilon}\neq\sigma^2_\varepsilon\left(T-K-1\right) $ i w konsekwencji
\begin{gather*}
\mathbb E \sigma^2_\varepsilon\neq
\mathbb E \tfrac{\hat{\varepsilon}'\hat{\varepsilon}}{T-K-1}=\sigma^2_\varepsilon
\end{gather*}
zatem wariancja reszt jest w takim przypadku obciążonym estymatorem wariancji składników losowych. Jeszcze poważniejszy problem związany jest z macierzą wariancji-kowariancji błędów estymacji. Jest ona równa macierzy $ \Sigma^2_{\hat{\beta}}=\sigma^2_\varepsilon\left(X'X\right) $ tylko wtedy, gdy $ \Omega=I_T $. Jeśli powyższa równość nie zachodzi, wtedy macierz wariancji-kowariancji przyjmuje postać
\begin{gather*}
\Sigma^2_{\hat{\beta}}=
\mathbb E \left(\hat{\beta}-\beta\right)\left(\hat{\beta}-\beta\right)'=
\sigma_\varepsilon^2\left(X'X\right)^{-1}\left(X'\Omega X\right)^{-1}
\end{gather*}
Wykorzystując estymator macierzy wariancji-kowariancji, prawdziwy dla sferycznych składników losowych, tj. $ \hat\Sigma_{\hat{\beta}}=\hat{\sigma}_\varepsilon^2(X'X)^{-1} $, w sytuacji, gdy składniki te nie są sferyczne popełniamy błąd zarówno z tytułu niewłaściwej postaci macierzy wariancji-kowariancji, jak również z tytułu obciążonej estymacji wariancji składników losowych. Na dodatek nie potrafimy w ogólnym przypadku określić jakiego rodzaju obciążenie towarzyszy wykorzystaniu macierzy $ \hat\Sigma_{\hat{\beta}}=\hat{\sigma}_\varepsilon^2(X'X)^{-1} $ sytuacji niesferycznych składników zakłócających. Wykorzystanie tej macierzy prowadzi do wyznaczenia niepoprawnych średnich błędów ocen parametrów strukturalnych.
Sugestia jest zatem następująca:\\
budując model ekonometryczny mależy dążyć do wyeliminowania niesferycznych składników losowych (drogą respecyfikacji modelu) lub jeśli nie jest to możliwe, oszacować parametry modelu UMNK, tj. eliminować skutki występowania niesferycznych składników losowych.
\section{Wybrane jednowymiarowe modele zmiennosci cen i stóp zwrotu}
Analiza szeregów czasowych rynku finansowego o wysokiej częstotliwosci obejmuje
\begin{itemize}
\item cechy analizy krótkookresowej (szeregi z tzw. krótką pamięcią)
\item podstawowy i uogólniony model grupowania wariancji ARCH
\item testowanie efektu ARCH i uogólnionego GARCH
\item niestandardowe modele ARCH (tzw. in mean, z symetrią, E-GARCH)
\item estymację i wykorzystanie modeli klasy GARCH
\end{itemize}
\subsection{Cechy analizy krótkookresowej}
Do cech procesów losowych (najczęściej procesów na rynkach finansowych) charakteryzują się wysoką częstotliwością zalicza się:
\begin{itemize}
\item naprzemienne występowanie okresów o zwiększonej fluktuacji i okresów niskiej zmienności zmiennej finansowe
\item skupiania wariancji w kolejnych jednostkach czasu, tj. dodatniej korelacji w dziedzinie zmienności zmiennej będącej przedmiotem zainteresowania, co przejawia się w wysokiej wariancji zmiennej powodowanej wzrostem tej wariancji w okresie poprzedzającym i analogicznie spadkiem wariancji na skutek niskiej wariancji w okresie poprzedzającym.
\end{itemize}
\subsection{Podstawowy i uogólniony model ARCH}
Rodzaje nieliniowych procesów stochastycznych.\\
W nieliniowej analizie jdenowymiarowych szeregów czasowych poszukuje się funkcji $ f $ wiążącej deny profces z ciagiem niezależnych zmiennych losowych o jednakowym rozkładzie
\begin{gather*}
Y_t=f\left(\varepsilon_t,\varepsilon_{t-1},\varepsilon_{t-2},\dots\right )
\end{gather*}
gdzie $ \varepsilon_t $ jest zmienną losową o średniej zero i jednostkowej wariancji.
Powyższa reprezentacja jest na tyle ogólna, że nie wiadomo jak dobierać postać funkcji $ f $. Najczęściej przyjmuje się, że nieliniowy proces ekonomiczny na postać
\begin{gather*}
Y_t=g\left(\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)+\varepsilon_th\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)
\end{gather*}
Procesy $ Y_t $ wyrażone z nieliniową funkcją $ g $ nazywamy procesami nieliniowymi w warunkowej wariancji.

Powyższa klasyfikacja ma sens, gdyż:
\begin{enumerate}
\item Warunkowa wartość oczekiwana $ Y_t $ możne być zapisana
\begin{gather*}
\mathbb E \left(Y_t\backslash\Omega_{t-1}\right)=g\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)
\end{gather*}
funkcja $ g $ opisuje zmiany wartości średniej procesu $ Y_t $ warunkowo względem informacji z przeszłości (zbiór $ \Omega_{t-1} $ oznacza zbiór wszystkich informacji dostępnych do momentu $ t-1 $).
\item Kwadrat funkcji $ h $ przedstawia zmiany warunkowej wariancji procesu $ Y_t $
\begin{align*}
&D^2\left(Y_t\backslash\Omega_{t-1}\right)
=\\=&
\mathbb E \left[Y_t-\mathbb E \left(Y_t\backslash\Omega_{t-1}\right)^2\backslash\Omega_{t-1}\right]
=\\=&
\mathbb E \left[\varepsilon_t^2 h^2\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)\backslash\Omega_{t-1}\right]
=\\=&
h^2\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)\mathbb E \left(\varepsilon_t^2 \right)
=\\=&
h^2\left (\varepsilon_{t-1},\varepsilon_{t-2},\dots \right)
\end{align*}
\end{enumerate}
Do najbardziej znanych modeli nieliniowych w warunkowej wartości średniej należą: procesy dwuliniowe, nieliniowe procesy autoregresji i średniej ruchomej, autoregresyjne modele progowe, przełącznikowe i wygładzonej przejścia, procesy autogresyjne o losowych współczynnikach. Znanymi procesami o zmiennej wariancji warunkowej są procesy ARCH/GARCH oraz procesy zmienności stochastycznej.

Podstawowym modelem ARCH jest układ dwurównaniowy
\begin{align*}
y_t=x_t^T\beta+\varepsilon
&&
h_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2
\end{align*}
gdzie
\begin{itemize}
\item $ x_t $ jest wektorem zmiennych objaśniających (najczęściej opóźnionych zmiennych endogenicznych postaci modelu autoregresji AR)
\item $ \beta  $ jest wektorem parametrów strukturalnych
\item $ \varepsilon_t $ jest składnikiem zakłócającym spełniającym warunek 
\begin{gather*}
\frac{\varepsilon_{t}}{\Omega_{t-1}}\sim\mathcal N(0,h)
\end{gather*}
\end{itemize}
W celu zapewnienia dodatniości warunkowej wariancji zakłada się ponadto, ze $ \alpha_0>0 $ i $ \alpha_i\le0  $. Warto zauważyć, że równanie powyższe jest nieliniowe za względu na zmienne, ale nie ze względu na parametry; równanie to (czyli granica sumy $ q $) wyznacza tzw. stopień modelu ARCH; mówimy wtedy o modelu ARCH(q). Model ARCH(q) opisuje poprawnie proces stacjonarny (lub inaczej ARCH(q) generuje proces stacjonarny), jeśli spełniony jest warunek
\begin{gather*}
\sum_{i=1}^{q}\alpha_i<1
\end{gather*}
Uogólnionym modelem ARCH, czyli modelem GARCH, jest
\begin{align*}
&y_t=x_t^T\beta+\varepsilon_t\\
&h_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-1}^2+\sum_{i=1}^{p}\gamma_ih_{t-1}
\end{align*}
gdzie oznaczenia zmiennych i parametrów jak w równaniach poprzednich.\\
W celu zapewnienia dodatniości warunkowej wariancji zakłada się ponadto, że $ \alpha_0>0 $ i $ \alpha_i\le0  $ i $ \gamma_1\le0  $. Granice sumowania $ q $ i $ p $ wyznaczają stopień modelu GARCH; mówimy, o modelu GARCH(q,p).

Stacjonarność procesu (czyli skończoność bezwarunkowej wariancji) opisanego modelem GARCH(q,p) jest zapewniona, jeśli spełniony jest warunek
\begin{gather*}
\sum_{i=1}^{q}\alpha_i+\sum_{i=1}^{p}\gamma_i<1
\end{gather*}
W zastosowaniach modelu rynków finansowych wygodnie jest korzystać z tzw. reprezentacji równoważnej modelu GARCH. Niech dany będzie proces $ v_t $ taki, że
\begin{align*}
&v_t=\varepsilon_t^2-h_t=(\eta_t^2-1)h_t\\
&h_t=\varepsilon-t^2-v_t\\
&\eta_t\sim\mathcal N(0,1)
\end{align*}
Z formuły wyrażającej warunkową wariancję w modelu GARCH wiemy, że $ h_t $ należy zapisać jako
\begin{align*}
&h_t
=\\=&
\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2+\sum_{i=1}^{p}\gamma_ih_{t-i}
\Leftrightarrow\\\Leftrightarrow&
h_t=\varepsilon_t^2-v_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2+\sum_{i=1}^{p}\gamma_ih_{t-i}\\&
\varepsilon_t^2-v_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2+\sum_{i=1}^{p}\gamma_i\left(\varepsilon_{t-i}^2-v_{t-i}\right)\\&
\varepsilon_t^2-v_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2+\sum_{i=1}^{p}\gamma_i\varepsilon_{t-i}^2-\sum_{i=1}^{p}\gamma_iv_{t-i}\\&
\varepsilon_t^2=\alpha_0+\underbrace{\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2+\sum_{i=1}^{p}\gamma_i\varepsilon_{t-i}^2}_{AR(\max(p,q))}-\underbrace{\sum_{i=1}^{p}\gamma_iv_{t-i}+v_t}_{MA(p)}\\&
\end{align*}
\subsection{Testowanie efektu ARCH/GARCH}
Testowanie efektu ARCH/GARCH jest ekwiwalentne, tj. istniejące testy nie pozwalają odróżnić obu procesów. Wynik testu wskazujący na obecność omawianego efektu pozwala jedynie z określonym prawdopodobieństwem wnioskować o obecności ARCH lub GARCH, beż możliwość rozróżnienia. Wnioskowanie o rzędach $ p $ i $ q $ procesów ARCHA/GARCH odbywa się na podstawie miar pojemności informacyjnej.
\subsubsection{Test Engle'a}
Jest to test typu mnożnika Lagrange'a, tzn. do testowania hipotezy zerowej niezbędna jest znajomość jedynie modelu z restrykcjami nałożonymi poprzez testową hipotezę. Przypomnijmy, że równaniem pomocniczym wariancji warunkowej w modelu ARCH jest
\begin{gather*}
h_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2
\end{gather*}
Engle zaproponował postać modelu AR dla kwadratów reszt uzyskanych z tej relacji, zatem szacowany (KMNK lub MNK) model przyjmuje postać
\begin{gather*}
\hat{\varepsilon}_k^2=\alpha_0+\sum_{i=1}^{q}\alpha_i\hat\varepsilon_{t-i}^2+\mu_t
\end{gather*}
Jeśli efekt ARCH/GARCH nie występuje, czyli nie występuje heteroskedastyczność wariancji warunkowej, wówczas wszystkie parametry $ \alpha_i $ powinny zanikać, tak więc hipotezami są
\begin{align*}
&H_0:\alpha_1=\alpha_2=\dots=\alpha_q=0\\
&H_A:\exists_{\alpha_i}\neq 0
\end{align*}
Statystyką testową jest typowa statystyka Lagrange'a
\begin{gather*}
LM=T\cdot R^2
\end{gather*}
gdzie $ R^2 $ jest współczynnikiem determinacji wyznaczonym dla modelu kwadratów reszt. Statystyka testowa $ LM $ ma rozkład graniczny $ \chi^2 $ o $ q $ stopniach swobody; wnioskowanie o odrzuceniu $ H_0 $ lub braku podstaw do odrzucenia jest typowe dla testu prawostronnego.
\subsubsection{Test McLeoda i Li}
W omawianym teście wykorzystuje się statystykę Ljunga-Boxa do weryfikacji hipotezy o braku autokorelacji kwadratów reszt, zatem test przebiega następująco
\begin{enumerate}
\item Oszacować relację KMNK
\item Wyznaczyć kwadraty reszt $ \hat\varepsilon_t^2 $
\item Obliczyć współczynniki autokorelacji (rzędu od 1 do $ q $) kwadratów reszt uzyskanych w punkcie poprzednim
\begin{gather*}
\hat{\rho}=\frac{cov\left(\hat \varepsilon_t^2,\hat \varepsilon_{t-i}^2\right)}{D^2\left(\hat \varepsilon_t^2\right)}
=
\frac{\frac{1}{T-i}\sum_{t=i+1}^{T}\hat \varepsilon_t^2\hat \varepsilon_{t-i}^2}{\frac{1}{T}\sum_{t=1}^{T}\hat \varepsilon_t^2}
\end{gather*}
\item Obliczyć statystykę Ljunga-Boxa
\begin{gather*}
Q'=T(T+2)\sum_{i=1}^{q}\frac{\rho_i}{T-i}
\end{gather*}
\item Statystyka Ljunga-Boxa ma rozkład graniczny $ \chi^2 $ o $ q $ stopniach swobody
\item Wobec zastosowanej statystyki testowej, zestawem hipotez jest
\begin{align*}
&H_0:\rho_1=\rho_2=\dots=\rho_q=0\\
&H_A:\exists_{\rho_i}\neq 0
\end{align*}
\end{enumerate}
\subsection{Niestandardowe modele GARCH}
\subsubsection{Model GARCH-M}
Model GARCH in MEAN (GARCH-M) przyjmuje postać
\begin{align*}
&y_t=x_t^T\beta +\lambda h_t+\varepsilon_t &&\frac{\varepsilon_t}{\Omega_{t-1}}\sim\mathcal N(0,h_t)\\
&h_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2+\sum_{i=1}^{p}\gamma_ih_{t-i}
\end{align*}
GRACH-M stosowany jest do modelowania ryzyka (premii za ryzyko). Jeżeli ocena parametru $ \lambda $ jest dodatnia i parametr może zostać uznany za statystycznie istotny, wówczas wzrost wariancji warunkowej $ h_t $ (czyli miary ryzyka) powoduje wzrost premii za ryzyko w postaci oczekiwanej stopy zwrotu z papieru ($ y_t $).
\subsubsection{Model GARCH z asymetrią reakcji}
Asymetria reakcji na "pakietowe" zmiany zmienności zmiennej będącej przedmiotem zainteresowania (np. stopa zwrotu $ t_t $) może być przybliżona prostym modelem GARCH-M
\begin{align*}
&r_t=a+\lambda h_t+\varepsilon_t
&&
\frac{\varepsilon_t}{\Omega_{t-1}}\sim \mathcal N(0,h_t)\\
&\varepsilon_t=v_t\sqrt{h_t}
&&
v_t\sim\mathcal N(0,1)\\
&h_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\varepsilon_{t-i}^2+\sum_{i=1}^{p}\gamma_ih_{t-i}
\end{align*}
Wówczas możliwe jest wyznaczenie $ v_t $ jako
\begin{gather*}
v_t=\frac{r_t-\left(a+\lambda h_t\right)}{\sqrt{h_t}}
\end{gather*}
Warto zauważyć, ze proces opisany przez to równanie charakteryzuje się rozkładem normalnym standardowym.
\subsubsection{Model E-GARCH}
W modelu E-GARCH czyni się typowe założenia odnoszące się do równania opisującego zmienną będącą przedmiotem zainteresowania, czyli
\begin{align*}
&y_t=x_t^T\beta+\varepsilon&&\frac{\varepsilon_t}{\Omega_{t-1}}\sim\mathcal N(0,h-t)\\
&\varepsilon_t=v_t\sqrt{h_t}&&v_t\sim\mathcal N(0,1)
\end{align*}
Funkcją warunkową wariancji jest
\begin{gather*}
\ln h_t=\alpha_0+\sum_{i=1}^{q}\alpha_i\left(\delta_1v_{t-1}+\delta_2\left(v_{t-1}-\sqrt{\frac{2}{\pi}}\right)\right)+\sum_{i=1}^{p}\gamma_i\ln h_{t-1}
\end{gather*}
Powyższy model jest modelem typu wykładniczego. Z definicji funkcji wykładniczej wynika, ze zapewniona jest nieujemność wariancji warunkowej.

\section{Ekonomiczne modele panelowe}
Definicja danych panelowych\\
\begin{tabular}{lll}
	dane przekrojowe         & $ y_i $    & $ i=1,\dots,N $                  \\
	szeregi czasowe          & $ y_t $    & $ t=1,\dots,T $                  \\
	dane przekrojowo-czasowe & $ y_{it} $ & $ i=1,\dots,N,\quad t=1,\dots,T$
\end{tabular}
\subsection{Dane panelowe}
\subsubsection{Przykłady danych panelowych.}
\begin{itemize}
\item Dane miesięczne o wydatkach dla 1000 gospodarstw domowych obserwowanych przez 5 lat - analiza preferencji konsumentów, badania skuteczności kompanii reklamowej.
\item Makroekonomiczne wskaźniki 25 krajów UE w ostatnich 10 lat publikowane przez Eurostat - badanie skuteczności europejskich funduszy strukturalnych.
\item Penn World Tables - skonstruowana przez Summersa i Hestona baza porównywalnych danych makroekonomicznych zawierająca informacje dla ponad 200 krajów dla 1960-2010 - analiza czynników wzrostu gospodarczego, weryfikacja hipotezy konwergencji gospodarczej.
\item Wskaźniki zatrudnienia i wynagrodzeń w Polsce w podziale na województwa za ostatnie 15 lat - badanie skuteczności polityki makroekonomicznej, szukanie czynników determinujących zmiany na rynku pracy.
\item Liczba inwestorów zagranicznych inwestujących w poszczególnych województwach w latach 1993-2015 - modelowanie częstotliwości występowania określonego zdarzenia - badanie atrakcyjności inwestycyjnej poszczególnych województw.
\item Dane firmy ubezpieczeniowej o ubezpieczeniach komunikacyjnych dotyczące np. szkodowości klientów w różnych latach - np. możliwość wykorzystania wyników do budowy systemu Bonus-Malus.
\item Informacje banków o kredytach konsumpcyjnych, hipotecznych i ich spłacalności na przestrzeni kilkunastu miesięcy lub lat - budowa modeli scoringowych - modeli wstępnej oceny ryzyka kredytowego.
\item Informacje firmy odzieżowej o przychodach i kosztach poszczególnych sklepów w sieci w różnych miastach lub centrach dla szeregu miesięcy - ocena skuteczności poszczególnych kampanii reklamowych, szczegółowa analiza indywidualnej specyfiki poszczególnych sklepów lub miast.
\end{itemize}
\subsubsection{Dlaczego warto stosować dane panelowe}
\begin{itemize}
\item Dane panelowe pozwalają na analizę zjawiska równocześnie w czasie jak i wymiarze przekrojowym lub przestrzennym.
\item Dane panelowe pozwalają na wyodrębnienie indywidualnej specyfiki poszczególnych obiektów.
\item Zastosowanie paneli danych pozwala na większą heterogeniczność (większe zróżnicowanie) jednostek badania.
\item Zapewnia większą liczbę stopni swobody oraz zwiększa efektywność oszacowania.
Wyodrębnienie efektów wpływu nieobserwowalnych zmiennych lub efektów.
\end{itemize}
\subsubsection{Specyfika danych panelowych}
\begin{itemize}
\item Obserwacje dotyczące tej samej jednostki mogą być ze sobą skorelowane.
\item Obserwacje dotyczące tego samego okresu mogą być ze sobą skorelowane
\item Czasem mamy do czynienia z przypadkami, gdzie nie występuje ani jeden, ani drugi rodzaj korelacji (przypadek akademicki)
\end{itemize}
\subsection{Ogólny zapis modelu panelowego}
Model statyczny
\begin{gather*}
Y_{it}=\beta_0+\beta'x_{it}+\alpha_{it}+v_t+u_{it}
\end{gather*}
gdzie
\begin{itemize}
\item $ \beta $ - wektor parametrów
\item $ x_{it} $ macierz obserwacji na zmiennych objaśniających
\item $ \alpha_i $ - efekty indywidualne, część zmienności $ y $ charakterystyczna dla efektu $ i $-tej jednostki ($ N $ efektów)
\item $ v_t $ efekty okresowe, część zmienności zmiennej $ y $ charakterystyczna dla okresu $ t $; ($ T $ efektów)
\item $ u_it $ czysto losowy składnik zakłócający
\end{itemize}
Rodzaje modeli panelowych
\begin{itemize}
\item[1a)] Jeżeli efekty indywidualne $ \alpha_i $ i efekty $ v_t $ są nieistotne, oznacza to, że mamy do czynienia z tak zwanym homogenicznym panele - analizowana przez nas relacja jest taka sama w każdym okresie i dla każdej badanej jednostki - Model Łącznej Regresji.
\item [a2)] Jeżeli zarówno efekty indywidualne jak i efekty okresowe są istotne, mamy do czynienia z heterogenicznym panelem - Dwukierunkowy Model Panelowy.
\item [a3)] Jeżeli tylko jedna grupa efektów jest istotna - Jednokierunkowy Model Panelowy
\item [b1)] Efekty indywidualne i (lub) okresowe mogą być efektami ustalonymi, czyli stałymi w danym czasie lub dla danej jednostki, nie zależą od czynników losowych - Model z Efektami Ustalonymi (ang. \textit{Fixed Effects Model}) - FE
\item [b2)] Efekty indywidualne i (lub) okresowe mogą być efektami losowymi, czyli zależą od czynników losowych i mogą się zmieniać, są zmiennymi losowymi o określonych rozkładach - Model z Efektami Losowymi (ang. \textit{Random Effects Model}) - RE.
\end{itemize}
Pozornie Niezależne Regresje\\
Czasami dysponując danymi panelowymi nie jesteśmy zainteresowani budową jednego modelu ekonometrycznego dal wszystkich posiadanych informacji. Szacowane sa wówczas oddzielne regresje dla każdej jednostki badania, ale dla podniesienia dokładności (efektywności) oszacowania, wykorzystana jest dodatkowa informacja wynikająca z faktu, że dla wszystkich jednostek badania można określić pewną strukturę stochastyczną; stosujemy Model Pozornie Niezależnych Regresji (ang. \textit{Seemingly Unrelated Regression Model}) - SUR
\subsection{Model regresji Łącznej}
Założenia
\begin{enumerate}
\item Mamy do czynienia z homogeniczną próbą - wszystkie jednostki badania są do siebie podobne. Oznacza to, że parametry szacowanego modelu są jednakowa dla wszystkich jednostek oraz we wszystkich okresach badania.
\item Różnice między empirycznymi wartościami zmiennej endogenicznej, a wartościami teoretycznymi są jedynie efektem działania zakłóceń losowych o tym samym rozkładzie dla każdej $ i $-tej jednostki i dla każdego okresu $ t $.
\end{enumerate}
Wybór takiego modelu może być poprzedzony odpowiednim testowaniem.
Model możemy zapisać jako
\begin{gather*}
y_{it}=\beta_0+\sum_{s=1}^{k}\beta_{sit}x_{sit}+u_{it}
\qquad i=1,\dots,N
\qquad t=1,\dots,T
\end{gather*}
gdzie:
\begin{itemize}
\item $ N $ - liczba jednostek w próbce
\item $ T $ - liczba okresów
\item $ k $ - liczba zmiennych objaśniających w modelu
\end{itemize}
Metody estymacji: MNK lub KMNK w przypadku niesferycznego składnika losowego.
\subsection{Model z Efektami Ustalonymi (FE)}
Jednokierunkowy (\textit{one-way})\\
Model możemy zapisać
\begin{gather*}
y_{it}=\alpha_{i}+\beta'x_{it}+u_{it}
\qquad i=1,\dots,N
\qquad t=1,\dots,T
\end{gather*}
$ \beta $ - wektor parametrów estymacji

Zapis macierzowy
\begin{gather*}
y_i=\alpha_ie+X_i\beta+u_i
\qquad i=1,\dots,N
\end{gather*}
Założenia do estymacji
\begin{itemize}
\item $ u_i $ oraz $ X_i $ są niezależne dla $ i=1,\dots,N $ - zmienne tworzące macierz $ X $ są ściśle egzogeniczne, czyli
\begin{gather*}
\forall_{i=1,\dots,N}\;
\mathbb E \left\{x_{it},u_{rs}\right\}=0
\end{gather*}
\item $ \mathbb E (u_i) =0$
\item $ \mathbb E(u_i,u_j')=\sigma^2 I,
\quad i=1,\dots,N, 
\quad j=1,\dots,N $ gdzie $ I $ jest macierzą jednostkową
\item $ r(X)=N+K\le NT $
\end{itemize}
Wprowadzamy $ N $ zmiennych zerojedynkowych - każda zmienna przyjmuje wartość 1 dla danej jednostki i 0 dla pozostałych jednostek.\\
Szacowanie:
\begin{itemize}
\item KMNK
\item Estymator Wewnątrzgrupowy
\end{itemize}
\subsection{Model z Efektami Losowymi (RE)}
Jednokierunkowy (\textit{one-way})\\
Ogólna postać modelu
\begin{gather*}
y_{it}=\beta_0+\beta'x_{it}+v_{it}
\qquad i=1,\dots,N
\qquad t=1,\dots,T
\end{gather*}
gdzie
\begin{itemize}
\item $ v_{it}=u_{it}+\alpha_i $ - dwuczęściowy składnik losowy
\end{itemize}
Dodatkowe założenia
\begin{itemize}
\item $ \mathbb E (\alpha_i)=0,\quad i=1,\dots,N $
\item Efekty indywidualne $ \alpha_i $ są niezależne od $ u_i $ oraz $ X_i $ dla $ i=1,\dots,N $
\item Efekty indywidualne dla rożnych jednostek są nieskorelowane, ale występuje korelacja między efektami z różnych okresów dla tej samej jednostki.
\end{itemize}
\begin{gather*}
\mathbb E (\alpha_i,\alpha_j)=
\left \{
\begin{array}{ll}
	0               & i\neq j \\
	\sigma^2_\alpha & i=j
\end{array}
\right .
\qquad i,j=1,\dots,N
\end{gather*}
Wariancja składnika losowego w modelu RE składa się z dwóch części
\begin{gather*}
\sigma^2_v=\sigma^2_u+\sigma^2_\alpha
\end{gather*}
gdzie
\begin{itemize}
\item $ \sigma_u^2 $ - wariancja składnika losowego
\item $ \sigma_\alpha^2 $ - wariancja efektów indywidualnych
\end{itemize}
\textbf{Uwaga!}\\
Składniki losowe w modelu RE są ze sobą skorelowane, czyli KMNK przestaje być efektywna, dlatego do oszacowania tego modelu zastosować estymator Uogólnionej Metody Najmniejszych Kwadratów.

Macierz wariancji i kowariancji
\begin{gather*}
V_{(NT\times NT)}=
\begin{bmatrix}
	\Omega _1 & 0         & \ldots & 0         \\
	0         & \Omega _2 & \ldots & 0         \\
	\vdots    & \vdots    & \ddots & \vdots    \\
	0         & 0         & \ldots & \Omega _N
\end{bmatrix}
=I_N\otimes \Omega_i
\end{gather*}
gdzie macierz $ \Omega_i $ jest w przypadku stałych efektów indywidualnych macierzą jednostkową rzędu $ T $, a w przypadku losowym efektów indywidualnych macierzą
\begin{gather*}
\Omega_t=\mathbb E (v_iv_i')=
\begin{bmatrix}
	(\sigma _{u}^2+\sigma_\alpha^2) & \sigma _{\alpha }^2             & \ldots & \sigma
 _{\alpha }^2            \\
	\sigma _{\alpha }^2             & (\sigma _{u}^2+\sigma_\alpha^2) & \ldots & \sigma
 _{\alpha }^2            \\
	\vdots                          & \vdots                          & \ddots & \vdots                          \\
	\sigma _{\alpha }^2             & \sigma _{\alpha }^2             & \ldots & (\sigma _{u}^2+\sigma_\alpha^2)
\end{bmatrix}
=\sigma_u^2I_T+\sigma_\alpha^2ii'
\end{gather*}
gdzie $ v_i=\alpha_i+u_i $
\subsection{Niespełnienie założeń}
\subsubsection{Heteroskedastyczność}
\begin{itemize}
\item \textit{groupwise heteroskedasticity} - macierze $ \Omega_i $ są różne dla poszczególnych jednostek, co może być spowodowane zmiennością czysto losowego składnika lub (w modelu random effects) zmiennością wariancji efektów indywidualnych.
\item \textit{cross-sectional correlation} - w macierzy blokowej $ V $ poza przekątną występują macierze niezerowe; sytuację taką najczęściej opisuje się poprzez współczynnik korelacji równoczesnych realizacji "czystego" zaburzenia losowego.
\end{itemize}
\subsubsection{Autokorelacja}
\begin{gather*}
\mathbb E (u_{it},u_{jt}|X_i,X_j)=\rho_{ij}
\end{gather*}
mogą wystąpić np. procesy autoregresyjne i/lub średniej ruchomej
\begin{gather*}
\mathbb E (u_{it},u_{is}|X_i)=\rho_{ts}
\end{gather*}
\subsection{Konsekwencje niespełnienia założeń}
Jeśli założenia o strukturze wariancji składnika losowego są błędne, to:
\begin{itemize}
\item Estymatory modelu po zostają nieobciążone, zgodne i asymptotycznie normalne, lecz są nieefektywne w porównaniu z estymatorem uwzględniającym strukturę prawdziwą.
\item Dodatkowo, obciążony jest estymator macierzy wariancji kowariancji parametrów, zatem mogą okazać się błędne wyniki o istotności zmiennych wyciągane na podstawie statystyki $ t $.
\end{itemize}
\subsection{Sposoby rozwiązania problemu}
\begin{itemize}
\item Zastosowanie UMNK z estymacją:
\begin{itemize}
\item dla modelu, w którym autokorelacja rzędu pierwszego: np. Cochrana-Orcutta
\item dla modelu ze zmienną wariancją tzw. Ważona Metoda Najmniejszych Kwadratów
\end{itemize}
\item Estymacja macierzy wariancji i kowariancji parametrów (czyli również błędów szacunku parametrów strukturalnych) w taki sposób, że byłaby odporna (roboust) na niespełnienie założeń o sferyczności składnika losowego.
\end{itemize}
\subsection{Estymatory odporne (roboust)}
\begin{enumerate}
\item Metoda zaproponowana przez Arellana (duże $ N $, małe $ T $). Macierz kowariancji i wariancji może by szacowana jako
\begin{gather*}
\Sigma_{\hat{\beta}}=NT(X'X)^{-1}\hat{\Omega}(X'X)^{-1}
\end{gather*}
gdzie $ \hat{\Omega} $ jest zgodnym estymatorem macierzy wariancji składnika losowego; dla wszelkich postaci heteroskedastyczności i autokorelacji " czystego składnika losowego $ u_{it} $ ma postać
\begin{gather*}
\hat{\Omega}=\frac{1}{NT}\sum_{i=1}^{N}\sum_{t=1}^{T}\hat{u}_{it}^2x_{it}x"_{it}
\end{gather*}
gdzie
\begin{itemize}
\item $ u_{it} $ - reszty modelu
\item $ x_{ti} $ - obserwacje na zmiennych objaśniających.
\end{itemize}
Jest to tzw. korekta White'a uogólniona do danych panelowych.
\item Metoda Arellano niedoszacowuje błędów w przypadku wystąpienia znaczącej autokorelacji zakłóceń pomiędzy jednostkami w tym samym momencie czasu. W takich przypadkach stosuje się
\begin{gather*}
\Sigma_{\hat{\beta}}=(X'X)^{-1}\left(\sum_{t=1}^{N}\sum_{j=1}^{N}\hat{\sigma}_{ij}(X_i'X_j)^{-1}\right)(X'X)^{-1}
\end{gather*}
gdzie $ \hat{\sigma}_{ij}=\frac{\hat{u}_i'\hat{u}_j}{T} $\\
$ T $ Oznacza długość szeregu czasowego dla każdej jednostki.
\end{enumerate}
\subsubsection{Heteroskedastyczność}
Dwa źródła
\begin{itemize}
\item duże zróżnicowanie jednostek w panelu
\item duża zmienność zakłóceń losowych w czasie
\end{itemize}
Przy dużym $ N $ i małym $ T $ uwzględnienie zmienności wariancji w modelu RE wiąże się z estymacją zbyt dużej liczby parametrów, stąd w takim przypadku powinno się stosować model FE. Wówczas macierz wariancji i kowariancji składników losowych
\begin{gather*}
V_{(NT\times NT)}=\Sigma_{(N\times N)}\otimes I_T
\end{gather*}
Model taki możemy szacować Uogólnioną Metodą Najmniejszych Kwadratów
\begin{gather*}
\hat{\beta}_{UMNK}=(x'\hat V^{-1}X)^{-1}
\end{gather*}
A elementy macierzy wariancji i kowariancji parametrów
\begin{gather*}
\Sigma_{\hat{\beta}}=(X'\hat V^{-1}X)^{-1}
\end{gather*}
a elementy macierzy $ \hat V $ obliczamy na podstawie reszt z oszacowania wstępnej regresji Klasyczną Metodą Najmniejszych Kwadratów
\begin{align*}
\hat{\sigma}_I^2=\frac{\hat u_i'\hat u_i}{T}
&&
\hat{\sigma}_{ij}=\frac{\hat u_i'\hat u_j}{T}
\end{align*}
\subsection{Model RE a model FE}
\begin{itemize}
\item Brak różnicy w estymacji, jeśli $ T $ dąży do nieskończoności
\item Jeśli efekty indywidualne nie są skorelowane za zmiennymi objaśniającymi, to zarówno FE jak i RE są nieobciążone i zgodne, ale RE jest efektywniejszy
\item Jeśli efekty indywidualne są skorelowane za zmiennymi objaśniającymi, to FE jest nieobciążony, a RE obciążony.
\end{itemize}
\subsection{Model z efektami ustalonymi (FE)}
Dwukierunkowy (\emph{two-way})\\
Jeżeli efekty okresowe (czasowe) mają być efektami ustalonymi to wyraz wolny dzielony jest na dwie części. Możemy  zatem zapisać:
\begin{gather*}
y_{it}=\alpha_i+\lambda_t+\beta'x_{it}+\varepsilon_{it}\qquad i=1,\dots,N\qquad t=1,\dots,T
\end{gather*}
gdzie
\begin{itemize}
\item $ \alpha_i $ - ustalony efekt indywidualny stały w każdym okresie czasu, ale inny (może być inny) dla każdego obiektu w panelu
\item $ \lambda_t $ - ustalony efekt, który ma tę samą wartość dla wszystkich jednostek w panelu w tym samym okresie, ale jest różny (może być różny) w każdym czasie.
\end{itemize}
Należy pamiętać, nie możemy wprowadzić równocześnie $ N $ efektów indywidualnych oraz $ T $ efektów okresowych, jeśli chcemy oszacować model $ \rightarrow $ dokładana współliniowość wektorów.
\subsection{Testowanie modeli FE, RE oraz FE vs RE}
\begin{itemize}
\item model FE: test Walda istotności efektów istotności efektów indywidualnych tzw. testowanie różnicy w sumie kwadratów reszt dla modelu \emph{pooled} (hipoteza zerowa) vs FE (hipoteza alteernatywna)
\item model RE: test Breuscha-Pagana (wersja testu mnożników Lagrange'a) istotności wariancji przy prawdziwości hipotezy zerowej wariancja jest stała w czasie dla wszystkich badanych jednostek
\item Fe vs RE: test hausmana; testowanie są różnice między oszacowaniami parametrów obu modeli; duże różnice (hipoteza alternatywna) mogą być spowodowane tym, że RE jest obciążony, czyli efekty indywidualne są skorelowane ze zmiennymi objaśniającymi lub nastąpił błąd w specyfikacji modelu; małe różnice (hipoteza zerowa) wskazują, że FE i RE są zgodne...
\end{itemize}
\subsection{Istotność efektów stałych w modelu FE}
Pytanie: czy wprowadzanie różnych dla poszczególnych wyrazów wolnych prowadzi do uzyskania dokładniejszych oszacowań?
\subsubsection{Test Walda (F)}
\begin{align*}
&H_0:\alpha_{it}=const,&&i=1,\dots,N&&t=1,\dots,T\\
&H_A:\forall_{i,j}\;\alpha_i\neq\alpha_j, \text{ale }\alpha_{it}=\alpha_{is}=\alpha_i&&i=1,\dots,N&&t,s=1,\dots,T
\end{align*}
Według hipotezy zerowej wyrazy wolne (wszystkich jednostek i okresów) mają tę samą własność. Według hipotezy alternatywnej wyrazy wolne są stałe w czasie, lecz mogą różnić się dla poszczególnych jednostek.

\textbf{Statystyka testowa z pórby}
\begin{gather*}
F=\frac{(R_1^2-R_0^2)/(N-1)}{(1-R_1^2)/(NT-N-k)}
\end{gather*}
gdzie:
\begin{itemize}
\item $ R_0^2 $ - wartość współczynnika determinacji dla modelu prawidłowego według hipotezy zerowej, czyli
\begin{gather*}
y_{it}=\alpha+\beta'x_{it}+\varepsilon_{it}\qquad i=1,\dots,N,\qquad t=1,\dots,T
\end{gather*}
\item $ R_1^2 $ - prawidłowa wartość współczynnika determinacji dla modelu poprawionego według hipotezy alternatywnej, czyli
\begin{gather*}
y_{it}=\alpha_i+\beta'x_{it}+\varepsilon_{it}\qquad i=1,\dots,N,\qquad t=1,\dots,T
\end{gather*}
\end{itemize}
Można również skorzystać ze statystyki testowej
\begin{gather*}
F=\frac{(S_1^2-S_0^2)/(N-1)}{(1-S_1^2)/(NT-N-k)}
\end{gather*}
gdzie:
\begin{itemize}
\item $ S_0^2 $ - suma kwadratów reszt dla modelu prawidłowego według hipotezy zerowej,
\item $ S_1^2 $ - dla modelu prawidłowego według hipotezy alternatywnej.
\end{itemize}
W obu przypadkach obszar krytyczny testu jest prawostronnie określony przez wartość krytyczną
\begin{gather*}
F_\alpha=(N-1,NT-N-k)
\end{gather*}
\subsection{Istotności efektów okresowych w FE}
Pytanie: czy właściwy jest model jednokierunkowy (\emph{one-way}), czy dwukierunkowy (\emph{two-way})?
\begin{align*}
&H_0:\gamma_{it}=\gamma_{is}=\gamma_i,&&i=1,\dots,N&&t,s=1,\dots,T\\
&H_A:\forall_{i,j}\;\gamma_i\neq\gamma_j,&&i=1,\dots,N&&t,s=1,\dots,T
\end{align*}
gdzie:
\begin{itemize}
\item $ \gamma_{it}=\alpha_i+\lambda_t $
\end{itemize}
Według hipotezy zerowej jest model jednokierunkowy, a według hipotezy alternatywnej model dwukierunkowy.

\textbf{Staytstyka z próby}
\begin{gather*}
F=\frac{(S_0^2-S_1^2)/(T-1)}{S_1^2/[NT-(N-1)-(T-1)-k]}
\end{gather*}
\subsection{Istotność efektów w modelu RE}
W modelu z efektami losowymi zakładamy, że składnik losowy zawiera w sobie zarówno efekty indywidualne jak i okresowe. Należy zbadać, czy wariancja składników losowych dla wszystkich obserwacji jest stała. Jeżeli jest stała, tzn. że brak istotnego zróżnicowania efektów i należy zastosować model regresji łącznej.
\subsubsection{Test Breuuscha-Pagana (jak w teście mnożników Lagrange'a)}
\begin{gather*}
H_0:\sigma_\alpha^2=0\\
H_A:\sigma_\alpha^2\neq 0
\end{gather*}
\textbf{Statystyka testowa z próby}
\begin{gather*}
LM=\frac{NT}{2(T-1)}
\left[\frac{\sum\limits_{i=1}^{N}\left(\sum\limits_{t=1}^{T}e_{it}\right)^2}{\sum\limits_{i=1}^{N}\sum\limits_{t=1}^{T}e_{it}^2}-1\right]^2
\end{gather*}
gdzie wartości reszt $ e_{it} $ to reszty z modelu regresji łącznej. Przy prawidłowości hipotezy zerowej powyższa statystyka ma rozkład $ \chi^2 $ z jednym stopniem swobody.
\subsection{Czy stosować model FE czy raczej RE?}
\begin{itemize}
\item Jakie założenia można przyjąć co do efektów indywidualnych?\\
Czy można je ocenić jako nieprzypadkowe (systematycznie) związane z poszczególnymi jednostkami?
\item Testowanie statystyczne - czy są spełnione przyjęte założenia?
\begin{itemize}
\item Kluczowym założeniem dla modelu RE jest założenie o niezależności efektów indywidualnych i zmiennych objaśniających.
\item Jeżeli założenie to nie jest spełnione wówczas estymator UMNK staje się estymatorem obciążonym.
\item W przypadku spełnienia założenia zarówno estymator UMNK jak i estymator KMNK dla modelu FE są zgodne i nieobciążone, jednak estymator UMNK jest bardziej lub co najmniej tak samo efektywny.
\end{itemize}
\end{itemize}
\subsubsection{Test Hausmana}
Polega na porównaniu wartości ocen parametrów uzyskanych przy pomocy obu estymatorów.
\begin{gather*}
H_0:\mathbb E (\xi_{it}|X)=0\\
H_A:\mathbb E (\xi_{it}|X)\neq 0
\end{gather*}
gdzie:
\begin{itemize}
\item $ \xi_{it}=\eta_i+u_{it} $
\item $ H_0 $ - oba estymatory są zgodne i nieobciążone, ale estymator UMNK dla modelu RE jest bardziej efektywny.
\item $ H_A $ - estymator UMNK dla RE jest obciążony, zatem należy stosować model FE, którego estymator jest nieobciążony
\end{itemize}
\textbf{Statystyka testowa z próby}
\begin{gather*}
m_1=
\left(\hat{\beta}_{RE}-\hat{\beta}_{FE}\right)^T
\left(\Var\left(\hat{\beta}_{FE}\right)-\Var\left(\hat{\beta}_{RE}\right)\right)^{-1}
\left(\hat{\beta}_{RE}-\hat{\beta}_{FE}\right)
\end{gather*}
gdzie:
\begin{itemize}
\item $ \hat{\beta}_{RE} $ - wektor ocen parametrów z modelu RE
\item $ \hat{\beta}_{FE} $ - wektor ocen parametrów z modelu FE
\item $ \Var(\hat{\beta}_{RE}) $ - macierz wariancji i kowariancji ocen parametrów modelu RE
\item $ \Var(\hat{\beta}_{FE}) $ - macierz wariancji i kowariancji ocen parametrów modelu FE
\end{itemize}
Jeżeli hipoteza $ H_0 $ jest prawdziwa to statystyka $ m_1 $ ma rozkład $ \chi^2 $ z $ k $  stopniami swobody (prawostronny obszar krytyczny).
\subsubsection{Macierz wariancji i kowariancji parametrów}
Ogólna formuła wyznaczania macierzy wariancji i kowariancji parametrów strukturalnych
\begin{gather*}
\Var\left(\hat{\beta }\right)=
\Sigma_{\hat{\beta}}=
\mathbb E \Bigl(\left(\hat{\beta}-\beta\right)\left(\hat{\beta}-\beta\right)'\Bigr)
=(X'X)^{-1}X'\Omega X(X^TX)^{-1}
\end{gather*}
gdzie $ \Omega $ jest macierzą wariancji i kowariancji składników losowych.\\
Tylko wtedy, gdy $ \Omega=\sigma^2I $ redukuje się do:
\begin{gather*}
\Var\left(\hat{\beta }\right)=
\Sigma_{\hat{\beta}}=
\mathbb E \Bigl(\left(\hat{\beta}-\beta\right)\left(\hat{\beta}-\beta\right)'\Bigr)
=\sigma_u^2(X^TX)^{-1}
\end{gather*}
\subsubsection{Współczynnik determinacji w analizie}
Dla modeli panelowych definiujemy różne współczynniki determinacji
\begin{itemize}
\item ogólny współczynnik determinacji
\begin{gather*}
R^2=r^2\left\{\hat{y}_{it},y_{it}\right\},\qquad i=1,\dots,N\qquad t=1,\dots,T
\end{gather*}
\item wewnątrzgrupowy współczynnik determinacji
\begin{gather*}
R^2=r^2\left\{\hat{y}_{it}-\overline{\hat{y}}_{it},y_{it}-\overline{y}_{it}\right\},\qquad i=1,\dots,N\qquad t=1,\dots,T
\end{gather*}
gdzie
\begin{gather*}
\begin{array}{l}
\overline{\hat{y}}_{it}=\frac{1}{T}\sum\limits_{t=1}^{T}\hat y_{it}\\
\overline{{y}}_{it}=\frac{1}{T}\sum\limits_{t=1}^{T} y_{it}
\end{array}
\qquad i=1,\dots,N
\end{gather*}
\item międzygrupowy współczynnik determinacji
\begin{gather*}
R^2_{BG}=r^2\left\{\overline{\hat{y}}_{it},\overline y_{it}\right\}
\end{gather*}
\end{itemize}
Wszystkie współczynniki wyznaczone są jako kwadraty współczynnika korelacji liniowej Pearsona i przyjmuje wartości z przedziału $ [0,1] $

Wartości przedstawionych współczynników mogą istotnie się różnić w zależności od postaci wybranego estymatora.
\begin{itemize}
\item Ogólny współczynnik determinacji przyjmuje z reguły największe wartości dla modelu regresji łącznej.
\item Zastosowanie modelu FE pozwala na maksymalizację $ R^2 $ wewnątrzgrupowego.
\item W regresji międzygrupowej uzyskamy najwyższe wartości współczynnika determinacji międzygrupowego.
\end{itemize}
\subsubsection{Szacowanie modeluu z Efektami ustalonymi (FE)}
Model FE ze zmiennymi zerojedynkowymi generalnie szacujemy KMNK (tzw. \emph{LSDV - LS with Dummy Variables}), pojawia się jednak często problem zbyt dużej liczby zmiennych w modelu - czyli zbyt dużego wymiaru macierzy $ X $. Rozwiązaniem (dla dużej liczby jednostek $ N $) jest Estymator Wewnątrzgrupowy.
\subsubsection{Estymator Wewnątrzgruopwy (\emph{Within estimator})}
Zdfeiniujemy następującee średnie wartości zmiennych dla każdej $ i $-tej jednostki:
\begin{align*}
\overline{y}_i=\frac{1}{T}\sum_{i=1}^{T}y_{it}
&&
\overline{x}_{ik}=\frac{1}{T}\sum_{i=1}^{T}x_{itk}
\end{align*}
Mamy zatem szereg średnich, każdy po $ N $ obserwacji. Możemy zapisać wówczas następujący model
\begin{gather}
\overline y_i=\alpha_i+\overline x_i\beta+\overline u_i\qquad i=1,\dots,N\label{r1}
\end{gather}
Model taki nazywany jest regresją międzygrupową (\emph{between group regression}) i jest modelem przekrojowym, gdzie wartości zmiennych są średnimi po czasie. Zwróćmy uwagę, że średnia wartość efektu indywidualnego dla $ i $-tej jednostki jest równa $ \alpha_i $, ponieważ efekt ten jest stały w czasie. Jeżeli zapiszemy informacje dla jednej jednostki ze wszystkich okresów w jeden wektor możemy zapisać, że
\begin{gather}
y_i=\alpha_i e+X_i\beta+u_i\label{r2}
\end{gather}
Jeżeli od równania \eqref{r2} odejmiemy równanie \eqref{r1} otrzymamy wówczas
\begin{gather*}
\left(y_i-\overline y_i\right)=\left(\alpha_i-\alpha_i\right)+
\left(x_i-\overline x_i\right)\beta+
\left(u_i-\overline u_i\right)
\end{gather*}
Zwróćmy uwagę, że poprzez taką transformację z modelu "znika" efekt indywidualny. Dzieje się tak dla każdej badanej jednostki. Poprzez transformację pierwotnych danych do postaci odchyleń od średnich uzyskujemy nowe zmienne:
\begin{align*}
&\tilde y_{it}=(y_{it}-\overline y_{it}),\\
&\tilde x_{it}=(x_{it}-\overline x_{it})
\end{align*}
Wówczas stosujemy KMNK dla przetransformowanych danych
\begin{gather*}
\hat \beta_{FE}=\left(\sum_{i=1}^{N}(\tilde X_i'\tilde X_i)\right)^{-1}\sum_{i=1}^{N}(\tilde X_i'\tilde Y_i)
\end{gather*}
Jest to tzw. estymator wewnątrzgrupowy (\emph{within group}). Może być zapisany również skalarnie jako
\begin{gather*}
\hat \beta_{FE}=
\left(\sum_{i=1}^{N}\sum_{t=1}^{T}(x_{it}-\overline x_{i})(x_{it}-\overline x_{i})'\right)^{-1}
\sum_{i=1}^{N}\sum_{t=1}^{T}(x_{it}-\overline x_{i})(y_{it}-\overline y_{i})'
\end{gather*}
Własności:
\begin{itemize}
\item Estymator wewnątrzgrupowy daje identyczne oceny parametrów jak MNK za zmiennymi zerojedynkowymi
\item Nie jest konieczne szacowanie $ N $ dodatkowych parametrów reprezentujących efekty indywidualne, jeżeli nie są one przedmiotem zainteresowania
\item Jeżeli zainteresowani jesteśmy konkretnymi wartościami efektów indywidualnych, mogą być one oszacowane następująco
\begin{gather*}
\hat \alpha_i=\overline y_i-\overline x_i'\hat \beta _{FE}
\end{gather*}
\item Estymator efektów indywidualnych jest zgodny tylko wówczas, gdy ilość obserwacji po czasie jest duża. $ T\rightarrow \infty  $.
\end{itemize}
Estymator macierzy wariancji-kowariancji definiujemy jako
\begin{gather*}
\Var\left(\hat \beta _{FE}\right)=\sigma _u^2\left(\sum_{i=1}^{N}\sum_{t=1}^{T}(x_{it}-\overline x_{i})(x_{it}-\overline x_{i})'\right)^{-1}
\end{gather*}
Nieznaną wartość wariancji zakłóceń losowych $ \sigma _u^2 $ zastępujemy wartością oszacowaną
\begin{gather*}
\hat \sigma _u^2=
\frac{\sum\limits_{i=1}^{N}\sum\limits_{t=1}^{T}e_{it}^2}{NT-N-k}=
\frac{\sum\limits_{i=1}^{N}\sum\limits_{t=1}^{T}\left(y_{it}-\hat \alpha_i-x_{it};\hat \beta_{FE}\right)^2}{NT-N-k}
\end{gather*}
gdzie:
\begin{itemize}
\item $ e_{it} $ - reszty oszacowanego przy pomocy estymatora wewnątrzgrupowego.
\end{itemize}
Z diagonalnych elementów macierzy $ \Var\left(\hat \beta_{FE} \right) $ wyliczamy błędy szacunku estymatora FE. Estymator wariancji ocen parametrów jest zgodny, gdy $ N\to \infty  $ lub $ T\to \infty  $.

\textbf{Uwaga!}\\
Estymator wewnątrzgrupowy nie może być wykorzystany do modelu, w którym wśród zmiennych objaśniających występują czynniki, które nie zmieniają się w czasie (marka i rodzaj produktu, płeć, pochodzenie, miejsce zamieszkania, położenie geograficzne itp.) Czynniki te są współliniowe z efektami indywidualnymi (nie mogą wchodzić razem do macierzy $ X $). W transformacji \emph{within} są eliminowane z modelu tak, jak efekty indywidualne (problem ten został rozwiązany poprzez zastosowanie tzw. estymatorów Hausmana-Taylora).
\subsubsection{Estymator wewnątrzgrupowy (\emph{within} dla modelu dwukierunkowego)}
Rozważmy cztery różne modele
\begin{enumerate}[Model (a):-]
\item Panelowy model z dekompozycją wyrazu wolnego na dwie części:
\begin{gather*}
y_{it}=\alpha _i+\lambda_t+\beta'x_{it}+u_{it}
\qquad i=1,\dots,N
\qquad t=1,\dots,T
\end{gather*}
\item Model uśredniony po czasie - \emph{between regression}
\begin{gather*}
\overline y_t=\alpha_i+\overline \lambda_t+\beta'\overline x_i +\overline u_i
\qquad i=1,\dots,N
\end{gather*}
\item Model uśredniony po jednostkach
\begin{gather*}
\overline y_t=\overline \alpha _i +\lambda _t+\beta'\overline x_t+\overline u_t
\qquad t=1,\dots,T
\end{gather*}
\item Obliczamy średnie wartości dla wszystkich obserwacji (łącznie po czasie i po jednostkach)
\begin{gather*}
\overline y _{it}=\overline \alpha _i+\overline \lambda _t+\beta '\overline x+\overline u_{it}
\end{gather*}
\end{enumerate}
(Zauważmy, że model (d) jest tylko pojedynczym równaniem)\\
Następnie wykonujemy następujące działania na poszczególnych równaniach stronami:
\begin{gather*}
(a)-(b)-(c)-(d)
\end{gather*}
W ten sposób eliminujemy wszystkie efekty ustalone indywidualne i okresowe
\begin{gather*}
\left(y_{it}-\overline y_i-\overline y_t+\overline y_{it}\right)=
\left(x_{it}-\overline x_i-\overline x_t+\overline x_{it}\right)+
\left(u_{it}-\overline u_i-\overline u_t+\overline u_{it}\right)\\
i=1,\dots,N
\qquad t=1,\dots,T
\end{gather*}
Jeżeli zdefiniujemy
\begin{align*}
&\dddot y_{it}=y_{it}-\overline y_i-\overline y_t+\overline y_{it}\\
&\dddot x_{it}=x_{it}-\overline x_i-\overline x_t+\overline x_{it}
\end{align*}
To możemy zapisać model postaci
\begin{gather*}
\dddot y_{it}=\beta'\dddot x_{it}+\dddot u_{it}
\end{gather*}
Model ten może być oszacowany KMNK na przekształconych danych.
\subsubsection{Jak znaleźć oceny efektów indywidualnych i okresowych?}
Załóżmy, że mamy następujący model
\begin{gather*}
y_{it}=\beta_0+\alpha_i+\lambda_t+\beta'x_{it}+u_{it}
\end{gather*}
w którym wprowadzimy $ (N-1)+(T-1) $ zmiennych sztucznych - pomijamy efekty dla pierwszego okresu $ \lambda_1 $. Możemy oszacować wartość $ i $-tego efektu indywidualnego jako
\begin{gather*}
\hat alpha _i=(\overline y_i-\overline y_1)-\hat \beta' (\overline x_i-\overline x _1)
\end{gather*}
Musimy pamiętać, że jakość estymacji zależy od liczby obserwacji dostępnych po czasie. Efekty okresowe można oszacować w analogiczny sposób.
\subsubsection{Błędy szacunku w modelach dwukierunkowych.}
Niech macierz kowariancji estymatorów parametrów
\begin{gather*}
\Var\left(\hat \beta \right)=\sigma _u^2\left(\dddot X'\dddot X\right)^{-1}
\end{gather*}
gdzie jako oszacowaną wartość wariancji zakłóceń stosujemy
\begin{gather*}
\hat \sigma _n^2=
\frac{\sum\limits_{i=1}^{N}\sum\limits_{t=1}^{T}\hat u_{it}^2}{NT-T-N-k}
\end{gather*}
gdzie
\begin{itemize}
\item $ \hat u_{it} $ - reszty wyliczone z estymacji modelu dwukierunkowego, $ i=1,\dots ,N$, $t=1,\dots,T $.
\end{itemize}
\subsection{Testowanie heteroskedastyczności}
Występowanie heteroskedastyczności możemy testować za pomocą trzech klasycznych testów ekonometrycznych, które są asymptotycznie równoważne (przy $ T\to \infty  $), jednak przy niewielkiej ilości okresów czasu mogą dawać różne rezultaty. We wszystkich trzech przypadkach testujemy:
\begin{center}
\begin{tabular}{ll}
$ H_0:\sigma_i=\sigma  $ & dla każdego $ i $\\
$ H_1:\sigma_i\neq\sigma_j  $ & dla $ i\neq j $
\end{tabular}
\end{center}
i wszystkie trzy statystyki testujące mają rozkład $ \chi^2(N-1) $
\begin{enumerate}
\item Dysponując resztami z modelu z ograniczeniami, czyli zakładającej homoskedastyczność metody KMNK, możemy znaleźć statystykę testy=u mnożnika Lagrange'a
\begin{gather*}
LM=\frac{T}{2}\sum_{i=1}^{N}\left(\frac{\hat \sigma _i^2}{\hat \sigma^2}-1\right)^2
\end{gather*}
gdzie:
\begin{gather*}
\hat \sigma ^2=\frac{1}{N}\sum_{i=1}^{N}\hat \sigma ^2_i=\frac{\hat u'\hat u}{NT}
\end{gather*}
\item Wstawiając estymatory uzyskane analogicznie na podstawie reszt z modelu bez ograniczeń (estymacja UMNK), otrzymujemy statystykę Walda
\begin{gather*}
W=\frac{T}{2}\sum_{i=1}^{N}\left(\frac{\hat \sigma ^2}{\hat \sigma_i^2}-1\right)^2
\end{gather*}
\item Estymując oba modele za pomocą metody największej wiarygodności (lub iteracyjnej UMNK) i znajdując $ \hat \sigma ^2 $ z modelu z ograniczeniami oraz $ \hat \sigma _i^2 $ z modelu bez ograniczeń, możemy obliczyć statystykę testu ilorazu wiarygodności
\begin{gather*}
LR= NT\ln \hat \sigma ^2-\sum_{i=1}^{N}T\ln \hat \sigma ^2
\end{gather*}
Niestety, wszystkie statystyki są wrażliwe na założenie o normalności reszt. Jeżeli założenie to nie jest spełnione, możemy posłużyć się skorygowaną statystyką Walda
\begin{gather*}
W'=\sum_{i=1}^{N}\frac{\left(góra\hat \sigma_i^2-\hat{\sigma ^2}\right)^2}{V_i}
\end{gather*}
gdzie
\begin{gather*}
V_i=\frac{1}{T(T-1)}\sum_{i=1}^{N}\left(\hat u_{it}^2-\hat \sigma _i^2\right)
\end{gather*}
\end{enumerate}
\subsection{Testowanie korelacji międzygrupowej}
Testowanie korelacji międzygrupowej (przekrojowej, międzyjednostkowej, \emph{cross-sectional}) - korelacja składników losowych pochodzących z różnych obiektów, ale z tego samego czasu. Najczęściej testowaniu podlega hipoteza zerowa o i istnieniu \emph{cross-sectional correlation}
\begin{align*}
&H_0:\Omega=
\begin{bmatrix}
	\sigma _1 & 0         & \ldots & 0         \\
	0         & \sigma _2 & \ldots & 0         \\
	\vdots    & \vdots    & \ddots & \vdots    \\
	0         & 0         & \ldots & \sigma _N
\end{bmatrix}\\
&H_1:\Omega=
\begin{bmatrix}
	\sigma _{11} & \sigma _{12} & \ldots & \sigma _{1N} \\
	\sigma _{21} & \sigma _{22} & \ldots & \sigma _{2N} \\
	\vdots       & \vdots       & \ddots & \vdots       \\
	\sigma _{N1} & \sigma _{N2} & \ldots & \sigma _{NN}
\end{bmatrix}
\end{align*}
\begin{enumerate}
\item Test mnożników Lagrange'a Breucha-Pagana będzie miał postać
\begin{gather*}
LM=T\sum_{i=2}^{N}\sum_{j=1}^{i-1}r_{ij}^2
\end{gather*}
gdzie
\begin{itemize}
\item $ r_{ij} $ to współczynnik korelacji reszt między obiektami $ i $-tym i $ j $-tym uzyskanymi z oszacowania UMNK (ze względu na założenie stałości parametrów strukturalnych względem obiektów.)
\end{itemize}
Statystyka z próby ma rozkład $ \chi ^2(N(N-1)/2) $
\item Test ilorazu wiarygodności (statystyka testu)
\begin{gather*}
LR=T\left(\sum_i \ln \hat \sigma_i^2-\ln \left|\hat \Sigma\right|\right)
\end{gather*}
gdzie:
\begin{itemize}
\item $ \hat \sigma_i^2 $ - wyliczane są z modelu oszacowanego z uwzględnieniem zjawiska heteroskedastyczności grupowej
\item $ \hat \Sigma $ - elementami tej macierzy są oceny kowariancji wyznaczone na podstawie reszt modelu oszacowanego z uwzględnieniem zjawiska heteroskedsastyczności i korelacji przekrojowej
\end{itemize}
Obie powyższe estymacje wykonane są przy pomocy metody największej wiarygodności.\\
Rozkład z próby ma rozkład $ \chi^2(N(N-1)\backslash2) $.
\end{enumerate}
\subsection{Szacowanie modelu z heteroskedastycznością}
Zjawisko heteroskedastyczności grupowej jest szczególnym przypadkiem heteroskedastyczności składnika losowego. W takim przypadku do estymacji wykorzystać można Ważoną Metodę Najmniejszych Kwadratów (WMNK):
\begin{itemize}
\item próba dzielona jest w sposób naturalny na podpróby składające się z obserwacji w czasie dla poszczególnych jednostek
\item na podstawie reszt KNK z kolejnych podprób, wyznacza się wariancje próbkowe $ \hat \sigma_i^2 $, które określają wagi przypisywane kolejnym jednostkom.
\end{itemize}
Wówczas estymator WMNK ma postać
\begin{gather*}
\hat \beta_{WMNK}=
\left(X^TV^{-1}X\right)^{-1}
\left(X^TV^{-1}y\right)=
\left[\sum_{i=1}^{N}\frac{1}{\sigma_i^2}X_i^TX_i\right]^{-1}
\left[\sum_{i=1}^{N}\frac{1}{\sigma_i^2}X_i^Ty_i\right]
\end{gather*}
\subsection{Szacowanie modelu z korelacją przekrojową i heteroskedastycznością grupową}
\begin{enumerate}
\item Metoda UMNK z estymacją (FGLS) - metoda Parka
\begin{itemize}
\item[Krok 1:] Wyznacza się oceny kowariancji pomiędzy obiektami Mi-tym i $ j $-tym na podstawie reszt uzyskanych ze zgodnego, choć obciążonego estymatora KMNK
\item[Krok 2:] Wyznacza się ocney parametrów strukturalnych według wzoru:
\begin{gather*}
\hat \beta_{UMNK}=
\left(X^TV^{-1}X\right)^{-1}
\left(X^TV^{-1}y\right)
\end{gather*}
gdzie elementami $ V $ są kowariancje wyznaczone w pierwszym kroku estymacji.
\end{itemize}
\item Metoda \emph{Panel-Corrected Standard Error} (PCSE)
\begin{itemize}
\item parametry strukturalne modelu szacowane są przy pomocy KMNK - oceny są zgodne i nieobciążone
\item problem nieefektywności rozwiązany jest poprzez zastosowanie specyficznej formuły obliczania średnich błędów szacunku parametrów, tzw. "skorygowanych błędów standardowych dla danych panelowych"
\item efektywność estymatora rośnie wraz ze wzrostem wymiaru czasowego danych względem jednostek
\end{itemize}
Macierz wariancji i kowariancji estymatorów
\begin{gather*}
D^2\left(\hat \beta \right)_{PCSE}=\left(X^TX\right)^{-1}
\left(X^T\hat VX\right)^{-1}
\left(X^TX\right)^{-1}
\end{gather*}
gdzie $ E $ jest macierzą reszt modelu oszacowanego KMNK, w której kolumna o numerze $ i $ zawiera reszty pochodzące z $ i $-tego obiektu.
\end{enumerate}
\subsection{Testowanie "zwykłej" autokorealcji składnika losowego}
W modelu panelowym może ponadto występować jeszcze klasyczna autokorelacja zakłóceń losowych. Możliwe warianty:
\begin{itemize}
\item[1a.] Składniki losowe z różnych obiektów są nieskorelowane, autokorealcja występuje tylko "wewnątrz" poszczególnych obiektów
\item[2a.] Oprócz autokorelacji dopuszcza się też występowanie korelacji przekrojowej
\item[1b.] Współczynnik autokorelacji może być jednakowy dla wszystkich obiektów
\item[2b.] Współczynnik autokorelacji może być różnych dla różnych obiektów
\end{itemize}
\begin{enumerate}
\item Test seryjnej autokorelacji dowolnego rzędu Breuscha-Godfrey'a (opaty na mnożnikach Lagrange'a); stosowany w modelach regresji łącznej FE i RE.
\item Test Baltagi-Li (oparty na mnożnikach Lagrange'a) - bada składniki losowe dla poszczególnych jednostek, czy nie mają charakteru AR(1) lub MA(1), stosowany w modelach RE
\item Test Durbina-Watsona, stosowany w modelach regresji łącznej FE i RE.
\end{enumerate}
\subsection{Szacowanie modelu z autokorelacją}
W celu usunięcia autokorelacji stosuje się zazwyczaj przekształcenie Praisa-Winstera - szeregi czasowe dotyczące kolejnych obiektów transformuje się w następujący sposób:
\begin{align*}
Y_i^*=
\begin{bmatrix}
\sqrt{1-r_i^2}y_{i1}\\
y_{i2}-r_iy_{i1}\\
\vdots \\
y_{it}-r_iy_{i(T-1)}
\end{bmatrix}
&&
X_i^*=
\begin{bmatrix}
\sqrt{1-r_i^2}y_{i1}\\
x_{i2}-r_ix_{i1}\\
\vdots \\
x_{it}-r_ix_{i(T-1)}
\end{bmatrix}
\end{align*}
gdzie
\begin{itemize}
\item $ r_i $ jest oceną współczynnika autokorelacji
\item $ x_{it} $ jest wektorem wartości wszystkich zmiennych objaśniających przyjmowanych dla $ i $-tego obiektu w okresie $ t $.
\end{itemize}
W modelu szaconym na przetransformowanych danych nie występuje autokorelacja.
\subsection{UMNK dla modelu z efektami losowymi}
W zapisie macierzowym
\begin{gather*}
Y=X\beta +v
\end{gather*}
Macierz wariancji i kowariancji dwuczęściowego składnika losowego można ogólnie zapisać jako
\begin{gather*}
\mathbb E \left(vv'\right)=
\mathbb E \left(
\begin{bmatrix}
	v_1v_1 & \ldots & v_1v_N \\
	\vdots & \ddots & \vdots \\
	v_Nv_1 & \ldots & v_Nv_N
\end{bmatrix}
\right)=\Omega
\end{gather*}
zauważmy, że z założeń o niezależności poszczególnych części składnika losowego wynika, że
\begin{itemize}
\item dla $ i\neq j $
\begin{gather*}
\mathbb E \left(v_iv_j'\right)=
\mathbb E \left(\alpha_i+u_i\right)\left(\alpha_j+u_j\right)'
=\\=
\mathbb E \left(\alpha_i\alpha_j'\right)+
\mathbb E \left(\alpha_iu_j'\right)+
\mathbb E \left(\alpha_iu_i\right)+
\mathbb E \left(u_iu_j'\right)=0
\end{gather*}
\item dla $ i=j $
\begin{gather*}
\mathbb E \left(v_iv_j'\right)=
\mathbb E \left(\alpha_i+u_i\right)\left(\alpha_i+u_i\right)'
=\\=
\mathbb E \left(\alpha_i^2\right)+\mathbb E \left(\alpha_iu_i\right)+\mathbb E \left(u_iu_i'\right)=
\sigma_\alpha^2+\sigma_u^2I=\omega
\end{gather*}
\end{itemize}
Macierz wariancji i kowariancji składników losowych w modelu RE jest macierzą blokowo-diagonalną:
\begin{gather*}
\Omega=
\begin{bmatrix}
	\omega & 0      & \ldots & 0      \\
	0      & \omega & \ldots & 0      \\
	\vdots & \vdots & \ddots & \vdots \\
	0      & 0      & \ldots & \omega
\end{bmatrix}\neq\sigma^2I
\end{gather*}
W UMNK potrzebujemy odwrotności tej macierzy, a zatem i odwrotności $ \omega $. Można wykazać, że przy przyjętych założeniach:
\begin{gather*}
\omega^{-1}=\frac{1}{\sigma_u^2}\left[I-\frac{\sigma_\alpha^2}{\sigma_u^2+T\sigma_\alpha^2}\right]
\end{gather*}
Nieznane wariancje można zatąpić ich ocenami uzyskanymi z oszacowania:
\begin{itemize}
\item $ \sigma_u^2 $ - z modelu FE
\item $ \sigma_\alpha^2 $ - z regresji międzygrupowej
\end{itemize}
Ostateczną postać estymatora UMNK dla modelu z efektami losowymi (RE) można zapisać
\begin{gather*}
\hat \beta_{RE}=
\left(
\sum_{i=1}^{N}\sum_{t=1}^{T}
\left(x_{it}-\overline{x_i}\right)
\left(x_{it}-\overline{x_i}\right)'+
\psi T\sum_{i=1}^{N}
\left(x_i-\overline{x}\right)
\left(x_i-\overline{x}\right)'
\right)^{-1}
\cdot\\\cdot
\left(
\sum_{i=1}^{N}\sum_{t=1}^{T}
\left(x_{it}-\overline{x_i}\right)
\left(x_{it}-\overline{x_i}\right)'+
\psi T\sum_{i=1}^{N}
\left(x_i-\overline{x}\right)
\left(y_i-\overline{y}\right)'
\right)^{-1}
\end{gather*}
gdzie
\begin{gather*}
\psi=\frac{\sigma_u^2}{\sigma_i^2+T\sigma_\alpha^2}
\end{gather*}
Przybliżona wartość losowych efektów indywidualnych można wyznaczyć jako
\begin{gather*}
\hat \mu_{RE}=\overline Y=\hat \beta_{RE}\overline x
\end{gather*}
Przypadki modeli panelowych niespełniających dotychczasowych założeń
\begin{enumerate}
\item Model ze zmiennymi stratami w czasie
\end{enumerate}
Załóżmy, że część zmiennych objaśniających w modelu panelowym jest stała w czasie. Zauważmy, że zmienne stałe w czasie są współliniowe w ustalonymi efektami indywidualnymi. Zatem nie możemy uzyskać ocen parametrów strukturalnych przy tych zamiennych w modelu FE.

Rozwiązania:
\begin{enumerate}
\item Stosujemy model RE - nie jest możliwe, jeśli zmienne objaśniający okazują się skorelowane ze składnikiem losowym - (test Hausmana).
\item Szacujemy model FE przy pomocy estymatora Hausmana-Tatylora
\item Stosujemy inny zastaw założeń dla efektów indywidualnych w modelu RE.
\end{enumerate}